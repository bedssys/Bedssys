{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN for Human Activity Recognition - 2D Pose Input\n",
    "\n",
    "This experiment is the classification of human activities using a 2D pose time series dataset and an LSTM RNN.\n",
    "The idea is to prove the concept that using a series of 2D poses, rather than 3D poses or a raw 2D images, can produce an accurate estimation of the behaviour of a person or animal.\n",
    "This is a step towards creating a method of classifying an animal's current behaviour state and predicting it's likely next state, allowing for better interaction with an autonomous mobile robot.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The aims of this experiment are:\n",
    "\n",
    "-  To determine if 2D pose has comparable accuracy to 3D pose for use in activity recognition. This would allow the use of RGB only cameras for human and animal pose estimation, as opposed to RGBD or a large motion capture dataset.\n",
    "\n",
    "\n",
    "- To determine if  2D pose has comparable accuracy to using raw RGB images for use in activity recognition. This is based on the idea that limiting the input feature vector can help to deal with a limited dataset, as is likely to occur in animal activity recognition, by allowing for a smaller model to be used (citation required).\n",
    "\n",
    "\n",
    "- To verify the concept for use in future works involving behaviour prediction from motion in 2D images.\n",
    "\n",
    "The network used in this experiment is based on that of Guillaume Chevalier, 'LSTMs for Human Activity Recognition, 2016'  https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition, available under the MIT License.\n",
    "Notable changes that have been made (other than accounting for dataset sizes) are:\n",
    " - Adapting for use with a large dataset ordered by class, using random sampling without replacement for mini-batch.  \n",
    " This allows for use of smaller batch sizes when using a dataset ordered by class. \"It has been observed in practice that when using a larger batch there is a significant degradation in the quality of the model, as measured by its ability to generalize\"  \n",
    "      _N.S Keskar, D. Mudigere, et al, 'On Large-Batch Training for Deep Learning: Generalization Gap and Sharp \n",
    "      Minima', ICLR 2017_ https://arxiv.org/abs/1609.04836\n",
    "      \n",
    " - Exponentially decaying learning rate implemented\n",
    "\n",
    "\n",
    "\n",
    "## Dataset overview\n",
    "\n",
    "The dataset consists of pose estimations, made using the software OpenPose (https://github.com/CMU-Perceptual-Computing-Lab/openpose's) on a subset of the Berkeley Multimodal Human Action Database (MHAD) dataset http://tele-immersion.citris-uc.org/berkeley_mhad.\n",
    "\n",
    "This dataset is comprised of 12 subjects doing the following 6 actions for 5 repetitions, filmed from 4 angles, repeated 5 times each.  \n",
    "\n",
    "- JUMPING,\n",
    "- JUMPING_JACKS,\n",
    "- BOXING,\n",
    "- WAVING_2HANDS,\n",
    "- WAVING_1HAND,\n",
    "- CLAPPING_HANDS.\n",
    "\n",
    "In total, there are 1438 videos (2 were missing) made up of 211200 individual frames.\n",
    "\n",
    "The below image is an example of the 4 camera views during the 'boxing' action for subject 1\n",
    "\n",
    "![alt text](images/boxing_all_views.gif.png \"Title\")\n",
    "\n",
    "The input for the LSTM is the 2D position of 18 joints across a timeseries of frames numbering n_steps (window-width), with an associated class label for the frame series.  \n",
    "A single frame's input (where j refers to a joint) is stored as:\n",
    "\n",
    "[  j0_x,  j0_y, j1_x, j1_y , j2_x, j2_y, j3_x, j3_y, j4_x, j4_y, j5_x, j5_y, j6_x, j6_y, j7_x, j7_y, j8_x, j8_y, j9_x, j9_y, j10_x, j10_y, j11_x, j11_y, j12_x, j12_y, j13_x, j13_y, j14_x, j14_y, j15_x, j15_y, j16_x, j16_y, j17_x, j17_y ]\n",
    "\n",
    "For the following experiment, very little preprocessing has been done to the dataset.  \n",
    "The following steps were taken:\n",
    "1. openpose run on individual frames, for each subject, action and view, outputting JSON of 18 joint x and y position keypoints and accuracies per frame\n",
    "2. JSONs converted into txt format, keeping only x and y positions of each frame, action being performed during frame, and order of frames. This is used to create a database of associated activity class number and corresponding series of joint 2D positions\n",
    "3. No further prepossessing was performed.  \n",
    "\n",
    "In some cases, multiple people were detected in each frame, in which only the first detection was used.\n",
    "\n",
    "The data has not been normalised with regards to subject position in the frame, motion across frame (if any), size of the subject, speed of action etc. It is essentially the raw 2D position of each joint viewed from a stationary camera.  \n",
    "In many cases, individual joints were not located and a position of [0.0,0.0] was given for that joint\n",
    "\n",
    "A summary of the dataset used for input is:\n",
    "\n",
    " - 211200 individual images \n",
    " - n_steps = 32 frames (~=1.5s at 22Hz)\n",
    " - Images with noisy pose detection (detection of >=2 people) = 5132  \n",
    " - Training_split = 0.8\n",
    " - Overlap = 0.8125 (26 / 32) ie 26 frame overlap\n",
    "   - Length X_train = 22625 * 32 frames\n",
    "   - Length X_test = 5751 * 32 frames\n",
    "   \n",
    "Note that their is no overlap between test and train sets, which were seperated by activity repetition entirely, before creating the 26 of 32 frame overlap.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Training and Results below: \n",
    "Training took approximately 4 mins running on a single GTX1080Ti, and was run for 22,000,000ish iterations with a batch size of 5000  (600 epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PF~4\\Python\\AN~3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf  # Version 1.0.0 (some previous versions are used in past commits)\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from random import randint\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Useful Constants\n",
    "\n",
    "# Output classes to learn how to classify\n",
    "# LABELS = [    \n",
    "#     \"JUMPING\",\n",
    "#     \"JUMPING_JACKS\",\n",
    "# #     \"BOXING\",\n",
    "#     \"WAVING_2HANDS\",\n",
    "#     \"WAVING_1HAND\",\n",
    "#     \"CLAPPING_HANDS\"\n",
    "# ] \n",
    "\n",
    "LABELS = [    \n",
    "    \"GO_IN\",\n",
    "    \"GO_OUT\",\n",
    "    \"WALK_LEFT\",\n",
    "    \"WALK_RIGHT\"\n",
    "] \n",
    "\n",
    "# DATASET_PATH = \"data/HAR_pose_activities/database/\"\n",
    "DATASET_PATH = \"data/Overlap_fixed4_separated/\"\n",
    "# DATASET_PATH = \"data/HAR_pose_activities/database/Training Default/\"\n",
    "\n",
    "X_train_path = DATASET_PATH + \"X_train.txt\"\n",
    "X_test_path = DATASET_PATH + \"X_test.txt\"\n",
    "# X_test_path = \"utilities/something/something.txt\"\n",
    "\n",
    "y_train_path = DATASET_PATH + \"Y_train.txt\"\n",
    "y_test_path = DATASET_PATH + \"Y_test.txt\"\n",
    "\n",
    "# n_steps = 32 # 32 timesteps per series\n",
    "n_steps = 5 # 32 timesteps per series\n",
    "# n_steps = 1 # 32 timesteps per series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the networks inputs\n",
    "\n",
    "def load_X(X_path):\n",
    "    file = open(X_path, 'r')\n",
    "    X_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.split(',') for row in file\n",
    "        ]], \n",
    "        dtype=np.float32\n",
    "    )\n",
    "    file.close()\n",
    "    blocks = int(len(X_) / n_steps)\n",
    "    \n",
    "    X_ = np.array(np.split(X_,blocks))\n",
    "\n",
    "    return X_ \n",
    "\n",
    "# Load the networks outputs\n",
    "\n",
    "def load_y(y_path):\n",
    "    file = open(y_path, 'r')\n",
    "    y_ = np.array(\n",
    "        [elem for elem in [\n",
    "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
    "        ]], \n",
    "        dtype=np.int32\n",
    "    )\n",
    "    file.close()\n",
    "    \n",
    "    # for 0-based indexing \n",
    "    return y_ - 1\n",
    "\n",
    "X_train = load_X(X_train_path)\n",
    "X_test = load_X(X_test_path)\n",
    "#print X_test\n",
    "\n",
    "y_train = load_y(y_train_path)\n",
    "y_test = load_y(y_test_path)\n",
    "# proof that it actually works for the skeptical: replace labelled classes with random classes to train on\n",
    "#for i in range(len(y_train)):\n",
    "#    y_train[i] = randint(0, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(X shape, y shape, every X's mean, every X's standard deviation)\n",
      "(1005, 5, 36) (1010, 1) 226.44403 140.37772\n",
      "\n",
      "The dataset has not been preprocessed, is not normalised etc\n"
     ]
    }
   ],
   "source": [
    "# Input Data \n",
    "\n",
    "training_data_count = len(X_train)  # 4519 training series (with 50% overlap between each serie)\n",
    "test_data_count = len(X_test)  # 1197 test series\n",
    "n_input = len(X_train[0][0])  # num input parameters per timestep\n",
    "\n",
    "n_hidden = 34 # Hidden layer num of features\n",
    "# n_classes = 6 \n",
    "n_classes = len(LABELS)\n",
    "\n",
    "#updated for learning-rate decay\n",
    "# calculated as: decayed_learning_rate = init_learning_rate * decay_rate ^ (global_step / decay_steps)\n",
    "decaying_learning_rate = True\n",
    "learning_rate = 0.0025 #used if decaying_learning_rate set to False\n",
    "init_learning_rate = 0.005\n",
    "decay_rate = 0.96 #the base of the exponential in the decay\n",
    "decay_steps = 100000 #used in decay every 60000 steps with a base of 0.96\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "lambda_loss_amount = 0.0015\n",
    "\n",
    "# training_iters = training_data_count *300  # Loop 300 times on the dataset, ie 300 epochs\n",
    "# training_iters = training_data_count *60\n",
    "# training_iters = training_data_count *120\n",
    "# training_iters = training_data_count *1\n",
    "# batch_size = 5\n",
    "batch_size = 60\n",
    "# batch_size = 512\n",
    "display_iter = batch_size*8  # To show test set accuracy during training\n",
    "\n",
    "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
    "print(X_train.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
    "print(\"\\nThe dataset has not been preprocessed, is not normalised etc\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LSTM_RNN(_X, _weights, _biases):\n",
    "    # model architecture based on \"guillaume-chevalier\" and \"aymericdamien\" under the MIT license.\n",
    "\n",
    "    _X = tf.transpose(_X, [1, 0, 2])  # permute n_steps and batch_size\n",
    "    _X = tf.reshape(_X, [-1, n_input])   \n",
    "    # Rectifies Linear Unit activation function used\n",
    "    _X = tf.nn.relu(tf.matmul(_X, _weights['hidden']) + _biases['hidden'])\n",
    "    # Split data because rnn cell needs a list of inputs for the RNN inner loop\n",
    "    _X = tf.split(_X, n_steps, 0) \n",
    "\n",
    "    # Define two stacked LSTM cells (two recurrent layers deep) with tensorflow\n",
    "    lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(n_hidden, forget_bias=1.0, state_is_tuple=True)\n",
    "    lstm_cells = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2], state_is_tuple=True)\n",
    "    outputs, states = tf.contrib.rnn.static_rnn(lstm_cells, _X, dtype=tf.float32)\n",
    "\n",
    "    # A single output is produced, in style of \"many to one\" classifier, refer to http://karpathy.github.io/2015/05/21/rnn-effectiveness/ for details\n",
    "    lstm_last_output = outputs[-1]\n",
    "    \n",
    "    # Linear activation\n",
    "    return tf.matmul(lstm_last_output, _weights['out']) + _biases['out']\n",
    "\n",
    "\n",
    "def extract_batch_size(_train, _labels, _unsampled, batch_size):\n",
    "    # Fetch a \"batch_size\" amount of data and labels from \"(X|y)_train\" data. \n",
    "    # Elements of each batch are chosen randomly, without replacement, from X_train with corresponding label from Y_train\n",
    "    # unsampled_indices keeps track of sampled data ensuring non-replacement. Resets when remaining datapoints < batch_size    \n",
    "    \n",
    "    shape = list(_train.shape)\n",
    "    shape[0] = batch_size\n",
    "    batch_s = np.empty(shape)\n",
    "    batch_labels = np.empty((batch_size,1)) \n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Loop index\n",
    "        # index = random sample from _unsampled (indices)\n",
    "        index = random.choice(_unsampled)\n",
    "        batch_s[i] = _train[index] \n",
    "        batch_labels[i] = _labels[index]\n",
    "        \n",
    "        _unsampled = list(_unsampled)\n",
    "        \n",
    "        _unsampled.remove(index)\n",
    "\n",
    "\n",
    "    return batch_s, batch_labels, _unsampled\n",
    "\n",
    "\n",
    "def one_hot(y_):\n",
    "    # One hot encoding of the network outputs\n",
    "    # e.g.: [[5], [0], [3]] --> [[0, 0, 0, 0, 0, 1], [1, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0]]\n",
    "    \n",
    "    y_ = y_.reshape(len(y_))\n",
    "    n_values = int(np.max(y_)) + 1\n",
    "    return np.eye(n_values)[np.array(y_, dtype=np.int32)]  # Returns FLOATS\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for _ in range(3):\n",
    "#     tf.reset_default_graph()\n",
    "#     var = tf.Variable(0)\n",
    "#     with tf.Session() as session:\n",
    "#         session.run(tf.global_variables_initializer())\n",
    "#         print(len(session.graph._nodes_by_name.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-488cfd9da3d0>:12: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is deprecated, please use tf.nn.rnn_cell.LSTMCell, which supports all the feature this cell currently has. Please replace the existing code with tf.nn.rnn_cell.LSTMCell(name='basic_lstm_cell').\n",
      "WARNING:tensorflow:From <ipython-input-7-982ce1458cc4>:22: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Graph input/output\n",
    "x = tf.placeholder(tf.float32, [None, n_steps, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "\n",
    "# Graph weights\n",
    "weights = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_input, n_hidden])), # Hidden layer weights\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden, n_classes], mean=1.0))\n",
    "}\n",
    "biases = {\n",
    "    'hidden': tf.Variable(tf.random_normal([n_hidden])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = LSTM_RNN(x, weights, biases)\n",
    "\n",
    "# Loss, optimizer and evaluation\n",
    "l2 = lambda_loss_amount * sum(\n",
    "    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
    ") # L2 loss prevents this overkill neural network to overfit the data\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred)) + l2 # Softmax loss\n",
    "if decaying_learning_rate:\n",
    "    learning_rate = tf.train.exponential_decay(init_learning_rate, global_step*batch_size, decay_steps, decay_rate, staircase=True)\n",
    "\n",
    "\n",
    "#decayed_learning_rate = learning_rate * decay_rate ^ (global_step / decay_steps) #exponentially decayed learning rate\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost,global_step=global_step) # Adam Optimizer\n",
    "\n",
    "# correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'out': <tf.Variable 'Variable_4:0' shape=(4,) dtype=float32_ref>, 'hidden': <tf.Variable 'Variable_3:0' shape=(34,) dtype=float32_ref>}\n",
      "<tf.Variable 'Variable_1:0' shape=(36, 34) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "print(biases)\n",
    "print(weights['hidden'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if decaying_learning_rate:\n",
    "#     learning_rate = tf.train.exponential_decay(init_learning_rate, global_step*batch_size, decay_steps, decay_rate, staircase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_losses = []\n",
    "test_accuracies = []\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "sess = tf.InteractiveSession(config=tf.ConfigProto(log_device_placement=True))\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training_iters = training_data_count *120\n",
    "# training_iters = training_data_count *5120\n",
    "# training_iters = training_data_count *2560\n",
    "training_iters = training_data_count *1024\n",
    "\n",
    "#create saver before training\n",
    "# saver = tf.train.Saver()\n",
    "saver = tf.train.Saver(var_list={'wh':weights['hidden'], 'wo':weights['out'], 'bh':biases['hidden'], 'bo':biases['out']})\n",
    "load = False\n",
    "train = True\n",
    "update = True\n",
    "\n",
    "#check if you want to retrain or import a saved model\n",
    "if load:\n",
    "    saver.restore(sess, DATASET_PATH + \"model.ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #60:  Learning rate = 0.005000:   Batch Loss = 3.000730, Accuracy = 0.3333333432674408\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.6848299503326416, Accuracy = 0.3495049476623535\n",
      "Iter #480:  Learning rate = 0.005000:   Batch Loss = 2.532169, Accuracy = 0.3499999940395355\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.5582127571105957, Accuracy = 0.3386138677597046\n",
      "Iter #960:  Learning rate = 0.005000:   Batch Loss = 2.488724, Accuracy = 0.44999998807907104\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.4342355728149414, Accuracy = 0.4801980257034302\n",
      "Iter #1440:  Learning rate = 0.005000:   Batch Loss = 2.330377, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.3621773719787598, Accuracy = 0.4356435537338257\n",
      "Iter #1920:  Learning rate = 0.005000:   Batch Loss = 2.323692, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.3205325603485107, Accuracy = 0.49405941367149353\n",
      "Iter #2400:  Learning rate = 0.005000:   Batch Loss = 2.176347, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.2641468048095703, Accuracy = 0.48514851927757263\n",
      "Iter #2880:  Learning rate = 0.005000:   Batch Loss = 2.521882, Accuracy = 0.4166666567325592\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.439216375350952, Accuracy = 0.4663366377353668\n",
      "Iter #3360:  Learning rate = 0.005000:   Batch Loss = 2.471199, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.3626961708068848, Accuracy = 0.447524756193161\n",
      "Iter #3840:  Learning rate = 0.005000:   Batch Loss = 2.265532, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.242063045501709, Accuracy = 0.500990092754364\n",
      "Iter #4320:  Learning rate = 0.005000:   Batch Loss = 2.163260, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.19743275642395, Accuracy = 0.501980185508728\n",
      "Iter #4800:  Learning rate = 0.005000:   Batch Loss = 2.074883, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.174170970916748, Accuracy = 0.5465346574783325\n",
      "Iter #5280:  Learning rate = 0.005000:   Batch Loss = 2.151426, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.197211980819702, Accuracy = 0.5188118815422058\n",
      "Iter #5760:  Learning rate = 0.005000:   Batch Loss = 2.298993, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.1521618366241455, Accuracy = 0.5168316960334778\n",
      "Iter #6240:  Learning rate = 0.005000:   Batch Loss = 2.136383, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.143472671508789, Accuracy = 0.5207920670509338\n",
      "Iter #6720:  Learning rate = 0.005000:   Batch Loss = 1.997211, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.097026824951172, Accuracy = 0.5415841341018677\n",
      "Iter #7200:  Learning rate = 0.005000:   Batch Loss = 2.133913, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.1135358810424805, Accuracy = 0.49504950642585754\n",
      "Iter #7680:  Learning rate = 0.005000:   Batch Loss = 2.100670, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.066786766052246, Accuracy = 0.5475247502326965\n",
      "Iter #8160:  Learning rate = 0.005000:   Batch Loss = 2.131417, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.087374687194824, Accuracy = 0.5584158301353455\n",
      "Iter #8640:  Learning rate = 0.005000:   Batch Loss = 2.126300, Accuracy = 0.4166666567325592\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.1873226165771484, Accuracy = 0.44356435537338257\n",
      "Iter #9120:  Learning rate = 0.005000:   Batch Loss = 2.142734, Accuracy = 0.44999998807907104\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.096440315246582, Accuracy = 0.5049505233764648\n",
      "Iter #9600:  Learning rate = 0.005000:   Batch Loss = 2.092747, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.0746681690216064, Accuracy = 0.5237624049186707\n",
      "Iter #10080:  Learning rate = 0.005000:   Batch Loss = 1.857753, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.0536999702453613, Accuracy = 0.5297029614448547\n",
      "Iter #10560:  Learning rate = 0.005000:   Batch Loss = 1.947291, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.015436887741089, Accuracy = 0.5455445647239685\n",
      "Iter #11040:  Learning rate = 0.005000:   Batch Loss = 1.950013, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.02091383934021, Accuracy = 0.5297029614448547\n",
      "Iter #11520:  Learning rate = 0.005000:   Batch Loss = 2.022724, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.037625789642334, Accuracy = 0.5198019742965698\n",
      "Iter #12000:  Learning rate = 0.005000:   Batch Loss = 2.010003, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.0774755477905273, Accuracy = 0.4633663296699524\n",
      "Iter #12480:  Learning rate = 0.005000:   Batch Loss = 1.960906, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.0271289348602295, Accuracy = 0.5049505233764648\n",
      "Iter #12960:  Learning rate = 0.005000:   Batch Loss = 1.966700, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.1050939559936523, Accuracy = 0.4128713011741638\n",
      "Iter #13440:  Learning rate = 0.005000:   Batch Loss = 2.108853, Accuracy = 0.4333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.118804693222046, Accuracy = 0.47623762488365173\n",
      "Iter #13920:  Learning rate = 0.005000:   Batch Loss = 1.949546, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.0270843505859375, Accuracy = 0.498019814491272\n",
      "Iter #14400:  Learning rate = 0.005000:   Batch Loss = 1.902223, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 2.0202412605285645, Accuracy = 0.49702969193458557\n",
      "Iter #14880:  Learning rate = 0.005000:   Batch Loss = 1.906073, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9789190292358398, Accuracy = 0.5346534848213196\n",
      "Iter #15360:  Learning rate = 0.005000:   Batch Loss = 1.814360, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9364863634109497, Accuracy = 0.5485148429870605\n",
      "Iter #15840:  Learning rate = 0.005000:   Batch Loss = 2.055125, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9531902074813843, Accuracy = 0.5217821598052979\n",
      "Iter #16320:  Learning rate = 0.005000:   Batch Loss = 1.981408, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.96315336227417, Accuracy = 0.5435643792152405\n",
      "Iter #16800:  Learning rate = 0.005000:   Batch Loss = 2.104590, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9613595008850098, Accuracy = 0.5158416032791138\n",
      "Iter #17280:  Learning rate = 0.005000:   Batch Loss = 1.869209, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.934711217880249, Accuracy = 0.5267326831817627\n",
      "Iter #17760:  Learning rate = 0.005000:   Batch Loss = 2.014728, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9144015312194824, Accuracy = 0.5415841341018677\n",
      "Iter #18240:  Learning rate = 0.005000:   Batch Loss = 1.949815, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9026939868927002, Accuracy = 0.5326732397079468\n",
      "Iter #18720:  Learning rate = 0.005000:   Batch Loss = 1.893166, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8742332458496094, Accuracy = 0.5653465390205383\n",
      "Iter #19200:  Learning rate = 0.005000:   Batch Loss = 1.904442, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.865267038345337, Accuracy = 0.5712871551513672\n",
      "Iter #19680:  Learning rate = 0.005000:   Batch Loss = 1.864997, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.928280234336853, Accuracy = 0.5247524976730347\n",
      "Iter #20160:  Learning rate = 0.005000:   Batch Loss = 1.923519, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9141514301300049, Accuracy = 0.5257425904273987\n",
      "Iter #20640:  Learning rate = 0.005000:   Batch Loss = 1.756666, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8611736297607422, Accuracy = 0.5554455518722534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #21120:  Learning rate = 0.005000:   Batch Loss = 1.685537, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8479132652282715, Accuracy = 0.5534653663635254\n",
      "Iter #21600:  Learning rate = 0.005000:   Batch Loss = 1.811218, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.845954418182373, Accuracy = 0.5594059228897095\n",
      "Iter #22080:  Learning rate = 0.005000:   Batch Loss = 1.952353, Accuracy = 0.4166666567325592\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9013092517852783, Accuracy = 0.48613861203193665\n",
      "Iter #22560:  Learning rate = 0.005000:   Batch Loss = 1.816046, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8780593872070312, Accuracy = 0.5198019742965698\n",
      "Iter #23040:  Learning rate = 0.005000:   Batch Loss = 1.729033, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.850522756576538, Accuracy = 0.5633663535118103\n",
      "Iter #23520:  Learning rate = 0.005000:   Batch Loss = 1.862913, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8331985473632812, Accuracy = 0.5712871551513672\n",
      "Iter #24000:  Learning rate = 0.005000:   Batch Loss = 1.772641, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8218085765838623, Accuracy = 0.5673267245292664\n",
      "Iter #24480:  Learning rate = 0.005000:   Batch Loss = 1.834276, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7844979763031006, Accuracy = 0.5752475261688232\n",
      "Iter #24960:  Learning rate = 0.005000:   Batch Loss = 1.863328, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8072102069854736, Accuracy = 0.5594059228897095\n",
      "Iter #25440:  Learning rate = 0.005000:   Batch Loss = 1.680660, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8068337440490723, Accuracy = 0.5772277116775513\n",
      "Iter #25920:  Learning rate = 0.005000:   Batch Loss = 1.639835, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8098464012145996, Accuracy = 0.5455445647239685\n",
      "Iter #26400:  Learning rate = 0.005000:   Batch Loss = 1.867079, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8226962089538574, Accuracy = 0.5475247502326965\n",
      "Iter #26880:  Learning rate = 0.005000:   Batch Loss = 1.993261, Accuracy = 0.4000000059604645\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9052730798721313, Accuracy = 0.512871265411377\n",
      "Iter #27360:  Learning rate = 0.005000:   Batch Loss = 1.841824, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.832160234451294, Accuracy = 0.5316831469535828\n",
      "Iter #27840:  Learning rate = 0.005000:   Batch Loss = 1.756680, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8366098403930664, Accuracy = 0.5168316960334778\n",
      "Iter #28320:  Learning rate = 0.005000:   Batch Loss = 1.801558, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8283123970031738, Accuracy = 0.5059406161308289\n",
      "Iter #28800:  Learning rate = 0.005000:   Batch Loss = 1.669872, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8359267711639404, Accuracy = 0.5118811726570129\n",
      "Iter #29280:  Learning rate = 0.005000:   Batch Loss = 1.923047, Accuracy = 0.4166666567325592\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7910957336425781, Accuracy = 0.5574257373809814\n",
      "Iter #29760:  Learning rate = 0.005000:   Batch Loss = 1.724499, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8293862342834473, Accuracy = 0.5188118815422058\n",
      "Iter #30240:  Learning rate = 0.005000:   Batch Loss = 1.878880, Accuracy = 0.4333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8087329864501953, Accuracy = 0.5554455518722534\n",
      "Iter #30720:  Learning rate = 0.005000:   Batch Loss = 1.927789, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7757806777954102, Accuracy = 0.5524752736091614\n",
      "Iter #31200:  Learning rate = 0.005000:   Batch Loss = 1.611903, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7459461688995361, Accuracy = 0.5841584205627441\n",
      "Iter #31680:  Learning rate = 0.005000:   Batch Loss = 1.544096, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7589170932769775, Accuracy = 0.5603960156440735\n",
      "Iter #32160:  Learning rate = 0.005000:   Batch Loss = 1.711863, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7676764726638794, Accuracy = 0.5366336703300476\n",
      "Iter #32640:  Learning rate = 0.005000:   Batch Loss = 1.532550, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7330681085586548, Accuracy = 0.5613861680030823\n",
      "Iter #33120:  Learning rate = 0.005000:   Batch Loss = 1.718117, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7462494373321533, Accuracy = 0.5544554591178894\n",
      "Iter #33600:  Learning rate = 0.005000:   Batch Loss = 1.629826, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7302650213241577, Accuracy = 0.5564356446266174\n",
      "Iter #34080:  Learning rate = 0.005000:   Batch Loss = 1.669148, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.743517279624939, Accuracy = 0.5693069100379944\n",
      "Iter #34560:  Learning rate = 0.005000:   Batch Loss = 1.737151, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7615883350372314, Accuracy = 0.500990092754364\n",
      "Iter #35040:  Learning rate = 0.005000:   Batch Loss = 1.577225, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7674312591552734, Accuracy = 0.5217821598052979\n",
      "Iter #35520:  Learning rate = 0.005000:   Batch Loss = 1.790622, Accuracy = 0.44999998807907104\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8076322078704834, Accuracy = 0.46138614416122437\n",
      "Iter #36000:  Learning rate = 0.005000:   Batch Loss = 1.965681, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.9586834907531738, Accuracy = 0.44158416986465454\n",
      "Iter #36480:  Learning rate = 0.005000:   Batch Loss = 1.619745, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.729943037033081, Accuracy = 0.5168316960334778\n",
      "Iter #36960:  Learning rate = 0.005000:   Batch Loss = 1.858617, Accuracy = 0.4000000059604645\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.8338234424591064, Accuracy = 0.42574256658554077\n",
      "Iter #37440:  Learning rate = 0.005000:   Batch Loss = 1.746658, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7861371040344238, Accuracy = 0.4752475321292877\n",
      "Iter #37920:  Learning rate = 0.005000:   Batch Loss = 1.682493, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7611043453216553, Accuracy = 0.5277227759361267\n",
      "Iter #38400:  Learning rate = 0.005000:   Batch Loss = 1.757549, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7472835779190063, Accuracy = 0.5188118815422058\n",
      "Iter #38880:  Learning rate = 0.005000:   Batch Loss = 1.714242, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7376707792282104, Accuracy = 0.5574257373809814\n",
      "Iter #39360:  Learning rate = 0.005000:   Batch Loss = 1.715752, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7533135414123535, Accuracy = 0.49405941367149353\n",
      "Iter #39840:  Learning rate = 0.005000:   Batch Loss = 1.777150, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.741189956665039, Accuracy = 0.5475247502326965\n",
      "Iter #40320:  Learning rate = 0.005000:   Batch Loss = 1.774464, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.728562593460083, Accuracy = 0.5425742864608765\n",
      "Iter #40800:  Learning rate = 0.005000:   Batch Loss = 1.656187, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7238695621490479, Accuracy = 0.5267326831817627\n",
      "Iter #41280:  Learning rate = 0.005000:   Batch Loss = 1.708540, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.698322057723999, Accuracy = 0.5514851212501526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #41760:  Learning rate = 0.005000:   Batch Loss = 1.687213, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.722165584564209, Accuracy = 0.5188118815422058\n",
      "Iter #42240:  Learning rate = 0.005000:   Batch Loss = 1.657632, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.737539529800415, Accuracy = 0.5435643792152405\n",
      "Iter #42720:  Learning rate = 0.005000:   Batch Loss = 1.731739, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7639024257659912, Accuracy = 0.502970278263092\n",
      "Iter #43200:  Learning rate = 0.005000:   Batch Loss = 1.773436, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.7606180906295776, Accuracy = 0.4920791983604431\n",
      "Iter #43680:  Learning rate = 0.005000:   Batch Loss = 1.733030, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6946486234664917, Accuracy = 0.5554455518722534\n",
      "Iter #44160:  Learning rate = 0.005000:   Batch Loss = 1.662331, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6617670059204102, Accuracy = 0.5564356446266174\n",
      "Iter #44640:  Learning rate = 0.005000:   Batch Loss = 1.801137, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6481413841247559, Accuracy = 0.5495049357414246\n",
      "Iter #45120:  Learning rate = 0.005000:   Batch Loss = 1.571698, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.618854284286499, Accuracy = 0.5960395932197571\n",
      "Iter #45600:  Learning rate = 0.005000:   Batch Loss = 1.661832, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6688201427459717, Accuracy = 0.5326732397079468\n",
      "Iter #46080:  Learning rate = 0.005000:   Batch Loss = 1.552457, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6781808137893677, Accuracy = 0.5257425904273987\n",
      "Iter #46560:  Learning rate = 0.005000:   Batch Loss = 1.711518, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6806213855743408, Accuracy = 0.5475247502326965\n",
      "Iter #47040:  Learning rate = 0.005000:   Batch Loss = 1.542300, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6648211479187012, Accuracy = 0.5445544719696045\n",
      "Iter #47520:  Learning rate = 0.005000:   Batch Loss = 1.578122, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6522704362869263, Accuracy = 0.5534653663635254\n",
      "Iter #48000:  Learning rate = 0.005000:   Batch Loss = 1.747991, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6532514095306396, Accuracy = 0.5207920670509338\n",
      "Iter #48480:  Learning rate = 0.005000:   Batch Loss = 1.801936, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.618713140487671, Accuracy = 0.5673267245292664\n",
      "Iter #48960:  Learning rate = 0.005000:   Batch Loss = 1.676660, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6385643482208252, Accuracy = 0.5495049357414246\n",
      "Iter #49440:  Learning rate = 0.005000:   Batch Loss = 1.536050, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.583611011505127, Accuracy = 0.591089129447937\n",
      "Iter #49920:  Learning rate = 0.005000:   Batch Loss = 1.634272, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6926815509796143, Accuracy = 0.5396039485931396\n",
      "Iter #50400:  Learning rate = 0.005000:   Batch Loss = 1.731652, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6624956130981445, Accuracy = 0.4653465449810028\n",
      "Iter #50880:  Learning rate = 0.005000:   Batch Loss = 1.541166, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6518728733062744, Accuracy = 0.5366336703300476\n",
      "Iter #51360:  Learning rate = 0.005000:   Batch Loss = 1.626436, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6682939529418945, Accuracy = 0.5108910799026489\n",
      "Iter #51840:  Learning rate = 0.005000:   Batch Loss = 1.695593, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6473801136016846, Accuracy = 0.501980185508728\n",
      "Iter #52320:  Learning rate = 0.005000:   Batch Loss = 1.661907, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6107590198516846, Accuracy = 0.5376237630844116\n",
      "Iter #52800:  Learning rate = 0.005000:   Batch Loss = 1.520878, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6135189533233643, Accuracy = 0.5534653663635254\n",
      "Iter #53280:  Learning rate = 0.005000:   Batch Loss = 1.646706, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6191338300704956, Accuracy = 0.5613861680030823\n",
      "Iter #53760:  Learning rate = 0.005000:   Batch Loss = 1.437476, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6273279190063477, Accuracy = 0.5554455518722534\n",
      "Iter #54240:  Learning rate = 0.005000:   Batch Loss = 1.522872, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6001977920532227, Accuracy = 0.5435643792152405\n",
      "Iter #54720:  Learning rate = 0.005000:   Batch Loss = 1.590474, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.631012201309204, Accuracy = 0.5564356446266174\n",
      "Iter #55200:  Learning rate = 0.005000:   Batch Loss = 1.631852, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6267342567443848, Accuracy = 0.5356435775756836\n",
      "Iter #55680:  Learning rate = 0.005000:   Batch Loss = 1.650144, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.63450288772583, Accuracy = 0.5207920670509338\n",
      "Iter #56160:  Learning rate = 0.005000:   Batch Loss = 1.500049, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6292294263839722, Accuracy = 0.5356435775756836\n",
      "Iter #56640:  Learning rate = 0.005000:   Batch Loss = 1.737554, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6361993551254272, Accuracy = 0.5346534848213196\n",
      "Iter #57120:  Learning rate = 0.005000:   Batch Loss = 1.682664, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.631094217300415, Accuracy = 0.5326732397079468\n",
      "Iter #57600:  Learning rate = 0.005000:   Batch Loss = 1.548983, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6312143802642822, Accuracy = 0.5544554591178894\n",
      "Iter #58080:  Learning rate = 0.005000:   Batch Loss = 1.801063, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6332042217254639, Accuracy = 0.5415841341018677\n",
      "Iter #58560:  Learning rate = 0.005000:   Batch Loss = 1.524832, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6510035991668701, Accuracy = 0.5237624049186707\n",
      "Iter #59040:  Learning rate = 0.005000:   Batch Loss = 1.766685, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6085368394851685, Accuracy = 0.5297029614448547\n",
      "Iter #59520:  Learning rate = 0.005000:   Batch Loss = 1.671864, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.601154088973999, Accuracy = 0.5198019742965698\n",
      "Iter #60000:  Learning rate = 0.005000:   Batch Loss = 1.534434, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5840598344802856, Accuracy = 0.5306930541992188\n",
      "Iter #60480:  Learning rate = 0.005000:   Batch Loss = 1.637966, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6119855642318726, Accuracy = 0.5267326831817627\n",
      "Iter #60960:  Learning rate = 0.005000:   Batch Loss = 1.660996, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5863170623779297, Accuracy = 0.5455445647239685\n",
      "Iter #61440:  Learning rate = 0.005000:   Batch Loss = 1.444888, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5785670280456543, Accuracy = 0.5435643792152405\n",
      "Iter #61920:  Learning rate = 0.005000:   Batch Loss = 1.506834, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.527550458908081, Accuracy = 0.5782178044319153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #62400:  Learning rate = 0.005000:   Batch Loss = 1.543455, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.511366605758667, Accuracy = 0.5811881422996521\n",
      "Iter #62880:  Learning rate = 0.005000:   Batch Loss = 1.378011, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5035066604614258, Accuracy = 0.5722772479057312\n",
      "Iter #63360:  Learning rate = 0.005000:   Batch Loss = 1.535634, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5338695049285889, Accuracy = 0.5732673406600952\n",
      "Iter #63840:  Learning rate = 0.005000:   Batch Loss = 1.371154, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5081298351287842, Accuracy = 0.5831683278083801\n",
      "Iter #64320:  Learning rate = 0.005000:   Batch Loss = 1.508483, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5412629842758179, Accuracy = 0.5326732397079468\n",
      "Iter #64800:  Learning rate = 0.005000:   Batch Loss = 1.732083, Accuracy = 0.4000000059604645\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.732560396194458, Accuracy = 0.4574257433414459\n",
      "Iter #65280:  Learning rate = 0.005000:   Batch Loss = 1.560120, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6018743515014648, Accuracy = 0.5396039485931396\n",
      "Iter #65760:  Learning rate = 0.005000:   Batch Loss = 1.678478, Accuracy = 0.44999998807907104\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.6061803102493286, Accuracy = 0.5099009871482849\n",
      "Iter #66240:  Learning rate = 0.005000:   Batch Loss = 1.485577, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5770922899246216, Accuracy = 0.5118811726570129\n",
      "Iter #66720:  Learning rate = 0.005000:   Batch Loss = 1.525560, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5658612251281738, Accuracy = 0.48613861203193665\n",
      "Iter #67200:  Learning rate = 0.005000:   Batch Loss = 1.524263, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5799657106399536, Accuracy = 0.5346534848213196\n",
      "Iter #67680:  Learning rate = 0.005000:   Batch Loss = 1.516900, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5390545129776, Accuracy = 0.5316831469535828\n",
      "Iter #68160:  Learning rate = 0.005000:   Batch Loss = 1.443308, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5389602184295654, Accuracy = 0.5158416032791138\n",
      "Iter #68640:  Learning rate = 0.005000:   Batch Loss = 1.486461, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5614477396011353, Accuracy = 0.5089108943939209\n",
      "Iter #69120:  Learning rate = 0.005000:   Batch Loss = 1.423634, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5445542335510254, Accuracy = 0.5435643792152405\n",
      "Iter #69600:  Learning rate = 0.005000:   Batch Loss = 1.537099, Accuracy = 0.4166666567325592\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5448952913284302, Accuracy = 0.5475247502326965\n",
      "Iter #70080:  Learning rate = 0.005000:   Batch Loss = 1.551354, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5368391275405884, Accuracy = 0.5594059228897095\n",
      "Iter #70560:  Learning rate = 0.005000:   Batch Loss = 1.499691, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5164642333984375, Accuracy = 0.5455445647239685\n",
      "Iter #71040:  Learning rate = 0.005000:   Batch Loss = 1.480839, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.508312463760376, Accuracy = 0.5376237630844116\n",
      "Iter #71520:  Learning rate = 0.005000:   Batch Loss = 1.424737, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4985140562057495, Accuracy = 0.5564356446266174\n",
      "Iter #72000:  Learning rate = 0.005000:   Batch Loss = 1.367277, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4896459579467773, Accuracy = 0.5495049357414246\n",
      "Iter #72480:  Learning rate = 0.005000:   Batch Loss = 1.479076, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5076768398284912, Accuracy = 0.5465346574783325\n",
      "Iter #72960:  Learning rate = 0.005000:   Batch Loss = 1.368345, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.513972282409668, Accuracy = 0.5297029614448547\n",
      "Iter #73440:  Learning rate = 0.005000:   Batch Loss = 1.592904, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4994807243347168, Accuracy = 0.5584158301353455\n",
      "Iter #73920:  Learning rate = 0.005000:   Batch Loss = 1.461703, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.49997878074646, Accuracy = 0.5544554591178894\n",
      "Iter #74400:  Learning rate = 0.005000:   Batch Loss = 1.457007, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4835503101348877, Accuracy = 0.5435643792152405\n",
      "Iter #74880:  Learning rate = 0.005000:   Batch Loss = 1.577030, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4635002613067627, Accuracy = 0.5693069100379944\n",
      "Iter #75360:  Learning rate = 0.005000:   Batch Loss = 1.358697, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4715410470962524, Accuracy = 0.5633663535118103\n",
      "Iter #75840:  Learning rate = 0.005000:   Batch Loss = 1.446658, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.462000846862793, Accuracy = 0.5772277116775513\n",
      "Iter #76320:  Learning rate = 0.005000:   Batch Loss = 1.512759, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4870151281356812, Accuracy = 0.5584158301353455\n",
      "Iter #76800:  Learning rate = 0.005000:   Batch Loss = 1.386644, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.480175495147705, Accuracy = 0.5702970027923584\n",
      "Iter #77280:  Learning rate = 0.005000:   Batch Loss = 1.438948, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5420608520507812, Accuracy = 0.5425742864608765\n",
      "Iter #77760:  Learning rate = 0.005000:   Batch Loss = 1.498868, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5007102489471436, Accuracy = 0.5663366317749023\n",
      "Iter #78240:  Learning rate = 0.005000:   Batch Loss = 1.527703, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4905083179473877, Accuracy = 0.5633663535118103\n",
      "Iter #78720:  Learning rate = 0.005000:   Batch Loss = 1.475292, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4966282844543457, Accuracy = 0.5584158301353455\n",
      "Iter #79200:  Learning rate = 0.005000:   Batch Loss = 1.524686, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.487337350845337, Accuracy = 0.5584158301353455\n",
      "Iter #79680:  Learning rate = 0.005000:   Batch Loss = 1.496627, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5470564365386963, Accuracy = 0.5405940413475037\n",
      "Iter #80160:  Learning rate = 0.005000:   Batch Loss = 1.524134, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5006409883499146, Accuracy = 0.5148515105247498\n",
      "Iter #80640:  Learning rate = 0.005000:   Batch Loss = 1.661928, Accuracy = 0.4333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.514434814453125, Accuracy = 0.5356435775756836\n",
      "Iter #81120:  Learning rate = 0.005000:   Batch Loss = 1.467365, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5481441020965576, Accuracy = 0.5198019742965698\n",
      "Iter #81600:  Learning rate = 0.005000:   Batch Loss = 1.495567, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4973118305206299, Accuracy = 0.5277227759361267\n",
      "Iter #82080:  Learning rate = 0.005000:   Batch Loss = 1.445461, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4997209310531616, Accuracy = 0.5207920670509338\n",
      "Iter #82560:  Learning rate = 0.005000:   Batch Loss = 1.631551, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5018446445465088, Accuracy = 0.5316831469535828\n",
      "Iter #83040:  Learning rate = 0.005000:   Batch Loss = 1.588320, Accuracy = 0.4166666567325592\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4660038948059082, Accuracy = 0.5168316960334778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #83520:  Learning rate = 0.005000:   Batch Loss = 1.452160, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4731948375701904, Accuracy = 0.5356435775756836\n",
      "Iter #84000:  Learning rate = 0.005000:   Batch Loss = 1.292314, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4577064514160156, Accuracy = 0.5613861680030823\n",
      "Iter #84480:  Learning rate = 0.005000:   Batch Loss = 1.473793, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4465138912200928, Accuracy = 0.5643564462661743\n",
      "Iter #84960:  Learning rate = 0.005000:   Batch Loss = 1.259652, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4454946517944336, Accuracy = 0.5495049357414246\n",
      "Iter #85440:  Learning rate = 0.005000:   Batch Loss = 1.333691, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.487476110458374, Accuracy = 0.5118811726570129\n",
      "Iter #85920:  Learning rate = 0.005000:   Batch Loss = 1.691268, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4962186813354492, Accuracy = 0.5227722525596619\n",
      "Iter #86400:  Learning rate = 0.005000:   Batch Loss = 1.484791, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4872034788131714, Accuracy = 0.5396039485931396\n",
      "Iter #86880:  Learning rate = 0.005000:   Batch Loss = 1.510946, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.487471580505371, Accuracy = 0.5425742864608765\n",
      "Iter #87360:  Learning rate = 0.005000:   Batch Loss = 1.570041, Accuracy = 0.44999998807907104\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4698536396026611, Accuracy = 0.5188118815422058\n",
      "Iter #87840:  Learning rate = 0.005000:   Batch Loss = 1.463575, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4319862127304077, Accuracy = 0.5514851212501526\n",
      "Iter #88320:  Learning rate = 0.005000:   Batch Loss = 1.394190, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4410325288772583, Accuracy = 0.5366336703300476\n",
      "Iter #88800:  Learning rate = 0.005000:   Batch Loss = 1.593982, Accuracy = 0.4000000059604645\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4143426418304443, Accuracy = 0.5485148429870605\n",
      "Iter #89280:  Learning rate = 0.005000:   Batch Loss = 1.316428, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4180076122283936, Accuracy = 0.5623762607574463\n",
      "Iter #89760:  Learning rate = 0.005000:   Batch Loss = 1.428675, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4162405729293823, Accuracy = 0.5534653663635254\n",
      "Iter #90240:  Learning rate = 0.005000:   Batch Loss = 1.293953, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4159958362579346, Accuracy = 0.5653465390205383\n",
      "Iter #90720:  Learning rate = 0.005000:   Batch Loss = 1.316651, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.429710030555725, Accuracy = 0.5841584205627441\n",
      "Iter #91200:  Learning rate = 0.005000:   Batch Loss = 1.497416, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4473797082901, Accuracy = 0.5257425904273987\n",
      "Iter #91680:  Learning rate = 0.005000:   Batch Loss = 1.392048, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4189504384994507, Accuracy = 0.5851485133171082\n",
      "Iter #92160:  Learning rate = 0.005000:   Batch Loss = 1.497621, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4244270324707031, Accuracy = 0.5435643792152405\n",
      "Iter #92640:  Learning rate = 0.005000:   Batch Loss = 1.367433, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4064924716949463, Accuracy = 0.5485148429870605\n",
      "Iter #93120:  Learning rate = 0.005000:   Batch Loss = 1.515573, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4065386056900024, Accuracy = 0.5475247502326965\n",
      "Iter #93600:  Learning rate = 0.005000:   Batch Loss = 1.421794, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.405409336090088, Accuracy = 0.5475247502326965\n",
      "Iter #94080:  Learning rate = 0.005000:   Batch Loss = 1.538449, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4014203548431396, Accuracy = 0.5653465390205383\n",
      "Iter #94560:  Learning rate = 0.005000:   Batch Loss = 1.453197, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3981883525848389, Accuracy = 0.5702970027923584\n",
      "Iter #95040:  Learning rate = 0.005000:   Batch Loss = 1.330865, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3978444337844849, Accuracy = 0.5762376189231873\n",
      "Iter #95520:  Learning rate = 0.005000:   Batch Loss = 1.411728, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.380008339881897, Accuracy = 0.5831683278083801\n",
      "Iter #96000:  Learning rate = 0.005000:   Batch Loss = 1.264326, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3679585456848145, Accuracy = 0.5594059228897095\n",
      "Iter #96480:  Learning rate = 0.005000:   Batch Loss = 1.400431, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3647477626800537, Accuracy = 0.5455445647239685\n",
      "Iter #96960:  Learning rate = 0.005000:   Batch Loss = 1.335002, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3815464973449707, Accuracy = 0.5544554591178894\n",
      "Iter #97440:  Learning rate = 0.005000:   Batch Loss = 1.221590, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3527971506118774, Accuracy = 0.5722772479057312\n",
      "Iter #97920:  Learning rate = 0.005000:   Batch Loss = 1.235922, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3811626434326172, Accuracy = 0.5732673406600952\n",
      "Iter #98400:  Learning rate = 0.005000:   Batch Loss = 1.350547, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3846410512924194, Accuracy = 0.5841584205627441\n",
      "Iter #98880:  Learning rate = 0.005000:   Batch Loss = 1.307338, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3633308410644531, Accuracy = 0.5653465390205383\n",
      "Iter #99360:  Learning rate = 0.005000:   Batch Loss = 1.342689, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3680962324142456, Accuracy = 0.5584158301353455\n",
      "Iter #99840:  Learning rate = 0.005000:   Batch Loss = 1.374912, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3682407140731812, Accuracy = 0.5584158301353455\n",
      "Iter #100320:  Learning rate = 0.004800:   Batch Loss = 1.364924, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3797768354415894, Accuracy = 0.5653465390205383\n",
      "Iter #100800:  Learning rate = 0.004800:   Batch Loss = 1.163163, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4098105430603027, Accuracy = 0.5376237630844116\n",
      "Iter #101280:  Learning rate = 0.004800:   Batch Loss = 1.435555, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3784832954406738, Accuracy = 0.5524752736091614\n",
      "Iter #101760:  Learning rate = 0.004800:   Batch Loss = 1.474806, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3815932273864746, Accuracy = 0.5415841341018677\n",
      "Iter #102240:  Learning rate = 0.004800:   Batch Loss = 1.248665, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3801586627960205, Accuracy = 0.5514851212501526\n",
      "Iter #102720:  Learning rate = 0.004800:   Batch Loss = 1.411693, Accuracy = 0.44999998807907104\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3602524995803833, Accuracy = 0.5831683278083801\n",
      "Iter #103200:  Learning rate = 0.004800:   Batch Loss = 1.361562, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3620471954345703, Accuracy = 0.5663366317749023\n",
      "Iter #103680:  Learning rate = 0.004800:   Batch Loss = 1.375683, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3418715000152588, Accuracy = 0.5752475261688232\n",
      "Iter #104160:  Learning rate = 0.004800:   Batch Loss = 1.419068, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3566877841949463, Accuracy = 0.5633663535118103\n",
      "Iter #104640:  Learning rate = 0.004800:   Batch Loss = 1.546616, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.366439938545227, Accuracy = 0.5534653663635254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #105120:  Learning rate = 0.004800:   Batch Loss = 1.179159, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.340811014175415, Accuracy = 0.5752475261688232\n",
      "Iter #105600:  Learning rate = 0.004800:   Batch Loss = 1.211304, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.351800799369812, Accuracy = 0.5514851212501526\n",
      "Iter #106080:  Learning rate = 0.004800:   Batch Loss = 1.260238, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3664933443069458, Accuracy = 0.5554455518722534\n",
      "Iter #106560:  Learning rate = 0.004800:   Batch Loss = 1.513700, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.445439338684082, Accuracy = 0.5207920670509338\n",
      "Iter #107040:  Learning rate = 0.004800:   Batch Loss = 1.386242, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3978052139282227, Accuracy = 0.5306930541992188\n",
      "Iter #107520:  Learning rate = 0.004800:   Batch Loss = 1.329160, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3674798011779785, Accuracy = 0.5643564462661743\n",
      "Iter #108000:  Learning rate = 0.004800:   Batch Loss = 1.376157, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3759307861328125, Accuracy = 0.5495049357414246\n",
      "Iter #108480:  Learning rate = 0.004800:   Batch Loss = 1.372157, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.433837652206421, Accuracy = 0.5534653663635254\n",
      "Iter #108960:  Learning rate = 0.004800:   Batch Loss = 1.469409, Accuracy = 0.44999998807907104\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3873016834259033, Accuracy = 0.5643564462661743\n",
      "Iter #109440:  Learning rate = 0.004800:   Batch Loss = 1.451879, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3961362838745117, Accuracy = 0.5524752736091614\n",
      "Iter #109920:  Learning rate = 0.004800:   Batch Loss = 1.343336, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3837890625, Accuracy = 0.5396039485931396\n",
      "Iter #110400:  Learning rate = 0.004800:   Batch Loss = 1.339219, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3983378410339355, Accuracy = 0.5465346574783325\n",
      "Iter #110880:  Learning rate = 0.004800:   Batch Loss = 1.425559, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4032487869262695, Accuracy = 0.5485148429870605\n",
      "Iter #111360:  Learning rate = 0.004800:   Batch Loss = 1.317132, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3843499422073364, Accuracy = 0.5584158301353455\n",
      "Iter #111840:  Learning rate = 0.004800:   Batch Loss = 1.365997, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.409061074256897, Accuracy = 0.5514851212501526\n",
      "Iter #112320:  Learning rate = 0.004800:   Batch Loss = 1.341879, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.446263074874878, Accuracy = 0.5366336703300476\n",
      "Iter #112800:  Learning rate = 0.004800:   Batch Loss = 1.290607, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.424033761024475, Accuracy = 0.5306930541992188\n",
      "Iter #113280:  Learning rate = 0.004800:   Batch Loss = 1.437671, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.442833423614502, Accuracy = 0.5168316960334778\n",
      "Iter #113760:  Learning rate = 0.004800:   Batch Loss = 1.660325, Accuracy = 0.4000000059604645\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4213101863861084, Accuracy = 0.5435643792152405\n",
      "Iter #114240:  Learning rate = 0.004800:   Batch Loss = 1.348094, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3725059032440186, Accuracy = 0.5524752736091614\n",
      "Iter #114720:  Learning rate = 0.004800:   Batch Loss = 1.355482, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3647713661193848, Accuracy = 0.5980197787284851\n",
      "Iter #115200:  Learning rate = 0.004800:   Batch Loss = 1.268852, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.361673355102539, Accuracy = 0.5772277116775513\n",
      "Iter #115680:  Learning rate = 0.004800:   Batch Loss = 1.211883, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3491909503936768, Accuracy = 0.5891088843345642\n",
      "Iter #116160:  Learning rate = 0.004800:   Batch Loss = 1.214050, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.331571102142334, Accuracy = 0.5950495004653931\n",
      "Iter #116640:  Learning rate = 0.004800:   Batch Loss = 1.747839, Accuracy = 0.4166666567325592\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3274011611938477, Accuracy = 0.5891088843345642\n",
      "Iter #117120:  Learning rate = 0.004800:   Batch Loss = 1.309162, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3567510843276978, Accuracy = 0.5544554591178894\n",
      "Iter #117600:  Learning rate = 0.004800:   Batch Loss = 1.194910, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3334436416625977, Accuracy = 0.5871286988258362\n",
      "Iter #118080:  Learning rate = 0.004800:   Batch Loss = 1.165531, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3492650985717773, Accuracy = 0.5990098714828491\n",
      "Iter #118560:  Learning rate = 0.004800:   Batch Loss = 1.405218, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3382155895233154, Accuracy = 0.5534653663635254\n",
      "Iter #119040:  Learning rate = 0.004800:   Batch Loss = 1.351909, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3588191270828247, Accuracy = 0.5485148429870605\n",
      "Iter #119520:  Learning rate = 0.004800:   Batch Loss = 1.289724, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3319929838180542, Accuracy = 0.5653465390205383\n",
      "Iter #120000:  Learning rate = 0.004800:   Batch Loss = 1.246954, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3339664936065674, Accuracy = 0.5405940413475037\n",
      "Iter #120480:  Learning rate = 0.004800:   Batch Loss = 1.151023, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3144586086273193, Accuracy = 0.5712871551513672\n",
      "Iter #120960:  Learning rate = 0.004800:   Batch Loss = 1.352667, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3089349269866943, Accuracy = 0.5693069100379944\n",
      "Iter #121440:  Learning rate = 0.004800:   Batch Loss = 1.377326, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.323957920074463, Accuracy = 0.5702970027923584\n",
      "Iter #121920:  Learning rate = 0.004800:   Batch Loss = 1.199110, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3189283609390259, Accuracy = 0.5841584205627441\n",
      "Iter #122400:  Learning rate = 0.004800:   Batch Loss = 1.256719, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3063806295394897, Accuracy = 0.590099036693573\n",
      "Iter #122880:  Learning rate = 0.004800:   Batch Loss = 1.262306, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3089758157730103, Accuracy = 0.5821782350540161\n",
      "Iter #123360:  Learning rate = 0.004800:   Batch Loss = 1.384208, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2991993427276611, Accuracy = 0.5712871551513672\n",
      "Iter #123840:  Learning rate = 0.004800:   Batch Loss = 1.281099, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3168703317642212, Accuracy = 0.5663366317749023\n",
      "Iter #124320:  Learning rate = 0.004800:   Batch Loss = 1.246760, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3264409303665161, Accuracy = 0.5326732397079468\n",
      "Iter #124800:  Learning rate = 0.004800:   Batch Loss = 1.323638, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3137836456298828, Accuracy = 0.5405940413475037\n",
      "Iter #125280:  Learning rate = 0.004800:   Batch Loss = 1.321149, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.30735182762146, Accuracy = 0.5574257373809814\n",
      "Iter #125760:  Learning rate = 0.004800:   Batch Loss = 1.237730, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2832868099212646, Accuracy = 0.5613861680030823\n",
      "Iter #126240:  Learning rate = 0.004800:   Batch Loss = 1.262727, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3010404109954834, Accuracy = 0.5584158301353455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #126720:  Learning rate = 0.004800:   Batch Loss = 1.345726, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.302715539932251, Accuracy = 0.5742574334144592\n",
      "Iter #127200:  Learning rate = 0.004800:   Batch Loss = 1.192301, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2984191179275513, Accuracy = 0.5643564462661743\n",
      "Iter #127680:  Learning rate = 0.004800:   Batch Loss = 1.349805, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3080300092697144, Accuracy = 0.5613861680030823\n",
      "Iter #128160:  Learning rate = 0.004800:   Batch Loss = 1.334885, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3021177053451538, Accuracy = 0.5207920670509338\n",
      "Iter #128640:  Learning rate = 0.004800:   Batch Loss = 1.236112, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2901535034179688, Accuracy = 0.5801980495452881\n",
      "Iter #129120:  Learning rate = 0.004800:   Batch Loss = 1.348565, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3365936279296875, Accuracy = 0.5475247502326965\n",
      "Iter #129600:  Learning rate = 0.004800:   Batch Loss = 1.493460, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3310608863830566, Accuracy = 0.5415841341018677\n",
      "Iter #130080:  Learning rate = 0.004800:   Batch Loss = 1.336833, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3309686183929443, Accuracy = 0.5574257373809814\n",
      "Iter #130560:  Learning rate = 0.004800:   Batch Loss = 1.242406, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.321026086807251, Accuracy = 0.5524752736091614\n",
      "Iter #131040:  Learning rate = 0.004800:   Batch Loss = 1.411543, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2841651439666748, Accuracy = 0.5851485133171082\n",
      "Iter #131520:  Learning rate = 0.004800:   Batch Loss = 1.150545, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2966078519821167, Accuracy = 0.5475247502326965\n",
      "Iter #132000:  Learning rate = 0.004800:   Batch Loss = 1.355109, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3052982091903687, Accuracy = 0.5752475261688232\n",
      "Iter #132480:  Learning rate = 0.004800:   Batch Loss = 1.278838, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3158800601959229, Accuracy = 0.5752475261688232\n",
      "Iter #132960:  Learning rate = 0.004800:   Batch Loss = 1.378076, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3565205335617065, Accuracy = 0.48613861203193665\n",
      "Iter #133440:  Learning rate = 0.004800:   Batch Loss = 1.381720, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3207895755767822, Accuracy = 0.5603960156440735\n",
      "Iter #133920:  Learning rate = 0.004800:   Batch Loss = 1.321733, Accuracy = 0.44999998807907104\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.338890552520752, Accuracy = 0.5366336703300476\n",
      "Iter #134400:  Learning rate = 0.004800:   Batch Loss = 1.249678, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3194432258605957, Accuracy = 0.5168316960334778\n",
      "Iter #134880:  Learning rate = 0.004800:   Batch Loss = 1.203590, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.334670066833496, Accuracy = 0.5267326831817627\n",
      "Iter #135360:  Learning rate = 0.004800:   Batch Loss = 1.165843, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.316426396369934, Accuracy = 0.5524752736091614\n",
      "Iter #135840:  Learning rate = 0.004800:   Batch Loss = 1.302086, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3103108406066895, Accuracy = 0.5267326831817627\n",
      "Iter #136320:  Learning rate = 0.004800:   Batch Loss = 1.266662, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3354263305664062, Accuracy = 0.5425742864608765\n",
      "Iter #136800:  Learning rate = 0.004800:   Batch Loss = 1.196339, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.296480417251587, Accuracy = 0.5603960156440735\n",
      "Iter #137280:  Learning rate = 0.004800:   Batch Loss = 1.248349, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.307438850402832, Accuracy = 0.5356435775756836\n",
      "Iter #137760:  Learning rate = 0.004800:   Batch Loss = 1.357368, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2912578582763672, Accuracy = 0.5603960156440735\n",
      "Iter #138240:  Learning rate = 0.004800:   Batch Loss = 1.212411, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2810636758804321, Accuracy = 0.5564356446266174\n",
      "Iter #138720:  Learning rate = 0.004800:   Batch Loss = 1.336700, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3054804801940918, Accuracy = 0.5544554591178894\n",
      "Iter #139200:  Learning rate = 0.004800:   Batch Loss = 1.242548, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3853398561477661, Accuracy = 0.5059406161308289\n",
      "Iter #139680:  Learning rate = 0.004800:   Batch Loss = 1.425422, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3182064294815063, Accuracy = 0.5475247502326965\n",
      "Iter #140160:  Learning rate = 0.004800:   Batch Loss = 1.101971, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2826541662216187, Accuracy = 0.590099036693573\n",
      "Iter #140640:  Learning rate = 0.004800:   Batch Loss = 1.561597, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5750925540924072, Accuracy = 0.43168318271636963\n",
      "Iter #141120:  Learning rate = 0.004800:   Batch Loss = 1.611171, Accuracy = 0.3333333432674408\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5319517850875854, Accuracy = 0.4811881184577942\n",
      "Iter #141600:  Learning rate = 0.004800:   Batch Loss = 1.671999, Accuracy = 0.44999998807907104\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5288759469985962, Accuracy = 0.4445544481277466\n",
      "Iter #142080:  Learning rate = 0.004800:   Batch Loss = 1.366719, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4356966018676758, Accuracy = 0.5267326831817627\n",
      "Iter #142560:  Learning rate = 0.004800:   Batch Loss = 1.453732, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3725712299346924, Accuracy = 0.5405940413475037\n",
      "Iter #143040:  Learning rate = 0.004800:   Batch Loss = 1.381306, Accuracy = 0.4000000059604645\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3847241401672363, Accuracy = 0.5247524976730347\n",
      "Iter #143520:  Learning rate = 0.004800:   Batch Loss = 1.558203, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4105647802352905, Accuracy = 0.5227722525596619\n",
      "Iter #144000:  Learning rate = 0.004800:   Batch Loss = 1.461849, Accuracy = 0.4166666567325592\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.388989806175232, Accuracy = 0.5386138558387756\n",
      "Iter #144480:  Learning rate = 0.004800:   Batch Loss = 1.265845, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3894977569580078, Accuracy = 0.5217821598052979\n",
      "Iter #144960:  Learning rate = 0.004800:   Batch Loss = 1.452989, Accuracy = 0.44999998807907104\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3355180025100708, Accuracy = 0.5574257373809814\n",
      "Iter #145440:  Learning rate = 0.004800:   Batch Loss = 1.395030, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4587361812591553, Accuracy = 0.4881187975406647\n",
      "Iter #145920:  Learning rate = 0.004800:   Batch Loss = 1.505178, Accuracy = 0.4166666567325592\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3679358959197998, Accuracy = 0.513861358165741\n",
      "Iter #146400:  Learning rate = 0.004800:   Batch Loss = 1.397874, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.358072280883789, Accuracy = 0.5396039485931396\n",
      "Iter #146880:  Learning rate = 0.004800:   Batch Loss = 1.426583, Accuracy = 0.4000000059604645\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3785239458084106, Accuracy = 0.4811881184577942\n",
      "Iter #147360:  Learning rate = 0.004800:   Batch Loss = 1.569760, Accuracy = 0.4333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3884402513504028, Accuracy = 0.5178217887878418\n",
      "Iter #147840:  Learning rate = 0.004800:   Batch Loss = 1.354041, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.445920467376709, Accuracy = 0.5059406161308289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #148320:  Learning rate = 0.004800:   Batch Loss = 1.617247, Accuracy = 0.4000000059604645\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.429112434387207, Accuracy = 0.5287128686904907\n",
      "Iter #148800:  Learning rate = 0.004800:   Batch Loss = 1.404204, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.436368703842163, Accuracy = 0.49405941367149353\n",
      "Iter #149280:  Learning rate = 0.004800:   Batch Loss = 1.469689, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3998271226882935, Accuracy = 0.5168316960334778\n",
      "Iter #149760:  Learning rate = 0.004800:   Batch Loss = 1.393310, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4383212327957153, Accuracy = 0.5326732397079468\n",
      "Iter #150240:  Learning rate = 0.004800:   Batch Loss = 1.378003, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4048364162445068, Accuracy = 0.5168316960334778\n",
      "Iter #150720:  Learning rate = 0.004800:   Batch Loss = 1.228047, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3569278717041016, Accuracy = 0.5514851212501526\n",
      "Iter #151200:  Learning rate = 0.004800:   Batch Loss = 1.247548, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.340989351272583, Accuracy = 0.5534653663635254\n",
      "Iter #151680:  Learning rate = 0.004800:   Batch Loss = 1.450516, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3306941986083984, Accuracy = 0.5386138558387756\n",
      "Iter #152160:  Learning rate = 0.004800:   Batch Loss = 1.210299, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.349855661392212, Accuracy = 0.5475247502326965\n",
      "Iter #152640:  Learning rate = 0.004800:   Batch Loss = 1.390226, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.379499912261963, Accuracy = 0.5207920670509338\n",
      "Iter #153120:  Learning rate = 0.004800:   Batch Loss = 1.386592, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3930721282958984, Accuracy = 0.5198019742965698\n",
      "Iter #153600:  Learning rate = 0.004800:   Batch Loss = 1.403489, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4053728580474854, Accuracy = 0.498019814491272\n",
      "Iter #154080:  Learning rate = 0.004800:   Batch Loss = 1.401695, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3663214445114136, Accuracy = 0.5118811726570129\n",
      "Iter #154560:  Learning rate = 0.004800:   Batch Loss = 1.277522, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.368356704711914, Accuracy = 0.48514851927757263\n",
      "Iter #155040:  Learning rate = 0.004800:   Batch Loss = 1.236676, Accuracy = 0.44999998807907104\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.316171646118164, Accuracy = 0.5346534848213196\n",
      "Iter #155520:  Learning rate = 0.004800:   Batch Loss = 1.211132, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3521721363067627, Accuracy = 0.49603959918022156\n",
      "Iter #156000:  Learning rate = 0.004800:   Batch Loss = 1.143467, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3041330575942993, Accuracy = 0.5425742864608765\n",
      "Iter #156480:  Learning rate = 0.004800:   Batch Loss = 1.417733, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3335638046264648, Accuracy = 0.5544554591178894\n",
      "Iter #156960:  Learning rate = 0.004800:   Batch Loss = 1.265939, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3659557104110718, Accuracy = 0.5118811726570129\n",
      "Iter #157440:  Learning rate = 0.004800:   Batch Loss = 1.255010, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4004101753234863, Accuracy = 0.5\n",
      "Iter #157920:  Learning rate = 0.004800:   Batch Loss = 1.311332, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3478291034698486, Accuracy = 0.5396039485931396\n",
      "Iter #158400:  Learning rate = 0.004800:   Batch Loss = 1.350448, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4030641317367554, Accuracy = 0.5108910799026489\n",
      "Iter #158880:  Learning rate = 0.004800:   Batch Loss = 1.393259, Accuracy = 0.4166666567325592\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4573750495910645, Accuracy = 0.46138614416122437\n",
      "Iter #159360:  Learning rate = 0.004800:   Batch Loss = 1.591316, Accuracy = 0.38333332538604736\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3594019412994385, Accuracy = 0.5049505233764648\n",
      "Iter #159840:  Learning rate = 0.004800:   Batch Loss = 1.432998, Accuracy = 0.4166666567325592\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3842021226882935, Accuracy = 0.5049505233764648\n",
      "Iter #160320:  Learning rate = 0.004800:   Batch Loss = 1.376789, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3457082509994507, Accuracy = 0.5376237630844116\n",
      "Iter #160800:  Learning rate = 0.004800:   Batch Loss = 1.236387, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3240549564361572, Accuracy = 0.5396039485931396\n",
      "Iter #161280:  Learning rate = 0.004800:   Batch Loss = 1.332394, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3572304248809814, Accuracy = 0.5336633920669556\n",
      "Iter #161760:  Learning rate = 0.004800:   Batch Loss = 1.314774, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3467674255371094, Accuracy = 0.5376237630844116\n",
      "Iter #162240:  Learning rate = 0.004800:   Batch Loss = 1.278817, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3458430767059326, Accuracy = 0.5415841341018677\n",
      "Iter #162720:  Learning rate = 0.004800:   Batch Loss = 1.354720, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.351569652557373, Accuracy = 0.5376237630844116\n",
      "Iter #163200:  Learning rate = 0.004800:   Batch Loss = 1.281904, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3088120222091675, Accuracy = 0.5594059228897095\n",
      "Iter #163680:  Learning rate = 0.004800:   Batch Loss = 1.236567, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3355902433395386, Accuracy = 0.5356435775756836\n",
      "Iter #164160:  Learning rate = 0.004800:   Batch Loss = 1.316367, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3374989032745361, Accuracy = 0.5554455518722534\n",
      "Iter #164640:  Learning rate = 0.004800:   Batch Loss = 1.424239, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3736008405685425, Accuracy = 0.5207920670509338\n",
      "Iter #165120:  Learning rate = 0.004800:   Batch Loss = 1.323396, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3059325218200684, Accuracy = 0.5495049357414246\n",
      "Iter #165600:  Learning rate = 0.004800:   Batch Loss = 1.522251, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3122071027755737, Accuracy = 0.5386138558387756\n",
      "Iter #166080:  Learning rate = 0.004800:   Batch Loss = 1.358381, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3661962747573853, Accuracy = 0.5297029614448547\n",
      "Iter #166560:  Learning rate = 0.004800:   Batch Loss = 1.326333, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4492156505584717, Accuracy = 0.4623762369155884\n",
      "Iter #167040:  Learning rate = 0.004800:   Batch Loss = 1.494062, Accuracy = 0.44999998807907104\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.43446683883667, Accuracy = 0.45841583609580994\n",
      "Iter #167520:  Learning rate = 0.004800:   Batch Loss = 1.379209, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.381834864616394, Accuracy = 0.5326732397079468\n",
      "Iter #168000:  Learning rate = 0.004800:   Batch Loss = 1.242044, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3474398851394653, Accuracy = 0.5287128686904907\n",
      "Iter #168480:  Learning rate = 0.004800:   Batch Loss = 1.384429, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3012886047363281, Accuracy = 0.5732673406600952\n",
      "Iter #168960:  Learning rate = 0.004800:   Batch Loss = 1.221285, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3367054462432861, Accuracy = 0.5168316960334778\n",
      "Iter #169440:  Learning rate = 0.004800:   Batch Loss = 1.308702, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3666658401489258, Accuracy = 0.5227722525596619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #169920:  Learning rate = 0.004800:   Batch Loss = 1.285803, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.360680341720581, Accuracy = 0.5267326831817627\n",
      "Iter #170400:  Learning rate = 0.004800:   Batch Loss = 1.453229, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3508611917495728, Accuracy = 0.5188118815422058\n",
      "Iter #170880:  Learning rate = 0.004800:   Batch Loss = 1.150477, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.304823637008667, Accuracy = 0.5673267245292664\n",
      "Iter #171360:  Learning rate = 0.004800:   Batch Loss = 1.302368, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3491307497024536, Accuracy = 0.5594059228897095\n",
      "Iter #171840:  Learning rate = 0.004800:   Batch Loss = 1.236526, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3690361976623535, Accuracy = 0.5277227759361267\n",
      "Iter #172320:  Learning rate = 0.004800:   Batch Loss = 1.515954, Accuracy = 0.3499999940395355\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3627090454101562, Accuracy = 0.5198019742965698\n",
      "Iter #172800:  Learning rate = 0.004800:   Batch Loss = 1.364544, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3501331806182861, Accuracy = 0.4653465449810028\n",
      "Iter #173280:  Learning rate = 0.004800:   Batch Loss = 1.355751, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.417701005935669, Accuracy = 0.503960371017456\n",
      "Iter #173760:  Learning rate = 0.004800:   Batch Loss = 1.258161, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.424680233001709, Accuracy = 0.498019814491272\n",
      "Iter #174240:  Learning rate = 0.004800:   Batch Loss = 1.295043, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.350279450416565, Accuracy = 0.5306930541992188\n",
      "Iter #174720:  Learning rate = 0.004800:   Batch Loss = 1.405482, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3404221534729004, Accuracy = 0.5366336703300476\n",
      "Iter #175200:  Learning rate = 0.004800:   Batch Loss = 1.517702, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3464165925979614, Accuracy = 0.5742574334144592\n",
      "Iter #175680:  Learning rate = 0.004800:   Batch Loss = 1.229917, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3116320371627808, Accuracy = 0.5752475261688232\n",
      "Iter #176160:  Learning rate = 0.004800:   Batch Loss = 1.229608, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2770640850067139, Accuracy = 0.5990098714828491\n",
      "Iter #176640:  Learning rate = 0.004800:   Batch Loss = 1.155898, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3106560707092285, Accuracy = 0.5584158301353455\n",
      "Iter #177120:  Learning rate = 0.004800:   Batch Loss = 1.279880, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3136812448501587, Accuracy = 0.591089129447937\n",
      "Iter #177600:  Learning rate = 0.004800:   Batch Loss = 1.373871, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3304450511932373, Accuracy = 0.5475247502326965\n",
      "Iter #178080:  Learning rate = 0.004800:   Batch Loss = 1.172659, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2605228424072266, Accuracy = 0.590099036693573\n",
      "Iter #178560:  Learning rate = 0.004800:   Batch Loss = 1.247684, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2507331371307373, Accuracy = 0.5693069100379944\n",
      "Iter #179040:  Learning rate = 0.004800:   Batch Loss = 1.076747, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2536582946777344, Accuracy = 0.5841584205627441\n",
      "Iter #179520:  Learning rate = 0.004800:   Batch Loss = 1.267601, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2798047065734863, Accuracy = 0.5821782350540161\n",
      "Iter #180000:  Learning rate = 0.004800:   Batch Loss = 1.202296, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3611434698104858, Accuracy = 0.499009907245636\n",
      "Iter #180480:  Learning rate = 0.004800:   Batch Loss = 1.375393, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.320502758026123, Accuracy = 0.4841584265232086\n",
      "Iter #180960:  Learning rate = 0.004800:   Batch Loss = 1.304311, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2700223922729492, Accuracy = 0.5643564462661743\n",
      "Iter #181440:  Learning rate = 0.004800:   Batch Loss = 1.154312, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3225775957107544, Accuracy = 0.5534653663635254\n",
      "Iter #181920:  Learning rate = 0.004800:   Batch Loss = 1.310618, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.363564372062683, Accuracy = 0.5079208016395569\n",
      "Iter #182400:  Learning rate = 0.004800:   Batch Loss = 1.320318, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3974254131317139, Accuracy = 0.4811881184577942\n",
      "Iter #182880:  Learning rate = 0.004800:   Batch Loss = 1.399822, Accuracy = 0.4000000059604645\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3979140520095825, Accuracy = 0.46930691599845886\n",
      "Iter #183360:  Learning rate = 0.004800:   Batch Loss = 1.469960, Accuracy = 0.44999998807907104\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.4247186183929443, Accuracy = 0.4663366377353668\n",
      "Iter #183840:  Learning rate = 0.004800:   Batch Loss = 1.311208, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3896318674087524, Accuracy = 0.5089108943939209\n",
      "Iter #184320:  Learning rate = 0.004800:   Batch Loss = 1.353050, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3587281703948975, Accuracy = 0.4801980257034302\n",
      "Iter #184800:  Learning rate = 0.004800:   Batch Loss = 1.275894, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3424426317214966, Accuracy = 0.5069307088851929\n",
      "Iter #185280:  Learning rate = 0.004800:   Batch Loss = 1.354478, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3246560096740723, Accuracy = 0.512871265411377\n",
      "Iter #185760:  Learning rate = 0.004800:   Batch Loss = 1.297802, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3216803073883057, Accuracy = 0.5287128686904907\n",
      "Iter #186240:  Learning rate = 0.004800:   Batch Loss = 1.238220, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2909958362579346, Accuracy = 0.5594059228897095\n",
      "Iter #186720:  Learning rate = 0.004800:   Batch Loss = 1.270740, Accuracy = 0.4000000059604645\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.29534912109375, Accuracy = 0.5158416032791138\n",
      "Iter #187200:  Learning rate = 0.004800:   Batch Loss = 1.293655, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3252391815185547, Accuracy = 0.5247524976730347\n",
      "Iter #187680:  Learning rate = 0.004800:   Batch Loss = 1.256580, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3325269222259521, Accuracy = 0.5079208016395569\n",
      "Iter #188160:  Learning rate = 0.004800:   Batch Loss = 1.234913, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3205981254577637, Accuracy = 0.5386138558387756\n",
      "Iter #188640:  Learning rate = 0.004800:   Batch Loss = 1.340898, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3074744939804077, Accuracy = 0.5346534848213196\n",
      "Iter #189120:  Learning rate = 0.004800:   Batch Loss = 1.393805, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.30062997341156, Accuracy = 0.5297029614448547\n",
      "Iter #189600:  Learning rate = 0.004800:   Batch Loss = 1.175397, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2785639762878418, Accuracy = 0.5435643792152405\n",
      "Iter #190080:  Learning rate = 0.004800:   Batch Loss = 1.188758, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2690269947052002, Accuracy = 0.5633663535118103\n",
      "Iter #190560:  Learning rate = 0.004800:   Batch Loss = 1.107262, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2995285987854004, Accuracy = 0.5574257373809814\n",
      "Iter #191040:  Learning rate = 0.004800:   Batch Loss = 1.098536, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.293444037437439, Accuracy = 0.5415841341018677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #191520:  Learning rate = 0.004800:   Batch Loss = 1.243405, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2903531789779663, Accuracy = 0.5504950284957886\n",
      "Iter #192000:  Learning rate = 0.004800:   Batch Loss = 1.275673, Accuracy = 0.4833333194255829\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3624563217163086, Accuracy = 0.5178217887878418\n",
      "Iter #192480:  Learning rate = 0.004800:   Batch Loss = 1.287436, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3330830335617065, Accuracy = 0.4811881184577942\n",
      "Iter #192960:  Learning rate = 0.004800:   Batch Loss = 1.334873, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3425557613372803, Accuracy = 0.513861358165741\n",
      "Iter #193440:  Learning rate = 0.004800:   Batch Loss = 1.334716, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3249528408050537, Accuracy = 0.512871265411377\n",
      "Iter #193920:  Learning rate = 0.004800:   Batch Loss = 1.279023, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2969497442245483, Accuracy = 0.5366336703300476\n",
      "Iter #194400:  Learning rate = 0.004800:   Batch Loss = 1.352952, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2627732753753662, Accuracy = 0.5574257373809814\n",
      "Iter #194880:  Learning rate = 0.004800:   Batch Loss = 1.268695, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.312334418296814, Accuracy = 0.5445544719696045\n",
      "Iter #195360:  Learning rate = 0.004800:   Batch Loss = 1.263864, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2960147857666016, Accuracy = 0.5732673406600952\n",
      "Iter #195840:  Learning rate = 0.004800:   Batch Loss = 1.402625, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2912659645080566, Accuracy = 0.5594059228897095\n",
      "Iter #196320:  Learning rate = 0.004800:   Batch Loss = 1.216421, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2903050184249878, Accuracy = 0.5742574334144592\n",
      "Iter #196800:  Learning rate = 0.004800:   Batch Loss = 1.343151, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2747619152069092, Accuracy = 0.5762376189231873\n",
      "Iter #197280:  Learning rate = 0.004800:   Batch Loss = 1.117062, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2805155515670776, Accuracy = 0.5683168172836304\n",
      "Iter #197760:  Learning rate = 0.004800:   Batch Loss = 1.228023, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3168734312057495, Accuracy = 0.5544554591178894\n",
      "Iter #198240:  Learning rate = 0.004800:   Batch Loss = 1.174913, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.263843297958374, Accuracy = 0.5693069100379944\n",
      "Iter #198720:  Learning rate = 0.004800:   Batch Loss = 1.372092, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2759349346160889, Accuracy = 0.5445544719696045\n",
      "Iter #199200:  Learning rate = 0.004800:   Batch Loss = 1.200566, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3145110607147217, Accuracy = 0.49702969193458557\n",
      "Iter #199680:  Learning rate = 0.004800:   Batch Loss = 1.273902, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.332362174987793, Accuracy = 0.5623762607574463\n",
      "Iter #200160:  Learning rate = 0.004608:   Batch Loss = 1.199544, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3248493671417236, Accuracy = 0.5227722525596619\n",
      "Iter #200640:  Learning rate = 0.004608:   Batch Loss = 1.386079, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3020191192626953, Accuracy = 0.5425742864608765\n",
      "Iter #201120:  Learning rate = 0.004608:   Batch Loss = 1.417067, Accuracy = 0.4166666567325592\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3276454210281372, Accuracy = 0.5207920670509338\n",
      "Iter #201600:  Learning rate = 0.004608:   Batch Loss = 1.240654, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3132965564727783, Accuracy = 0.5782178044319153\n",
      "Iter #202080:  Learning rate = 0.004608:   Batch Loss = 1.076768, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3123252391815186, Accuracy = 0.5425742864608765\n",
      "Iter #202560:  Learning rate = 0.004608:   Batch Loss = 1.259262, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3120193481445312, Accuracy = 0.5386138558387756\n",
      "Iter #203040:  Learning rate = 0.004608:   Batch Loss = 1.390305, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.284407377243042, Accuracy = 0.5386138558387756\n",
      "Iter #203520:  Learning rate = 0.004608:   Batch Loss = 1.290906, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2856627702713013, Accuracy = 0.5544554591178894\n",
      "Iter #204000:  Learning rate = 0.004608:   Batch Loss = 1.397293, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3053348064422607, Accuracy = 0.5524752736091614\n",
      "Iter #204480:  Learning rate = 0.004608:   Batch Loss = 1.249843, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.289125919342041, Accuracy = 0.5445544719696045\n",
      "Iter #204960:  Learning rate = 0.004608:   Batch Loss = 1.088307, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.27182936668396, Accuracy = 0.5405940413475037\n",
      "Iter #205440:  Learning rate = 0.004608:   Batch Loss = 1.203801, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2506473064422607, Accuracy = 0.5722772479057312\n",
      "Iter #205920:  Learning rate = 0.004608:   Batch Loss = 1.156660, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2804579734802246, Accuracy = 0.5524752736091614\n",
      "Iter #206400:  Learning rate = 0.004608:   Batch Loss = 1.215309, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2622660398483276, Accuracy = 0.5732673406600952\n",
      "Iter #206880:  Learning rate = 0.004608:   Batch Loss = 1.144169, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2560120820999146, Accuracy = 0.5653465390205383\n",
      "Iter #207360:  Learning rate = 0.004608:   Batch Loss = 1.104128, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.252936601638794, Accuracy = 0.5663366317749023\n",
      "Iter #207840:  Learning rate = 0.004608:   Batch Loss = 1.098921, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2634023427963257, Accuracy = 0.5623762607574463\n",
      "Iter #208320:  Learning rate = 0.004608:   Batch Loss = 1.152484, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2615481615066528, Accuracy = 0.5732673406600952\n",
      "Iter #208800:  Learning rate = 0.004608:   Batch Loss = 1.163457, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2317476272583008, Accuracy = 0.5792078971862793\n",
      "Iter #209280:  Learning rate = 0.004608:   Batch Loss = 1.262380, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2420272827148438, Accuracy = 0.5811881422996521\n",
      "Iter #209760:  Learning rate = 0.004608:   Batch Loss = 1.167693, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2642112970352173, Accuracy = 0.5366336703300476\n",
      "Iter #210240:  Learning rate = 0.004608:   Batch Loss = 1.202337, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2841593027114868, Accuracy = 0.5574257373809814\n",
      "Iter #210720:  Learning rate = 0.004608:   Batch Loss = 1.343680, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2598137855529785, Accuracy = 0.5564356446266174\n",
      "Iter #211200:  Learning rate = 0.004608:   Batch Loss = 1.475656, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2300682067871094, Accuracy = 0.606930673122406\n",
      "Iter #211680:  Learning rate = 0.004608:   Batch Loss = 1.129063, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2318801879882812, Accuracy = 0.6000000238418579\n",
      "Iter #212160:  Learning rate = 0.004608:   Batch Loss = 1.309189, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2335996627807617, Accuracy = 0.5811881422996521\n",
      "Iter #212640:  Learning rate = 0.004608:   Batch Loss = 1.173035, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2246789932250977, Accuracy = 0.5693069100379944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #213120:  Learning rate = 0.004608:   Batch Loss = 1.145737, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.221211552619934, Accuracy = 0.6118811964988708\n",
      "Iter #213600:  Learning rate = 0.004608:   Batch Loss = 1.272362, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2099348306655884, Accuracy = 0.6178217530250549\n",
      "Iter #214080:  Learning rate = 0.004608:   Batch Loss = 1.263008, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.218787670135498, Accuracy = 0.5871286988258362\n",
      "Iter #214560:  Learning rate = 0.004608:   Batch Loss = 1.031549, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2182748317718506, Accuracy = 0.5782178044319153\n",
      "Iter #215040:  Learning rate = 0.004608:   Batch Loss = 1.144793, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2141058444976807, Accuracy = 0.5960395932197571\n",
      "Iter #215520:  Learning rate = 0.004608:   Batch Loss = 1.102555, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2039132118225098, Accuracy = 0.5950495004653931\n",
      "Iter #216000:  Learning rate = 0.004608:   Batch Loss = 1.159370, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.245601773262024, Accuracy = 0.5712871551513672\n",
      "Iter #216480:  Learning rate = 0.004608:   Batch Loss = 1.257903, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2400448322296143, Accuracy = 0.5782178044319153\n",
      "Iter #216960:  Learning rate = 0.004608:   Batch Loss = 1.270610, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2664425373077393, Accuracy = 0.5712871551513672\n",
      "Iter #217440:  Learning rate = 0.004608:   Batch Loss = 1.035330, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2385928630828857, Accuracy = 0.604950487613678\n",
      "Iter #217920:  Learning rate = 0.004608:   Batch Loss = 1.098699, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2096357345581055, Accuracy = 0.6188119053840637\n",
      "Iter #218400:  Learning rate = 0.004608:   Batch Loss = 1.213124, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2297849655151367, Accuracy = 0.594059407711029\n",
      "Iter #218880:  Learning rate = 0.004608:   Batch Loss = 1.168370, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.234227180480957, Accuracy = 0.5960395932197571\n",
      "Iter #219360:  Learning rate = 0.004608:   Batch Loss = 1.032637, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.22434401512146, Accuracy = 0.593069314956665\n",
      "Iter #219840:  Learning rate = 0.004608:   Batch Loss = 1.120551, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.229913353919983, Accuracy = 0.5891088843345642\n",
      "Iter #220320:  Learning rate = 0.004608:   Batch Loss = 1.164160, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2196513414382935, Accuracy = 0.5970296859741211\n",
      "Iter #220800:  Learning rate = 0.004608:   Batch Loss = 1.218549, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2426540851593018, Accuracy = 0.5811881422996521\n",
      "Iter #221280:  Learning rate = 0.004608:   Batch Loss = 1.145103, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2749899625778198, Accuracy = 0.5574257373809814\n",
      "Iter #221760:  Learning rate = 0.004608:   Batch Loss = 1.407926, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2462878227233887, Accuracy = 0.5633663535118103\n",
      "Iter #222240:  Learning rate = 0.004608:   Batch Loss = 1.093948, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.216149091720581, Accuracy = 0.5762376189231873\n",
      "Iter #222720:  Learning rate = 0.004608:   Batch Loss = 1.144555, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2274353504180908, Accuracy = 0.5782178044319153\n",
      "Iter #223200:  Learning rate = 0.004608:   Batch Loss = 1.410395, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.33687424659729, Accuracy = 0.499009907245636\n",
      "Iter #223680:  Learning rate = 0.004608:   Batch Loss = 1.234944, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2595241069793701, Accuracy = 0.5633663535118103\n",
      "Iter #224160:  Learning rate = 0.004608:   Batch Loss = 1.297793, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2467352151870728, Accuracy = 0.5801980495452881\n",
      "Iter #224640:  Learning rate = 0.004608:   Batch Loss = 1.377598, Accuracy = 0.44999998807907104\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2530701160430908, Accuracy = 0.5871286988258362\n",
      "Iter #225120:  Learning rate = 0.004608:   Batch Loss = 1.260564, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.236253023147583, Accuracy = 0.5801980495452881\n",
      "Iter #225600:  Learning rate = 0.004608:   Batch Loss = 1.276060, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.240809679031372, Accuracy = 0.590099036693573\n",
      "Iter #226080:  Learning rate = 0.004608:   Batch Loss = 1.228979, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2296143770217896, Accuracy = 0.6138613820075989\n",
      "Iter #226560:  Learning rate = 0.004608:   Batch Loss = 1.318722, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.232813835144043, Accuracy = 0.5990098714828491\n",
      "Iter #227040:  Learning rate = 0.004608:   Batch Loss = 1.006629, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2257616519927979, Accuracy = 0.5980197787284851\n",
      "Iter #227520:  Learning rate = 0.004608:   Batch Loss = 1.276624, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2186529636383057, Accuracy = 0.5960395932197571\n",
      "Iter #228000:  Learning rate = 0.004608:   Batch Loss = 1.237724, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.23073148727417, Accuracy = 0.5871286988258362\n",
      "Iter #228480:  Learning rate = 0.004608:   Batch Loss = 1.097324, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2331323623657227, Accuracy = 0.5990098714828491\n",
      "Iter #228960:  Learning rate = 0.004608:   Batch Loss = 1.142282, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2517783641815186, Accuracy = 0.592079222202301\n",
      "Iter #229440:  Learning rate = 0.004608:   Batch Loss = 1.274020, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3038043975830078, Accuracy = 0.5574257373809814\n",
      "Iter #229920:  Learning rate = 0.004608:   Batch Loss = 1.208796, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.266155481338501, Accuracy = 0.5475247502326965\n",
      "Iter #230400:  Learning rate = 0.004608:   Batch Loss = 1.107910, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2244930267333984, Accuracy = 0.5851485133171082\n",
      "Iter #230880:  Learning rate = 0.004608:   Batch Loss = 1.150420, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2308542728424072, Accuracy = 0.5653465390205383\n",
      "Iter #231360:  Learning rate = 0.004608:   Batch Loss = 1.160060, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.220627784729004, Accuracy = 0.5643564462661743\n",
      "Iter #231840:  Learning rate = 0.004608:   Batch Loss = 1.116460, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.247267484664917, Accuracy = 0.5702970027923584\n",
      "Iter #232320:  Learning rate = 0.004608:   Batch Loss = 1.169479, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.219805359840393, Accuracy = 0.594059407711029\n",
      "Iter #232800:  Learning rate = 0.004608:   Batch Loss = 1.065026, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2131221294403076, Accuracy = 0.5891088843345642\n",
      "Iter #233280:  Learning rate = 0.004608:   Batch Loss = 1.219198, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2101328372955322, Accuracy = 0.5801980495452881\n",
      "Iter #233760:  Learning rate = 0.004608:   Batch Loss = 1.353478, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.216204047203064, Accuracy = 0.5772277116775513\n",
      "Iter #234240:  Learning rate = 0.004608:   Batch Loss = 1.202995, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2052229642868042, Accuracy = 0.5782178044319153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #234720:  Learning rate = 0.004608:   Batch Loss = 1.268908, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1960301399230957, Accuracy = 0.5742574334144592\n",
      "Iter #235200:  Learning rate = 0.004608:   Batch Loss = 1.134902, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1952693462371826, Accuracy = 0.6000000238418579\n",
      "Iter #235680:  Learning rate = 0.004608:   Batch Loss = 0.984539, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2028141021728516, Accuracy = 0.594059407711029\n",
      "Iter #236160:  Learning rate = 0.004608:   Batch Loss = 1.163305, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1805176734924316, Accuracy = 0.593069314956665\n",
      "Iter #236640:  Learning rate = 0.004608:   Batch Loss = 1.088651, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2266331911087036, Accuracy = 0.5851485133171082\n",
      "Iter #237120:  Learning rate = 0.004608:   Batch Loss = 1.186653, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1927735805511475, Accuracy = 0.5960395932197571\n",
      "Iter #237600:  Learning rate = 0.004608:   Batch Loss = 1.081957, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1792523860931396, Accuracy = 0.5841584205627441\n",
      "Iter #238080:  Learning rate = 0.004608:   Batch Loss = 1.214117, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.168097972869873, Accuracy = 0.6099010109901428\n",
      "Iter #238560:  Learning rate = 0.004608:   Batch Loss = 1.048598, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1824535131454468, Accuracy = 0.6019802093505859\n",
      "Iter #239040:  Learning rate = 0.004608:   Batch Loss = 1.172078, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1710216999053955, Accuracy = 0.5851485133171082\n",
      "Iter #239520:  Learning rate = 0.004608:   Batch Loss = 1.283954, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1622750759124756, Accuracy = 0.6000000238418579\n",
      "Iter #240000:  Learning rate = 0.004608:   Batch Loss = 1.164192, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2094370126724243, Accuracy = 0.5841584205627441\n",
      "Iter #240480:  Learning rate = 0.004608:   Batch Loss = 1.017941, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.186411738395691, Accuracy = 0.605940580368042\n",
      "Iter #240960:  Learning rate = 0.004608:   Batch Loss = 1.059069, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1827865839004517, Accuracy = 0.5990098714828491\n",
      "Iter #241440:  Learning rate = 0.004608:   Batch Loss = 1.257375, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1973322629928589, Accuracy = 0.60297030210495\n",
      "Iter #241920:  Learning rate = 0.004608:   Batch Loss = 1.175452, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1693401336669922, Accuracy = 0.6118811964988708\n",
      "Iter #242400:  Learning rate = 0.004608:   Batch Loss = 1.041542, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1604381799697876, Accuracy = 0.6188119053840637\n",
      "Iter #242880:  Learning rate = 0.004608:   Batch Loss = 1.099320, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1516711711883545, Accuracy = 0.6386138796806335\n",
      "Iter #243360:  Learning rate = 0.004608:   Batch Loss = 1.139368, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1504443883895874, Accuracy = 0.6405940651893616\n",
      "Iter #243840:  Learning rate = 0.004608:   Batch Loss = 1.030310, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.149906873703003, Accuracy = 0.6227722764015198\n",
      "Iter #244320:  Learning rate = 0.004608:   Batch Loss = 1.243105, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1458145380020142, Accuracy = 0.6465346813201904\n",
      "Iter #244800:  Learning rate = 0.004608:   Batch Loss = 1.101814, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1404316425323486, Accuracy = 0.6475247740745544\n",
      "Iter #245280:  Learning rate = 0.004608:   Batch Loss = 1.051526, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1427192687988281, Accuracy = 0.6356435418128967\n",
      "Iter #245760:  Learning rate = 0.004608:   Batch Loss = 1.166229, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.202514410018921, Accuracy = 0.5504950284957886\n",
      "Iter #246240:  Learning rate = 0.004608:   Batch Loss = 1.014834, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2157008647918701, Accuracy = 0.5663366317749023\n",
      "Iter #246720:  Learning rate = 0.004608:   Batch Loss = 1.155806, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1841752529144287, Accuracy = 0.606930673122406\n",
      "Iter #247200:  Learning rate = 0.004608:   Batch Loss = 1.264058, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1713718175888062, Accuracy = 0.5970296859741211\n",
      "Iter #247680:  Learning rate = 0.004608:   Batch Loss = 1.148338, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.17193603515625, Accuracy = 0.6237623691558838\n",
      "Iter #248160:  Learning rate = 0.004608:   Batch Loss = 1.114003, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1589338779449463, Accuracy = 0.6099010109901428\n",
      "Iter #248640:  Learning rate = 0.004608:   Batch Loss = 1.094875, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.150437831878662, Accuracy = 0.6178217530250549\n",
      "Iter #249120:  Learning rate = 0.004608:   Batch Loss = 1.150167, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1519678831100464, Accuracy = 0.6089109182357788\n",
      "Iter #249600:  Learning rate = 0.004608:   Batch Loss = 1.156497, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1651537418365479, Accuracy = 0.6237623691558838\n",
      "Iter #250080:  Learning rate = 0.004608:   Batch Loss = 1.247443, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.187317967414856, Accuracy = 0.604950487613678\n",
      "Iter #250560:  Learning rate = 0.004608:   Batch Loss = 1.102117, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2096394300460815, Accuracy = 0.5613861680030823\n",
      "Iter #251040:  Learning rate = 0.004608:   Batch Loss = 1.247381, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2025094032287598, Accuracy = 0.5643564462661743\n",
      "Iter #251520:  Learning rate = 0.004608:   Batch Loss = 1.150360, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.198146939277649, Accuracy = 0.5851485133171082\n",
      "Iter #252000:  Learning rate = 0.004608:   Batch Loss = 1.058971, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.198591709136963, Accuracy = 0.594059407711029\n",
      "Iter #252480:  Learning rate = 0.004608:   Batch Loss = 1.008796, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1884064674377441, Accuracy = 0.594059407711029\n",
      "Iter #252960:  Learning rate = 0.004608:   Batch Loss = 1.197896, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1804941892623901, Accuracy = 0.6118811964988708\n",
      "Iter #253440:  Learning rate = 0.004608:   Batch Loss = 1.026210, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1926790475845337, Accuracy = 0.5821782350540161\n",
      "Iter #253920:  Learning rate = 0.004608:   Batch Loss = 1.115352, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.189049482345581, Accuracy = 0.5683168172836304\n",
      "Iter #254400:  Learning rate = 0.004608:   Batch Loss = 1.155949, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1547826528549194, Accuracy = 0.6207920908927917\n",
      "Iter #254880:  Learning rate = 0.004608:   Batch Loss = 1.083937, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1548477411270142, Accuracy = 0.6316831707954407\n",
      "Iter #255360:  Learning rate = 0.004608:   Batch Loss = 1.007040, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1444107294082642, Accuracy = 0.6396039724349976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #255840:  Learning rate = 0.004608:   Batch Loss = 1.121993, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1572147607803345, Accuracy = 0.6346534490585327\n",
      "Iter #256320:  Learning rate = 0.004608:   Batch Loss = 1.074739, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1453291177749634, Accuracy = 0.6207920908927917\n",
      "Iter #256800:  Learning rate = 0.004608:   Batch Loss = 0.985891, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1508156061172485, Accuracy = 0.6415841579437256\n",
      "Iter #257280:  Learning rate = 0.004608:   Batch Loss = 0.877366, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1417940855026245, Accuracy = 0.6346534490585327\n",
      "Iter #257760:  Learning rate = 0.004608:   Batch Loss = 1.032047, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1298766136169434, Accuracy = 0.6495049595832825\n",
      "Iter #258240:  Learning rate = 0.004608:   Batch Loss = 1.036412, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.149955153465271, Accuracy = 0.6465346813201904\n",
      "Iter #258720:  Learning rate = 0.004608:   Batch Loss = 0.927503, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1538348197937012, Accuracy = 0.6455445289611816\n",
      "Iter #259200:  Learning rate = 0.004608:   Batch Loss = 1.088753, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1775578260421753, Accuracy = 0.6287128925323486\n",
      "Iter #259680:  Learning rate = 0.004608:   Batch Loss = 0.968651, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.160569667816162, Accuracy = 0.6326732635498047\n",
      "Iter #260160:  Learning rate = 0.004608:   Batch Loss = 1.083459, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1629701852798462, Accuracy = 0.6267326474189758\n",
      "Iter #260640:  Learning rate = 0.004608:   Batch Loss = 1.163844, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1721819639205933, Accuracy = 0.6326732635498047\n",
      "Iter #261120:  Learning rate = 0.004608:   Batch Loss = 1.162379, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1490578651428223, Accuracy = 0.6356435418128967\n",
      "Iter #261600:  Learning rate = 0.004608:   Batch Loss = 1.022887, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1261801719665527, Accuracy = 0.6544554233551025\n",
      "Iter #262080:  Learning rate = 0.004608:   Batch Loss = 1.122939, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1345402002334595, Accuracy = 0.6415841579437256\n",
      "Iter #262560:  Learning rate = 0.004608:   Batch Loss = 1.049119, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1297721862792969, Accuracy = 0.6455445289611816\n",
      "Iter #263040:  Learning rate = 0.004608:   Batch Loss = 1.196980, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1286439895629883, Accuracy = 0.6158415675163269\n",
      "Iter #263520:  Learning rate = 0.004608:   Batch Loss = 1.306355, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.157809853553772, Accuracy = 0.6386138796806335\n",
      "Iter #264000:  Learning rate = 0.004608:   Batch Loss = 1.093833, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1663845777511597, Accuracy = 0.6346534490585327\n",
      "Iter #264480:  Learning rate = 0.004608:   Batch Loss = 1.048296, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1935181617736816, Accuracy = 0.6118811964988708\n",
      "Iter #264960:  Learning rate = 0.004608:   Batch Loss = 1.218945, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1754720211029053, Accuracy = 0.6316831707954407\n",
      "Iter #265440:  Learning rate = 0.004608:   Batch Loss = 1.248244, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2448610067367554, Accuracy = 0.5702970027923584\n",
      "Iter #265920:  Learning rate = 0.004608:   Batch Loss = 1.418768, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2042107582092285, Accuracy = 0.5970296859741211\n",
      "Iter #266400:  Learning rate = 0.004608:   Batch Loss = 1.186828, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.264145851135254, Accuracy = 0.5108910799026489\n",
      "Iter #266880:  Learning rate = 0.004608:   Batch Loss = 1.117013, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2317314147949219, Accuracy = 0.5841584205627441\n",
      "Iter #267360:  Learning rate = 0.004608:   Batch Loss = 1.104444, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2206498384475708, Accuracy = 0.5811881422996521\n",
      "Iter #267840:  Learning rate = 0.004608:   Batch Loss = 1.132043, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.202273964881897, Accuracy = 0.5801980495452881\n",
      "Iter #268320:  Learning rate = 0.004608:   Batch Loss = 1.298098, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1789385080337524, Accuracy = 0.6019802093505859\n",
      "Iter #268800:  Learning rate = 0.004608:   Batch Loss = 1.010053, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1685322523117065, Accuracy = 0.6188119053840637\n",
      "Iter #269280:  Learning rate = 0.004608:   Batch Loss = 1.257258, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.149638295173645, Accuracy = 0.6316831707954407\n",
      "Iter #269760:  Learning rate = 0.004608:   Batch Loss = 1.112322, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1573175191879272, Accuracy = 0.6356435418128967\n",
      "Iter #270240:  Learning rate = 0.004608:   Batch Loss = 1.178070, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1561384201049805, Accuracy = 0.6237623691558838\n",
      "Iter #270720:  Learning rate = 0.004608:   Batch Loss = 1.197193, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.156151533126831, Accuracy = 0.60792076587677\n",
      "Iter #271200:  Learning rate = 0.004608:   Batch Loss = 0.986051, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1529995203018188, Accuracy = 0.6158415675163269\n",
      "Iter #271680:  Learning rate = 0.004608:   Batch Loss = 1.078717, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1396907567977905, Accuracy = 0.6237623691558838\n",
      "Iter #272160:  Learning rate = 0.004608:   Batch Loss = 1.171679, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1393141746520996, Accuracy = 0.6287128925323486\n",
      "Iter #272640:  Learning rate = 0.004608:   Batch Loss = 1.028265, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1249524354934692, Accuracy = 0.6465346813201904\n",
      "Iter #273120:  Learning rate = 0.004608:   Batch Loss = 1.121820, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.153679370880127, Accuracy = 0.6346534490585327\n",
      "Iter #273600:  Learning rate = 0.004608:   Batch Loss = 1.086598, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1573327779769897, Accuracy = 0.6445544362068176\n",
      "Iter #274080:  Learning rate = 0.004608:   Batch Loss = 0.908661, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1597011089324951, Accuracy = 0.6326732635498047\n",
      "Iter #274560:  Learning rate = 0.004608:   Batch Loss = 1.021026, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1415531635284424, Accuracy = 0.6356435418128967\n",
      "Iter #275040:  Learning rate = 0.004608:   Batch Loss = 1.137279, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1428825855255127, Accuracy = 0.6316831707954407\n",
      "Iter #275520:  Learning rate = 0.004608:   Batch Loss = 1.120454, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1380561590194702, Accuracy = 0.6534653306007385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #276000:  Learning rate = 0.004608:   Batch Loss = 1.365615, Accuracy = 0.44999998807907104\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.127719759941101, Accuracy = 0.6702970266342163\n",
      "Iter #276480:  Learning rate = 0.004608:   Batch Loss = 1.011684, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.14219069480896, Accuracy = 0.6366336345672607\n",
      "Iter #276960:  Learning rate = 0.004608:   Batch Loss = 1.277903, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1220457553863525, Accuracy = 0.6603960394859314\n",
      "Iter #277440:  Learning rate = 0.004608:   Batch Loss = 1.138156, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1100348234176636, Accuracy = 0.6702970266342163\n",
      "Iter #277920:  Learning rate = 0.004608:   Batch Loss = 0.869372, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1330403089523315, Accuracy = 0.6475247740745544\n",
      "Iter #278400:  Learning rate = 0.004608:   Batch Loss = 1.110451, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1453356742858887, Accuracy = 0.6514851450920105\n",
      "Iter #278880:  Learning rate = 0.004608:   Batch Loss = 1.058125, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1382085084915161, Accuracy = 0.6702970266342163\n",
      "Iter #279360:  Learning rate = 0.004608:   Batch Loss = 1.271242, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1871806383132935, Accuracy = 0.6316831707954407\n",
      "Iter #279840:  Learning rate = 0.004608:   Batch Loss = 1.129127, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.171311616897583, Accuracy = 0.6257425546646118\n",
      "Iter #280320:  Learning rate = 0.004608:   Batch Loss = 1.079943, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1664555072784424, Accuracy = 0.6326732635498047\n",
      "Iter #280800:  Learning rate = 0.004608:   Batch Loss = 1.070008, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1507099866867065, Accuracy = 0.6485148668289185\n",
      "Iter #281280:  Learning rate = 0.004608:   Batch Loss = 1.006807, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1670000553131104, Accuracy = 0.6475247740745544\n",
      "Iter #281760:  Learning rate = 0.004608:   Batch Loss = 1.075696, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2408267259597778, Accuracy = 0.6019802093505859\n",
      "Iter #282240:  Learning rate = 0.004608:   Batch Loss = 1.088703, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.3104254007339478, Accuracy = 0.5306930541992188\n",
      "Iter #282720:  Learning rate = 0.004608:   Batch Loss = 1.202384, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2367286682128906, Accuracy = 0.5831683278083801\n",
      "Iter #283200:  Learning rate = 0.004608:   Batch Loss = 1.174952, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2145036458969116, Accuracy = 0.6108911037445068\n",
      "Iter #283680:  Learning rate = 0.004608:   Batch Loss = 1.058018, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1689373254776, Accuracy = 0.6336633563041687\n",
      "Iter #284160:  Learning rate = 0.004608:   Batch Loss = 1.022727, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.158651351928711, Accuracy = 0.6089109182357788\n",
      "Iter #284640:  Learning rate = 0.004608:   Batch Loss = 1.029661, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1487120389938354, Accuracy = 0.6237623691558838\n",
      "Iter #285120:  Learning rate = 0.004608:   Batch Loss = 1.117517, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1379213333129883, Accuracy = 0.6376237869262695\n",
      "Iter #285600:  Learning rate = 0.004608:   Batch Loss = 1.077560, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1307743787765503, Accuracy = 0.6366336345672607\n",
      "Iter #286080:  Learning rate = 0.004608:   Batch Loss = 1.034840, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1550955772399902, Accuracy = 0.6316831707954407\n",
      "Iter #286560:  Learning rate = 0.004608:   Batch Loss = 1.222897, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1520891189575195, Accuracy = 0.6128712892532349\n",
      "Iter #287040:  Learning rate = 0.004608:   Batch Loss = 0.996856, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1453285217285156, Accuracy = 0.6435643434524536\n",
      "Iter #287520:  Learning rate = 0.004608:   Batch Loss = 1.082444, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1388115882873535, Accuracy = 0.6752475500106812\n",
      "Iter #288000:  Learning rate = 0.004608:   Batch Loss = 0.961499, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1083872318267822, Accuracy = 0.6594059467315674\n",
      "Iter #288480:  Learning rate = 0.004608:   Batch Loss = 0.992772, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1191775798797607, Accuracy = 0.6693069338798523\n",
      "Iter #288960:  Learning rate = 0.004608:   Batch Loss = 1.167253, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1036732196807861, Accuracy = 0.6752475500106812\n",
      "Iter #289440:  Learning rate = 0.004608:   Batch Loss = 1.017316, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1040061712265015, Accuracy = 0.6613861322402954\n",
      "Iter #289920:  Learning rate = 0.004608:   Batch Loss = 0.979499, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1092803478240967, Accuracy = 0.6643564105033875\n",
      "Iter #290400:  Learning rate = 0.004608:   Batch Loss = 1.062139, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1165833473205566, Accuracy = 0.6574257612228394\n",
      "Iter #290880:  Learning rate = 0.004608:   Batch Loss = 1.059499, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1311084032058716, Accuracy = 0.6673267483711243\n",
      "Iter #291360:  Learning rate = 0.004608:   Batch Loss = 0.966281, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1002694368362427, Accuracy = 0.6663366556167603\n",
      "Iter #291840:  Learning rate = 0.004608:   Batch Loss = 1.004149, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0898929834365845, Accuracy = 0.6633663177490234\n",
      "Iter #292320:  Learning rate = 0.004608:   Batch Loss = 1.006414, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1279642581939697, Accuracy = 0.6316831707954407\n",
      "Iter #292800:  Learning rate = 0.004608:   Batch Loss = 1.123676, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.13568115234375, Accuracy = 0.6415841579437256\n",
      "Iter #293280:  Learning rate = 0.004608:   Batch Loss = 1.126138, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1684404611587524, Accuracy = 0.6237623691558838\n",
      "Iter #293760:  Learning rate = 0.004608:   Batch Loss = 0.960267, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1212183237075806, Accuracy = 0.6584158539772034\n",
      "Iter #294240:  Learning rate = 0.004608:   Batch Loss = 1.020954, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.098228931427002, Accuracy = 0.695049524307251\n",
      "Iter #294720:  Learning rate = 0.004608:   Batch Loss = 1.001532, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1046860218048096, Accuracy = 0.6851485371589661\n",
      "Iter #295200:  Learning rate = 0.004608:   Batch Loss = 1.146082, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.124704122543335, Accuracy = 0.6247524619102478\n",
      "Iter #295680:  Learning rate = 0.004608:   Batch Loss = 0.915117, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1227879524230957, Accuracy = 0.6366336345672607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #296160:  Learning rate = 0.004608:   Batch Loss = 0.925206, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1188085079193115, Accuracy = 0.6544554233551025\n",
      "Iter #296640:  Learning rate = 0.004608:   Batch Loss = 0.902224, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1065207719802856, Accuracy = 0.6574257612228394\n",
      "Iter #297120:  Learning rate = 0.004608:   Batch Loss = 1.224872, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1356061697006226, Accuracy = 0.6435643434524536\n",
      "Iter #297600:  Learning rate = 0.004608:   Batch Loss = 0.986928, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1350804567337036, Accuracy = 0.6366336345672607\n",
      "Iter #298080:  Learning rate = 0.004608:   Batch Loss = 1.010711, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1630339622497559, Accuracy = 0.592079222202301\n",
      "Iter #298560:  Learning rate = 0.004608:   Batch Loss = 1.006482, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1605368852615356, Accuracy = 0.60297030210495\n",
      "Iter #299040:  Learning rate = 0.004608:   Batch Loss = 1.052149, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1215829849243164, Accuracy = 0.6277227997779846\n",
      "Iter #299520:  Learning rate = 0.004608:   Batch Loss = 1.081421, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1038224697113037, Accuracy = 0.6386138796806335\n",
      "Iter #300000:  Learning rate = 0.004424:   Batch Loss = 1.090079, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.229880928993225, Accuracy = 0.5504950284957886\n",
      "Iter #300480:  Learning rate = 0.004424:   Batch Loss = 1.207258, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1910316944122314, Accuracy = 0.6138613820075989\n",
      "Iter #300960:  Learning rate = 0.004424:   Batch Loss = 1.049045, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.132655382156372, Accuracy = 0.6465346813201904\n",
      "Iter #301440:  Learning rate = 0.004424:   Batch Loss = 1.091844, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.120147705078125, Accuracy = 0.6633663177490234\n",
      "Iter #301920:  Learning rate = 0.004424:   Batch Loss = 1.086183, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.104531168937683, Accuracy = 0.6702970266342163\n",
      "Iter #302400:  Learning rate = 0.004424:   Batch Loss = 0.974478, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.105264663696289, Accuracy = 0.6702970266342163\n",
      "Iter #302880:  Learning rate = 0.004424:   Batch Loss = 1.026260, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1355584859848022, Accuracy = 0.6386138796806335\n",
      "Iter #303360:  Learning rate = 0.004424:   Batch Loss = 1.032722, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1101548671722412, Accuracy = 0.6623762249946594\n",
      "Iter #303840:  Learning rate = 0.004424:   Batch Loss = 1.045217, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1194978952407837, Accuracy = 0.6801980137825012\n",
      "Iter #304320:  Learning rate = 0.004424:   Batch Loss = 1.068909, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1162405014038086, Accuracy = 0.6633663177490234\n",
      "Iter #304800:  Learning rate = 0.004424:   Batch Loss = 1.080143, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1072818040847778, Accuracy = 0.6475247740745544\n",
      "Iter #305280:  Learning rate = 0.004424:   Batch Loss = 0.967126, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.125959038734436, Accuracy = 0.6594059467315674\n",
      "Iter #305760:  Learning rate = 0.004424:   Batch Loss = 1.088189, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1822057962417603, Accuracy = 0.6336633563041687\n",
      "Iter #306240:  Learning rate = 0.004424:   Batch Loss = 1.250854, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.178434133529663, Accuracy = 0.6287128925323486\n",
      "Iter #306720:  Learning rate = 0.004424:   Batch Loss = 1.043287, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1341718435287476, Accuracy = 0.6633663177490234\n",
      "Iter #307200:  Learning rate = 0.004424:   Batch Loss = 1.028691, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1380794048309326, Accuracy = 0.6584158539772034\n",
      "Iter #307680:  Learning rate = 0.004424:   Batch Loss = 1.107664, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1186267137527466, Accuracy = 0.6574257612228394\n",
      "Iter #308160:  Learning rate = 0.004424:   Batch Loss = 1.104947, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1385395526885986, Accuracy = 0.6435643434524536\n",
      "Iter #308640:  Learning rate = 0.004424:   Batch Loss = 0.917393, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.107332706451416, Accuracy = 0.6633663177490234\n",
      "Iter #309120:  Learning rate = 0.004424:   Batch Loss = 1.108862, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1075066328048706, Accuracy = 0.6732673048973083\n",
      "Iter #309600:  Learning rate = 0.004424:   Batch Loss = 1.075442, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1483551263809204, Accuracy = 0.6277227997779846\n",
      "Iter #310080:  Learning rate = 0.004424:   Batch Loss = 1.172331, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.109230399131775, Accuracy = 0.6782178282737732\n",
      "Iter #310560:  Learning rate = 0.004424:   Batch Loss = 1.136662, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1362931728363037, Accuracy = 0.6475247740745544\n",
      "Iter #311040:  Learning rate = 0.004424:   Batch Loss = 0.989957, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1828922033309937, Accuracy = 0.6148514747619629\n",
      "Iter #311520:  Learning rate = 0.004424:   Batch Loss = 1.057368, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1735588312149048, Accuracy = 0.603960394859314\n",
      "Iter #312000:  Learning rate = 0.004424:   Batch Loss = 1.183720, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1595677137374878, Accuracy = 0.6099010109901428\n",
      "Iter #312480:  Learning rate = 0.004424:   Batch Loss = 0.869461, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2018495798110962, Accuracy = 0.6108911037445068\n",
      "Iter #312960:  Learning rate = 0.004424:   Batch Loss = 1.067989, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1717525720596313, Accuracy = 0.6099010109901428\n",
      "Iter #313440:  Learning rate = 0.004424:   Batch Loss = 0.978489, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.233534574508667, Accuracy = 0.5673267245292664\n",
      "Iter #313920:  Learning rate = 0.004424:   Batch Loss = 0.993833, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2051024436950684, Accuracy = 0.6089109182357788\n",
      "Iter #314400:  Learning rate = 0.004424:   Batch Loss = 1.058120, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1939334869384766, Accuracy = 0.603960394859314\n",
      "Iter #314880:  Learning rate = 0.004424:   Batch Loss = 1.066125, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.165095329284668, Accuracy = 0.6326732635498047\n",
      "Iter #315360:  Learning rate = 0.004424:   Batch Loss = 1.209830, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1743367910385132, Accuracy = 0.6217821836471558\n",
      "Iter #315840:  Learning rate = 0.004424:   Batch Loss = 1.025594, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1592768430709839, Accuracy = 0.6247524619102478\n",
      "Iter #316320:  Learning rate = 0.004424:   Batch Loss = 1.120858, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1212329864501953, Accuracy = 0.6336633563041687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #316800:  Learning rate = 0.004424:   Batch Loss = 1.045403, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1574327945709229, Accuracy = 0.6118811964988708\n",
      "Iter #317280:  Learning rate = 0.004424:   Batch Loss = 1.000540, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1211920976638794, Accuracy = 0.6495049595832825\n",
      "Iter #317760:  Learning rate = 0.004424:   Batch Loss = 1.028127, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0878026485443115, Accuracy = 0.6821781992912292\n",
      "Iter #318240:  Learning rate = 0.004424:   Batch Loss = 1.117192, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.126128911972046, Accuracy = 0.6277227997779846\n",
      "Iter #318720:  Learning rate = 0.004424:   Batch Loss = 1.082660, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1031337976455688, Accuracy = 0.6643564105033875\n",
      "Iter #319200:  Learning rate = 0.004424:   Batch Loss = 1.141555, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2433924674987793, Accuracy = 0.591089129447937\n",
      "Iter #319680:  Learning rate = 0.004424:   Batch Loss = 1.168478, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1998600959777832, Accuracy = 0.5534653663635254\n",
      "Iter #320160:  Learning rate = 0.004424:   Batch Loss = 1.255266, Accuracy = 0.5\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2420012950897217, Accuracy = 0.5178217887878418\n",
      "Iter #320640:  Learning rate = 0.004424:   Batch Loss = 1.148909, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2228620052337646, Accuracy = 0.5762376189231873\n",
      "Iter #321120:  Learning rate = 0.004424:   Batch Loss = 1.066084, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2302851676940918, Accuracy = 0.5712871551513672\n",
      "Iter #321600:  Learning rate = 0.004424:   Batch Loss = 1.459041, Accuracy = 0.46666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2471338510513306, Accuracy = 0.5445544719696045\n",
      "Iter #322080:  Learning rate = 0.004424:   Batch Loss = 1.326614, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2316055297851562, Accuracy = 0.5891088843345642\n",
      "Iter #322560:  Learning rate = 0.004424:   Batch Loss = 1.379942, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2062374353408813, Accuracy = 0.5653465390205383\n",
      "Iter #323040:  Learning rate = 0.004424:   Batch Loss = 1.123037, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1543245315551758, Accuracy = 0.6118811964988708\n",
      "Iter #323520:  Learning rate = 0.004424:   Batch Loss = 1.126932, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1730444431304932, Accuracy = 0.5732673406600952\n",
      "Iter #324000:  Learning rate = 0.004424:   Batch Loss = 1.064038, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.169597864151001, Accuracy = 0.5990098714828491\n",
      "Iter #324480:  Learning rate = 0.004424:   Batch Loss = 1.081548, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1834824085235596, Accuracy = 0.5762376189231873\n",
      "Iter #324960:  Learning rate = 0.004424:   Batch Loss = 1.082496, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2000752687454224, Accuracy = 0.5623762607574463\n",
      "Iter #325440:  Learning rate = 0.004424:   Batch Loss = 1.226980, Accuracy = 0.550000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1690021753311157, Accuracy = 0.60297030210495\n",
      "Iter #325920:  Learning rate = 0.004424:   Batch Loss = 1.134172, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.158591866493225, Accuracy = 0.6207920908927917\n",
      "Iter #326400:  Learning rate = 0.004424:   Batch Loss = 1.203427, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.186922311782837, Accuracy = 0.592079222202301\n",
      "Iter #326880:  Learning rate = 0.004424:   Batch Loss = 1.186468, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1694742441177368, Accuracy = 0.590099036693573\n",
      "Iter #327360:  Learning rate = 0.004424:   Batch Loss = 1.089480, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.217209815979004, Accuracy = 0.5782178044319153\n",
      "Iter #327840:  Learning rate = 0.004424:   Batch Loss = 1.341135, Accuracy = 0.4333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1953151226043701, Accuracy = 0.5782178044319153\n",
      "Iter #328320:  Learning rate = 0.004424:   Batch Loss = 1.154037, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1631070375442505, Accuracy = 0.605940580368042\n",
      "Iter #328800:  Learning rate = 0.004424:   Batch Loss = 1.133286, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.162785530090332, Accuracy = 0.6346534490585327\n",
      "Iter #329280:  Learning rate = 0.004424:   Batch Loss = 1.057861, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.159085750579834, Accuracy = 0.6178217530250549\n",
      "Iter #329760:  Learning rate = 0.004424:   Batch Loss = 1.065275, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1449315547943115, Accuracy = 0.6514851450920105\n",
      "Iter #330240:  Learning rate = 0.004424:   Batch Loss = 1.051120, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1304925680160522, Accuracy = 0.6326732635498047\n",
      "Iter #330720:  Learning rate = 0.004424:   Batch Loss = 1.102708, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1248083114624023, Accuracy = 0.6247524619102478\n",
      "Iter #331200:  Learning rate = 0.004424:   Batch Loss = 1.011931, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0972297191619873, Accuracy = 0.6683168411254883\n",
      "Iter #331680:  Learning rate = 0.004424:   Batch Loss = 1.077610, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1101436614990234, Accuracy = 0.6544554233551025\n",
      "Iter #332160:  Learning rate = 0.004424:   Batch Loss = 1.057832, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1089781522750854, Accuracy = 0.6524752378463745\n",
      "Iter #332640:  Learning rate = 0.004424:   Batch Loss = 1.027757, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0848721265792847, Accuracy = 0.6732673048973083\n",
      "Iter #333120:  Learning rate = 0.004424:   Batch Loss = 1.028619, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0986237525939941, Accuracy = 0.6653465628623962\n",
      "Iter #333600:  Learning rate = 0.004424:   Batch Loss = 1.136207, Accuracy = 0.5166666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0826352834701538, Accuracy = 0.6524752378463745\n",
      "Iter #334080:  Learning rate = 0.004424:   Batch Loss = 1.218930, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1060913801193237, Accuracy = 0.6376237869262695\n",
      "Iter #334560:  Learning rate = 0.004424:   Batch Loss = 1.105083, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0943236351013184, Accuracy = 0.6722772121429443\n",
      "Iter #335040:  Learning rate = 0.004424:   Batch Loss = 0.973668, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.101322054862976, Accuracy = 0.6554455161094666\n",
      "Iter #335520:  Learning rate = 0.004424:   Batch Loss = 0.991809, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0836597681045532, Accuracy = 0.6613861322402954\n",
      "Iter #336000:  Learning rate = 0.004424:   Batch Loss = 0.946865, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.082148551940918, Accuracy = 0.684158444404602\n",
      "Iter #336480:  Learning rate = 0.004424:   Batch Loss = 1.050715, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.072219729423523, Accuracy = 0.6782178282737732\n",
      "Iter #336960:  Learning rate = 0.004424:   Batch Loss = 1.082765, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0847562551498413, Accuracy = 0.6702970266342163\n",
      "Iter #337440:  Learning rate = 0.004424:   Batch Loss = 1.041902, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.099256157875061, Accuracy = 0.6722772121429443\n",
      "Iter #337920:  Learning rate = 0.004424:   Batch Loss = 0.993831, Accuracy = 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1008362770080566, Accuracy = 0.6504950523376465\n",
      "Iter #338400:  Learning rate = 0.004424:   Batch Loss = 1.070603, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0869839191436768, Accuracy = 0.6693069338798523\n",
      "Iter #338880:  Learning rate = 0.004424:   Batch Loss = 1.006989, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0689141750335693, Accuracy = 0.6732673048973083\n",
      "Iter #339360:  Learning rate = 0.004424:   Batch Loss = 1.011873, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0667939186096191, Accuracy = 0.6920791864395142\n",
      "Iter #339840:  Learning rate = 0.004424:   Batch Loss = 1.086972, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0794503688812256, Accuracy = 0.6722772121429443\n",
      "Iter #340320:  Learning rate = 0.004424:   Batch Loss = 1.064609, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.066763162612915, Accuracy = 0.6702970266342163\n",
      "Iter #340800:  Learning rate = 0.004424:   Batch Loss = 1.015179, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.141135573387146, Accuracy = 0.6207920908927917\n",
      "Iter #341280:  Learning rate = 0.004424:   Batch Loss = 1.076535, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.132839560508728, Accuracy = 0.6128712892532349\n",
      "Iter #341760:  Learning rate = 0.004424:   Batch Loss = 1.172962, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1090424060821533, Accuracy = 0.6485148668289185\n",
      "Iter #342240:  Learning rate = 0.004424:   Batch Loss = 1.019949, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1131269931793213, Accuracy = 0.6544554233551025\n",
      "Iter #342720:  Learning rate = 0.004424:   Batch Loss = 1.035728, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0978449583053589, Accuracy = 0.6732673048973083\n",
      "Iter #343200:  Learning rate = 0.004424:   Batch Loss = 1.154932, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1026372909545898, Accuracy = 0.6574257612228394\n",
      "Iter #343680:  Learning rate = 0.004424:   Batch Loss = 0.858623, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1424732208251953, Accuracy = 0.6188119053840637\n",
      "Iter #344160:  Learning rate = 0.004424:   Batch Loss = 1.157164, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1673024892807007, Accuracy = 0.6118811964988708\n",
      "Iter #344640:  Learning rate = 0.004424:   Batch Loss = 1.039392, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1417521238327026, Accuracy = 0.6188119053840637\n",
      "Iter #345120:  Learning rate = 0.004424:   Batch Loss = 1.034238, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1360368728637695, Accuracy = 0.6554455161094666\n",
      "Iter #345600:  Learning rate = 0.004424:   Batch Loss = 1.087735, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.186869740486145, Accuracy = 0.590099036693573\n",
      "Iter #346080:  Learning rate = 0.004424:   Batch Loss = 1.028225, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.2232000827789307, Accuracy = 0.5980197787284851\n",
      "Iter #346560:  Learning rate = 0.004424:   Batch Loss = 1.188364, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.174703598022461, Accuracy = 0.6108911037445068\n",
      "Iter #347040:  Learning rate = 0.004424:   Batch Loss = 1.173883, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1447519063949585, Accuracy = 0.5990098714828491\n",
      "Iter #347520:  Learning rate = 0.004424:   Batch Loss = 1.035896, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.120233178138733, Accuracy = 0.6267326474189758\n",
      "Iter #348000:  Learning rate = 0.004424:   Batch Loss = 1.179686, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1077619791030884, Accuracy = 0.6613861322402954\n",
      "Iter #348480:  Learning rate = 0.004424:   Batch Loss = 1.037068, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1168229579925537, Accuracy = 0.6613861322402954\n",
      "Iter #348960:  Learning rate = 0.004424:   Batch Loss = 1.048370, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1342524290084839, Accuracy = 0.6287128925323486\n",
      "Iter #349440:  Learning rate = 0.004424:   Batch Loss = 1.024496, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.105190396308899, Accuracy = 0.6405940651893616\n",
      "Iter #349920:  Learning rate = 0.004424:   Batch Loss = 1.050058, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1232367753982544, Accuracy = 0.6554455161094666\n",
      "Iter #350400:  Learning rate = 0.004424:   Batch Loss = 1.074427, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1160920858383179, Accuracy = 0.6415841579437256\n",
      "Iter #350880:  Learning rate = 0.004424:   Batch Loss = 1.069586, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.109785795211792, Accuracy = 0.6722772121429443\n",
      "Iter #351360:  Learning rate = 0.004424:   Batch Loss = 0.982282, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.129998803138733, Accuracy = 0.6623762249946594\n",
      "Iter #351840:  Learning rate = 0.004424:   Batch Loss = 0.976371, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.121160626411438, Accuracy = 0.6564356684684753\n",
      "Iter #352320:  Learning rate = 0.004424:   Batch Loss = 0.918410, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1080832481384277, Accuracy = 0.6574257612228394\n",
      "Iter #352800:  Learning rate = 0.004424:   Batch Loss = 0.941285, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0957415103912354, Accuracy = 0.6613861322402954\n",
      "Iter #353280:  Learning rate = 0.004424:   Batch Loss = 0.963858, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.080113172531128, Accuracy = 0.6782178282737732\n",
      "Iter #353760:  Learning rate = 0.004424:   Batch Loss = 1.136823, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0876891613006592, Accuracy = 0.6881188154220581\n",
      "Iter #354240:  Learning rate = 0.004424:   Batch Loss = 0.924169, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1145069599151611, Accuracy = 0.6584158539772034\n",
      "Iter #354720:  Learning rate = 0.004424:   Batch Loss = 1.028536, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1322777271270752, Accuracy = 0.6425742506980896\n",
      "Iter #355200:  Learning rate = 0.004424:   Batch Loss = 1.017219, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1287527084350586, Accuracy = 0.6396039724349976\n",
      "Iter #355680:  Learning rate = 0.004424:   Batch Loss = 0.940060, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.080830454826355, Accuracy = 0.6564356684684753\n",
      "Iter #356160:  Learning rate = 0.004424:   Batch Loss = 0.997800, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.11056387424469, Accuracy = 0.6673267483711243\n",
      "Iter #356640:  Learning rate = 0.004424:   Batch Loss = 1.105259, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0803276300430298, Accuracy = 0.7009900808334351\n",
      "Iter #357120:  Learning rate = 0.004424:   Batch Loss = 0.942260, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.083594799041748, Accuracy = 0.6594059467315674\n",
      "Iter #357600:  Learning rate = 0.004424:   Batch Loss = 0.899461, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.057298183441162, Accuracy = 0.699009895324707\n",
      "Iter #358080:  Learning rate = 0.004424:   Batch Loss = 0.997976, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0932676792144775, Accuracy = 0.6831682920455933\n",
      "Iter #358560:  Learning rate = 0.004424:   Batch Loss = 1.151773, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0803437232971191, Accuracy = 0.6851485371589661\n",
      "Iter #359040:  Learning rate = 0.004424:   Batch Loss = 1.207911, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0530633926391602, Accuracy = 0.7069306969642639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #359520:  Learning rate = 0.004424:   Batch Loss = 0.989342, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0644639730453491, Accuracy = 0.711881160736084\n",
      "Iter #360000:  Learning rate = 0.004424:   Batch Loss = 0.896317, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0271046161651611, Accuracy = 0.7277227640151978\n",
      "Iter #360480:  Learning rate = 0.004424:   Batch Loss = 0.892330, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0450263023376465, Accuracy = 0.7039604187011719\n",
      "Iter #360960:  Learning rate = 0.004424:   Batch Loss = 0.908017, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0496320724487305, Accuracy = 0.696039617061615\n",
      "Iter #361440:  Learning rate = 0.004424:   Batch Loss = 1.152527, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0529930591583252, Accuracy = 0.709900975227356\n",
      "Iter #361920:  Learning rate = 0.004424:   Batch Loss = 0.846679, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0535224676132202, Accuracy = 0.6831682920455933\n",
      "Iter #362400:  Learning rate = 0.004424:   Batch Loss = 1.136644, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0530757904052734, Accuracy = 0.6861386299133301\n",
      "Iter #362880:  Learning rate = 0.004424:   Batch Loss = 1.180708, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0429871082305908, Accuracy = 0.7059406042098999\n",
      "Iter #363360:  Learning rate = 0.004424:   Batch Loss = 0.961833, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0423389673233032, Accuracy = 0.709900975227356\n",
      "Iter #363840:  Learning rate = 0.004424:   Batch Loss = 1.156855, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0697691440582275, Accuracy = 0.6881188154220581\n",
      "Iter #364320:  Learning rate = 0.004424:   Batch Loss = 1.003436, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0929871797561646, Accuracy = 0.6603960394859314\n",
      "Iter #364800:  Learning rate = 0.004424:   Batch Loss = 0.931413, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0702542066574097, Accuracy = 0.6930692791938782\n",
      "Iter #365280:  Learning rate = 0.004424:   Batch Loss = 1.149963, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0850021839141846, Accuracy = 0.6891089081764221\n",
      "Iter #365760:  Learning rate = 0.004424:   Batch Loss = 1.083994, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.097638726234436, Accuracy = 0.6801980137825012\n",
      "Iter #366240:  Learning rate = 0.004424:   Batch Loss = 1.009950, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1115154027938843, Accuracy = 0.6871287226676941\n",
      "Iter #366720:  Learning rate = 0.004424:   Batch Loss = 1.125498, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.197037696838379, Accuracy = 0.6108911037445068\n",
      "Iter #367200:  Learning rate = 0.004424:   Batch Loss = 1.003823, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1098510026931763, Accuracy = 0.6663366556167603\n",
      "Iter #367680:  Learning rate = 0.004424:   Batch Loss = 1.125546, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1386178731918335, Accuracy = 0.6514851450920105\n",
      "Iter #368160:  Learning rate = 0.004424:   Batch Loss = 1.118893, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1344664096832275, Accuracy = 0.6574257612228394\n",
      "Iter #368640:  Learning rate = 0.004424:   Batch Loss = 1.053832, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1600619554519653, Accuracy = 0.6346534490585327\n",
      "Iter #369120:  Learning rate = 0.004424:   Batch Loss = 1.118601, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1267738342285156, Accuracy = 0.6306930780410767\n",
      "Iter #369600:  Learning rate = 0.004424:   Batch Loss = 1.084005, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1015013456344604, Accuracy = 0.6623762249946594\n",
      "Iter #370080:  Learning rate = 0.004424:   Batch Loss = 1.027997, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1480544805526733, Accuracy = 0.6584158539772034\n",
      "Iter #370560:  Learning rate = 0.004424:   Batch Loss = 1.101603, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1101919412612915, Accuracy = 0.6831682920455933\n",
      "Iter #371040:  Learning rate = 0.004424:   Batch Loss = 1.018554, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.072716236114502, Accuracy = 0.6772277355194092\n",
      "Iter #371520:  Learning rate = 0.004424:   Batch Loss = 1.063259, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1085078716278076, Accuracy = 0.6702970266342163\n",
      "Iter #372000:  Learning rate = 0.004424:   Batch Loss = 0.970363, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0883806943893433, Accuracy = 0.6891089081764221\n",
      "Iter #372480:  Learning rate = 0.004424:   Batch Loss = 1.008804, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0608704090118408, Accuracy = 0.7158415913581848\n",
      "Iter #372960:  Learning rate = 0.004424:   Batch Loss = 1.103967, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.070471167564392, Accuracy = 0.6811881065368652\n",
      "Iter #373440:  Learning rate = 0.004424:   Batch Loss = 1.121052, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.086350917816162, Accuracy = 0.6772277355194092\n",
      "Iter #373920:  Learning rate = 0.004424:   Batch Loss = 1.146695, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0489495992660522, Accuracy = 0.696039617061615\n",
      "Iter #374400:  Learning rate = 0.004424:   Batch Loss = 1.024506, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.072737216949463, Accuracy = 0.6920791864395142\n",
      "Iter #374880:  Learning rate = 0.004424:   Batch Loss = 0.988987, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0896543264389038, Accuracy = 0.6861386299133301\n",
      "Iter #375360:  Learning rate = 0.004424:   Batch Loss = 1.064330, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0748072862625122, Accuracy = 0.6910890936851501\n",
      "Iter #375840:  Learning rate = 0.004424:   Batch Loss = 1.087838, Accuracy = 0.5833333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0729069709777832, Accuracy = 0.6821781992912292\n",
      "Iter #376320:  Learning rate = 0.004424:   Batch Loss = 1.036180, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0693012475967407, Accuracy = 0.6930692791938782\n",
      "Iter #376800:  Learning rate = 0.004424:   Batch Loss = 1.061966, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0793530941009521, Accuracy = 0.699999988079071\n",
      "Iter #377280:  Learning rate = 0.004424:   Batch Loss = 0.950924, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0276637077331543, Accuracy = 0.7306930422782898\n",
      "Iter #377760:  Learning rate = 0.004424:   Batch Loss = 1.104775, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0984874963760376, Accuracy = 0.6594059467315674\n",
      "Iter #378240:  Learning rate = 0.004424:   Batch Loss = 0.886882, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0987683534622192, Accuracy = 0.6811881065368652\n",
      "Iter #378720:  Learning rate = 0.004424:   Batch Loss = 0.957158, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.084502935409546, Accuracy = 0.6821781992912292\n",
      "Iter #379200:  Learning rate = 0.004424:   Batch Loss = 1.046172, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0619405508041382, Accuracy = 0.7049505114555359\n",
      "Iter #379680:  Learning rate = 0.004424:   Batch Loss = 1.067518, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0957566499710083, Accuracy = 0.6851485371589661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #380160:  Learning rate = 0.004424:   Batch Loss = 1.142757, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.079988718032837, Accuracy = 0.6762376427650452\n",
      "Iter #380640:  Learning rate = 0.004424:   Batch Loss = 0.972092, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0746922492980957, Accuracy = 0.6881188154220581\n",
      "Iter #381120:  Learning rate = 0.004424:   Batch Loss = 1.097369, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0815436840057373, Accuracy = 0.684158444404602\n",
      "Iter #381600:  Learning rate = 0.004424:   Batch Loss = 0.864658, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.144758939743042, Accuracy = 0.6465346813201904\n",
      "Iter #382080:  Learning rate = 0.004424:   Batch Loss = 1.065160, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.135664701461792, Accuracy = 0.6722772121429443\n",
      "Iter #382560:  Learning rate = 0.004424:   Batch Loss = 1.166875, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.106748342514038, Accuracy = 0.6742573976516724\n",
      "Iter #383040:  Learning rate = 0.004424:   Batch Loss = 1.087120, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1293940544128418, Accuracy = 0.6663366556167603\n",
      "Iter #383520:  Learning rate = 0.004424:   Batch Loss = 1.072692, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1259231567382812, Accuracy = 0.6663366556167603\n",
      "Iter #384000:  Learning rate = 0.004424:   Batch Loss = 1.137290, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0843440294265747, Accuracy = 0.698019802570343\n",
      "Iter #384480:  Learning rate = 0.004424:   Batch Loss = 0.910947, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0931158065795898, Accuracy = 0.6881188154220581\n",
      "Iter #384960:  Learning rate = 0.004424:   Batch Loss = 1.166579, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1105159521102905, Accuracy = 0.6900990009307861\n",
      "Iter #385440:  Learning rate = 0.004424:   Batch Loss = 1.160944, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1043806076049805, Accuracy = 0.6910890936851501\n",
      "Iter #385920:  Learning rate = 0.004424:   Batch Loss = 1.066305, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1127965450286865, Accuracy = 0.6792079210281372\n",
      "Iter #386400:  Learning rate = 0.004424:   Batch Loss = 1.051451, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1015512943267822, Accuracy = 0.6683168411254883\n",
      "Iter #386880:  Learning rate = 0.004424:   Batch Loss = 1.088274, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1198574304580688, Accuracy = 0.6356435418128967\n",
      "Iter #387360:  Learning rate = 0.004424:   Batch Loss = 0.894348, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1050626039505005, Accuracy = 0.6712871193885803\n",
      "Iter #387840:  Learning rate = 0.004424:   Batch Loss = 0.984245, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0967183113098145, Accuracy = 0.6762376427650452\n",
      "Iter #388320:  Learning rate = 0.004424:   Batch Loss = 1.044125, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1011816263198853, Accuracy = 0.6623762249946594\n",
      "Iter #388800:  Learning rate = 0.004424:   Batch Loss = 0.880359, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.088855266571045, Accuracy = 0.6891089081764221\n",
      "Iter #389280:  Learning rate = 0.004424:   Batch Loss = 1.217847, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.073438286781311, Accuracy = 0.6861386299133301\n",
      "Iter #389760:  Learning rate = 0.004424:   Batch Loss = 0.921611, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0472561120986938, Accuracy = 0.7079207897186279\n",
      "Iter #390240:  Learning rate = 0.004424:   Batch Loss = 1.142920, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0691707134246826, Accuracy = 0.6881188154220581\n",
      "Iter #390720:  Learning rate = 0.004424:   Batch Loss = 1.094326, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.138672947883606, Accuracy = 0.6504950523376465\n",
      "Iter #391200:  Learning rate = 0.004424:   Batch Loss = 1.097501, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1365196704864502, Accuracy = 0.6504950523376465\n",
      "Iter #391680:  Learning rate = 0.004424:   Batch Loss = 1.393103, Accuracy = 0.6000000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1043357849121094, Accuracy = 0.6881188154220581\n",
      "Iter #392160:  Learning rate = 0.004424:   Batch Loss = 1.168212, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0747802257537842, Accuracy = 0.696039617061615\n",
      "Iter #392640:  Learning rate = 0.004424:   Batch Loss = 0.984088, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0797522068023682, Accuracy = 0.6881188154220581\n",
      "Iter #393120:  Learning rate = 0.004424:   Batch Loss = 1.079358, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0721039772033691, Accuracy = 0.6801980137825012\n",
      "Iter #393600:  Learning rate = 0.004424:   Batch Loss = 0.889554, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0712153911590576, Accuracy = 0.698019802570343\n",
      "Iter #394080:  Learning rate = 0.004424:   Batch Loss = 0.981817, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0663517713546753, Accuracy = 0.6930692791938782\n",
      "Iter #394560:  Learning rate = 0.004424:   Batch Loss = 0.879931, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0882177352905273, Accuracy = 0.6752475500106812\n",
      "Iter #395040:  Learning rate = 0.004424:   Batch Loss = 1.069008, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.076565146446228, Accuracy = 0.6900990009307861\n",
      "Iter #395520:  Learning rate = 0.004424:   Batch Loss = 0.986587, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0429331064224243, Accuracy = 0.7059406042098999\n",
      "Iter #396000:  Learning rate = 0.004424:   Batch Loss = 0.878910, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0457115173339844, Accuracy = 0.7049505114555359\n",
      "Iter #396480:  Learning rate = 0.004424:   Batch Loss = 1.054764, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0511183738708496, Accuracy = 0.699009895324707\n",
      "Iter #396960:  Learning rate = 0.004424:   Batch Loss = 0.940301, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0619866847991943, Accuracy = 0.6663366556167603\n",
      "Iter #397440:  Learning rate = 0.004424:   Batch Loss = 0.732939, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0612833499908447, Accuracy = 0.6930692791938782\n",
      "Iter #397920:  Learning rate = 0.004424:   Batch Loss = 0.807118, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0906749963760376, Accuracy = 0.6811881065368652\n",
      "Iter #398400:  Learning rate = 0.004424:   Batch Loss = 0.970141, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0874780416488647, Accuracy = 0.6891089081764221\n",
      "Iter #398880:  Learning rate = 0.004424:   Batch Loss = 0.907559, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.078319787979126, Accuracy = 0.6861386299133301\n",
      "Iter #399360:  Learning rate = 0.004424:   Batch Loss = 0.874598, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0409728288650513, Accuracy = 0.7089108824729919\n",
      "Iter #399840:  Learning rate = 0.004424:   Batch Loss = 0.961843, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0368212461471558, Accuracy = 0.7257425785064697\n",
      "Iter #400320:  Learning rate = 0.004247:   Batch Loss = 0.944563, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0245161056518555, Accuracy = 0.7148514986038208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #400800:  Learning rate = 0.004247:   Batch Loss = 0.976573, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0234227180480957, Accuracy = 0.6792079210281372\n",
      "Iter #401280:  Learning rate = 0.004247:   Batch Loss = 1.043443, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0208635330200195, Accuracy = 0.7247524857521057\n",
      "Iter #401760:  Learning rate = 0.004247:   Batch Loss = 0.984056, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0338919162750244, Accuracy = 0.7138614058494568\n",
      "Iter #402240:  Learning rate = 0.004247:   Batch Loss = 0.859534, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.031101942062378, Accuracy = 0.7168316841125488\n",
      "Iter #402720:  Learning rate = 0.004247:   Batch Loss = 0.936788, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.035939335823059, Accuracy = 0.709900975227356\n",
      "Iter #403200:  Learning rate = 0.004247:   Batch Loss = 0.948165, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0499924421310425, Accuracy = 0.7039604187011719\n",
      "Iter #403680:  Learning rate = 0.004247:   Batch Loss = 0.961939, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0381386280059814, Accuracy = 0.7069306969642639\n",
      "Iter #404160:  Learning rate = 0.004247:   Batch Loss = 1.108315, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0520645380020142, Accuracy = 0.7079207897186279\n",
      "Iter #404640:  Learning rate = 0.004247:   Batch Loss = 0.987182, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0537207126617432, Accuracy = 0.7059406042098999\n",
      "Iter #405120:  Learning rate = 0.004247:   Batch Loss = 1.273006, Accuracy = 0.5666666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0341606140136719, Accuracy = 0.709900975227356\n",
      "Iter #405600:  Learning rate = 0.004247:   Batch Loss = 0.964282, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0235096216201782, Accuracy = 0.7188118696212769\n",
      "Iter #406080:  Learning rate = 0.004247:   Batch Loss = 0.995152, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0153695344924927, Accuracy = 0.71089106798172\n",
      "Iter #406560:  Learning rate = 0.004247:   Batch Loss = 1.151167, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.016833782196045, Accuracy = 0.7207920551300049\n",
      "Iter #407040:  Learning rate = 0.004247:   Batch Loss = 0.969283, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.010855793952942, Accuracy = 0.7198019623756409\n",
      "Iter #407520:  Learning rate = 0.004247:   Batch Loss = 1.079032, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0240867137908936, Accuracy = 0.7059406042098999\n",
      "Iter #408000:  Learning rate = 0.004247:   Batch Loss = 1.011852, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.037644386291504, Accuracy = 0.7089108824729919\n",
      "Iter #408480:  Learning rate = 0.004247:   Batch Loss = 0.950386, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0095746517181396, Accuracy = 0.7257425785064697\n",
      "Iter #408960:  Learning rate = 0.004247:   Batch Loss = 0.961739, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.015038013458252, Accuracy = 0.7336633801460266\n",
      "Iter #409440:  Learning rate = 0.004247:   Batch Loss = 1.061228, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1438660621643066, Accuracy = 0.6683168411254883\n",
      "Iter #409920:  Learning rate = 0.004247:   Batch Loss = 1.115109, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0826200246810913, Accuracy = 0.684158444404602\n",
      "Iter #410400:  Learning rate = 0.004247:   Batch Loss = 0.968788, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0921293497085571, Accuracy = 0.6782178282737732\n",
      "Iter #410880:  Learning rate = 0.004247:   Batch Loss = 0.899984, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0607712268829346, Accuracy = 0.694059431552887\n",
      "Iter #411360:  Learning rate = 0.004247:   Batch Loss = 1.002185, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.038707971572876, Accuracy = 0.697029709815979\n",
      "Iter #411840:  Learning rate = 0.004247:   Batch Loss = 1.061532, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.025825023651123, Accuracy = 0.7178217768669128\n",
      "Iter #412320:  Learning rate = 0.004247:   Batch Loss = 1.057435, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0115878582000732, Accuracy = 0.71089106798172\n",
      "Iter #412800:  Learning rate = 0.004247:   Batch Loss = 0.878092, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.037259817123413, Accuracy = 0.709900975227356\n",
      "Iter #413280:  Learning rate = 0.004247:   Batch Loss = 0.981178, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0568945407867432, Accuracy = 0.6900990009307861\n",
      "Iter #413760:  Learning rate = 0.004247:   Batch Loss = 1.015941, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0154695510864258, Accuracy = 0.7306930422782898\n",
      "Iter #414240:  Learning rate = 0.004247:   Batch Loss = 0.997744, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0097299814224243, Accuracy = 0.7277227640151978\n",
      "Iter #414720:  Learning rate = 0.004247:   Batch Loss = 0.901901, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0520902872085571, Accuracy = 0.699999988079071\n",
      "Iter #415200:  Learning rate = 0.004247:   Batch Loss = 1.162974, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0384362936019897, Accuracy = 0.7207920551300049\n",
      "Iter #415680:  Learning rate = 0.004247:   Batch Loss = 0.846558, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0098730325698853, Accuracy = 0.7356435656547546\n",
      "Iter #416160:  Learning rate = 0.004247:   Batch Loss = 1.010122, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0230480432510376, Accuracy = 0.7089108824729919\n",
      "Iter #416640:  Learning rate = 0.004247:   Batch Loss = 1.007838, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9780625104904175, Accuracy = 0.7386138439178467\n",
      "Iter #417120:  Learning rate = 0.004247:   Batch Loss = 0.852874, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9943175315856934, Accuracy = 0.7306930422782898\n",
      "Iter #417600:  Learning rate = 0.004247:   Batch Loss = 1.085021, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0209122896194458, Accuracy = 0.711881160736084\n",
      "Iter #418080:  Learning rate = 0.004247:   Batch Loss = 0.873415, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0061296224594116, Accuracy = 0.7435643672943115\n",
      "Iter #418560:  Learning rate = 0.004247:   Batch Loss = 1.006903, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9923452138900757, Accuracy = 0.7405940890312195\n",
      "Iter #419040:  Learning rate = 0.004247:   Batch Loss = 1.059341, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0047223567962646, Accuracy = 0.7326732873916626\n",
      "Iter #419520:  Learning rate = 0.004247:   Batch Loss = 0.890105, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0023192167282104, Accuracy = 0.7475247383117676\n",
      "Iter #420000:  Learning rate = 0.004247:   Batch Loss = 0.900308, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0022737979888916, Accuracy = 0.7316831946372986\n",
      "Iter #420480:  Learning rate = 0.004247:   Batch Loss = 0.971273, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.038356065750122, Accuracy = 0.7188118696212769\n",
      "Iter #420960:  Learning rate = 0.004247:   Batch Loss = 1.033691, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0327918529510498, Accuracy = 0.7148514986038208\n",
      "Iter #421440:  Learning rate = 0.004247:   Batch Loss = 1.119929, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.022210717201233, Accuracy = 0.7148514986038208\n",
      "Iter #421920:  Learning rate = 0.004247:   Batch Loss = 1.075538, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9928207397460938, Accuracy = 0.7386138439178467\n",
      "Iter #422400:  Learning rate = 0.004247:   Batch Loss = 0.959987, Accuracy = 0.7833333611488342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0055245161056519, Accuracy = 0.7217822074890137\n",
      "Iter #422880:  Learning rate = 0.004247:   Batch Loss = 0.934859, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.00429105758667, Accuracy = 0.7178217768669128\n",
      "Iter #423360:  Learning rate = 0.004247:   Batch Loss = 0.960238, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0207622051239014, Accuracy = 0.7029703259468079\n",
      "Iter #423840:  Learning rate = 0.004247:   Batch Loss = 0.888412, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9954763650894165, Accuracy = 0.7287128567695618\n",
      "Iter #424320:  Learning rate = 0.004247:   Batch Loss = 0.894214, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0207853317260742, Accuracy = 0.7059406042098999\n",
      "Iter #424800:  Learning rate = 0.004247:   Batch Loss = 1.012631, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.020536184310913, Accuracy = 0.7019801735877991\n",
      "Iter #425280:  Learning rate = 0.004247:   Batch Loss = 0.898397, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9862614870071411, Accuracy = 0.7257425785064697\n",
      "Iter #425760:  Learning rate = 0.004247:   Batch Loss = 0.813009, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0062851905822754, Accuracy = 0.7148514986038208\n",
      "Iter #426240:  Learning rate = 0.004247:   Batch Loss = 0.959554, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.970400333404541, Accuracy = 0.7287128567695618\n",
      "Iter #426720:  Learning rate = 0.004247:   Batch Loss = 0.838910, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9692328572273254, Accuracy = 0.7435643672943115\n",
      "Iter #427200:  Learning rate = 0.004247:   Batch Loss = 1.084988, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9657670259475708, Accuracy = 0.7405940890312195\n",
      "Iter #427680:  Learning rate = 0.004247:   Batch Loss = 0.829987, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9502925872802734, Accuracy = 0.7524752616882324\n",
      "Iter #428160:  Learning rate = 0.004247:   Batch Loss = 0.952965, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.965315043926239, Accuracy = 0.7346534729003906\n",
      "Iter #428640:  Learning rate = 0.004247:   Batch Loss = 0.992891, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9659208655357361, Accuracy = 0.7405940890312195\n",
      "Iter #429120:  Learning rate = 0.004247:   Batch Loss = 0.913531, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9537979960441589, Accuracy = 0.7584158182144165\n",
      "Iter #429600:  Learning rate = 0.004247:   Batch Loss = 0.818654, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9653842449188232, Accuracy = 0.7316831946372986\n",
      "Iter #430080:  Learning rate = 0.004247:   Batch Loss = 1.045157, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9981027245521545, Accuracy = 0.7287128567695618\n",
      "Iter #430560:  Learning rate = 0.004247:   Batch Loss = 1.129177, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0084311962127686, Accuracy = 0.7306930422782898\n",
      "Iter #431040:  Learning rate = 0.004247:   Batch Loss = 1.212341, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0521913766860962, Accuracy = 0.71089106798172\n",
      "Iter #431520:  Learning rate = 0.004247:   Batch Loss = 0.910495, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9989945888519287, Accuracy = 0.7445544600486755\n",
      "Iter #432000:  Learning rate = 0.004247:   Batch Loss = 0.959230, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9693906307220459, Accuracy = 0.7544554471969604\n",
      "Iter #432480:  Learning rate = 0.004247:   Batch Loss = 0.810673, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9697397351264954, Accuracy = 0.7445544600486755\n",
      "Iter #432960:  Learning rate = 0.004247:   Batch Loss = 0.902470, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9729246497154236, Accuracy = 0.7495049238204956\n",
      "Iter #433440:  Learning rate = 0.004247:   Batch Loss = 0.854033, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9593328833580017, Accuracy = 0.7534653544425964\n",
      "Iter #433920:  Learning rate = 0.004247:   Batch Loss = 0.741224, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9652794003486633, Accuracy = 0.7465346455574036\n",
      "Iter #434400:  Learning rate = 0.004247:   Batch Loss = 0.951855, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9581753611564636, Accuracy = 0.7584158182144165\n",
      "Iter #434880:  Learning rate = 0.004247:   Batch Loss = 1.108839, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9555376768112183, Accuracy = 0.7564356327056885\n",
      "Iter #435360:  Learning rate = 0.004247:   Batch Loss = 1.046424, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9623467922210693, Accuracy = 0.7504950761795044\n",
      "Iter #435840:  Learning rate = 0.004247:   Batch Loss = 0.766600, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9363349080085754, Accuracy = 0.7594059109687805\n",
      "Iter #436320:  Learning rate = 0.004247:   Batch Loss = 0.895687, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.947930097579956, Accuracy = 0.7594059109687805\n",
      "Iter #436800:  Learning rate = 0.004247:   Batch Loss = 0.972381, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9342423677444458, Accuracy = 0.7673267126083374\n",
      "Iter #437280:  Learning rate = 0.004247:   Batch Loss = 0.918565, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9527106285095215, Accuracy = 0.7504950761795044\n",
      "Iter #437760:  Learning rate = 0.004247:   Batch Loss = 0.702568, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9412165880203247, Accuracy = 0.7594059109687805\n",
      "Iter #438240:  Learning rate = 0.004247:   Batch Loss = 0.781874, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.956785261631012, Accuracy = 0.7475247383117676\n",
      "Iter #438720:  Learning rate = 0.004247:   Batch Loss = 1.014353, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9498672485351562, Accuracy = 0.7524752616882324\n",
      "Iter #439200:  Learning rate = 0.004247:   Batch Loss = 0.775853, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9335763454437256, Accuracy = 0.7653465270996094\n",
      "Iter #439680:  Learning rate = 0.004247:   Batch Loss = 1.014951, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9414340257644653, Accuracy = 0.7643564343452454\n",
      "Iter #440160:  Learning rate = 0.004247:   Batch Loss = 0.815573, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9607416391372681, Accuracy = 0.7504950761795044\n",
      "Iter #440640:  Learning rate = 0.004247:   Batch Loss = 1.132562, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0623658895492554, Accuracy = 0.6792079210281372\n",
      "Iter #441120:  Learning rate = 0.004247:   Batch Loss = 0.962388, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0062085390090942, Accuracy = 0.7287128567695618\n",
      "Iter #441600:  Learning rate = 0.004247:   Batch Loss = 0.907158, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.031538963317871, Accuracy = 0.7069306969642639\n",
      "Iter #442080:  Learning rate = 0.004247:   Batch Loss = 0.895848, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9903767108917236, Accuracy = 0.7287128567695618\n",
      "Iter #442560:  Learning rate = 0.004247:   Batch Loss = 0.621671, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9782003164291382, Accuracy = 0.7356435656547546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #443040:  Learning rate = 0.004247:   Batch Loss = 1.176079, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0099735260009766, Accuracy = 0.7336633801460266\n",
      "Iter #443520:  Learning rate = 0.004247:   Batch Loss = 0.863826, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9616316556930542, Accuracy = 0.7554455399513245\n",
      "Iter #444000:  Learning rate = 0.004247:   Batch Loss = 0.830376, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0062222480773926, Accuracy = 0.7326732873916626\n",
      "Iter #444480:  Learning rate = 0.004247:   Batch Loss = 1.161054, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0588083267211914, Accuracy = 0.6891089081764221\n",
      "Iter #444960:  Learning rate = 0.004247:   Batch Loss = 0.805910, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.078274130821228, Accuracy = 0.6801980137825012\n",
      "Iter #445440:  Learning rate = 0.004247:   Batch Loss = 0.904537, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0552594661712646, Accuracy = 0.7009900808334351\n",
      "Iter #445920:  Learning rate = 0.004247:   Batch Loss = 0.865962, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.081437349319458, Accuracy = 0.6871287226676941\n",
      "Iter #446400:  Learning rate = 0.004247:   Batch Loss = 1.110044, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0780328512191772, Accuracy = 0.6861386299133301\n",
      "Iter #446880:  Learning rate = 0.004247:   Batch Loss = 0.952259, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0411689281463623, Accuracy = 0.7257425785064697\n",
      "Iter #447360:  Learning rate = 0.004247:   Batch Loss = 0.980346, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0274078845977783, Accuracy = 0.7277227640151978\n",
      "Iter #447840:  Learning rate = 0.004247:   Batch Loss = 0.906543, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0198689699172974, Accuracy = 0.7306930422782898\n",
      "Iter #448320:  Learning rate = 0.004247:   Batch Loss = 0.944866, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0003066062927246, Accuracy = 0.7376237511634827\n",
      "Iter #448800:  Learning rate = 0.004247:   Batch Loss = 1.012683, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0178476572036743, Accuracy = 0.7158415913581848\n",
      "Iter #449280:  Learning rate = 0.004247:   Batch Loss = 0.865128, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0311095714569092, Accuracy = 0.7257425785064697\n",
      "Iter #449760:  Learning rate = 0.004247:   Batch Loss = 1.136525, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0070013999938965, Accuracy = 0.7287128567695618\n",
      "Iter #450240:  Learning rate = 0.004247:   Batch Loss = 0.918196, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9920607209205627, Accuracy = 0.7366336584091187\n",
      "Iter #450720:  Learning rate = 0.004247:   Batch Loss = 0.879307, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0026286840438843, Accuracy = 0.7287128567695618\n",
      "Iter #451200:  Learning rate = 0.004247:   Batch Loss = 0.967531, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0013277530670166, Accuracy = 0.7207920551300049\n",
      "Iter #451680:  Learning rate = 0.004247:   Batch Loss = 0.900023, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0152078866958618, Accuracy = 0.7346534729003906\n",
      "Iter #452160:  Learning rate = 0.004247:   Batch Loss = 0.968309, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0313756465911865, Accuracy = 0.7059406042098999\n",
      "Iter #452640:  Learning rate = 0.004247:   Batch Loss = 1.011642, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0070048570632935, Accuracy = 0.7188118696212769\n",
      "Iter #453120:  Learning rate = 0.004247:   Batch Loss = 0.876475, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0254820585250854, Accuracy = 0.7089108824729919\n",
      "Iter #453600:  Learning rate = 0.004247:   Batch Loss = 0.878843, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.008662223815918, Accuracy = 0.7376237511634827\n",
      "Iter #454080:  Learning rate = 0.004247:   Batch Loss = 1.121240, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0091770887374878, Accuracy = 0.7207920551300049\n",
      "Iter #454560:  Learning rate = 0.004247:   Batch Loss = 0.835867, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0167070627212524, Accuracy = 0.7297029495239258\n",
      "Iter #455040:  Learning rate = 0.004247:   Batch Loss = 0.899024, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0161164999008179, Accuracy = 0.7217822074890137\n",
      "Iter #455520:  Learning rate = 0.004247:   Batch Loss = 0.940034, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0097910165786743, Accuracy = 0.7247524857521057\n",
      "Iter #456000:  Learning rate = 0.004247:   Batch Loss = 0.766550, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.005202054977417, Accuracy = 0.7287128567695618\n",
      "Iter #456480:  Learning rate = 0.004247:   Batch Loss = 0.840772, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0132747888565063, Accuracy = 0.7326732873916626\n",
      "Iter #456960:  Learning rate = 0.004247:   Batch Loss = 0.994729, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0158445835113525, Accuracy = 0.7237623929977417\n",
      "Iter #457440:  Learning rate = 0.004247:   Batch Loss = 0.995569, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0266437530517578, Accuracy = 0.7138614058494568\n",
      "Iter #457920:  Learning rate = 0.004247:   Batch Loss = 0.914515, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0198585987091064, Accuracy = 0.7059406042098999\n",
      "Iter #458400:  Learning rate = 0.004247:   Batch Loss = 0.894416, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.007936716079712, Accuracy = 0.7178217768669128\n",
      "Iter #458880:  Learning rate = 0.004247:   Batch Loss = 0.997788, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0225377082824707, Accuracy = 0.7227723002433777\n",
      "Iter #459360:  Learning rate = 0.004247:   Batch Loss = 1.086713, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0058971643447876, Accuracy = 0.7178217768669128\n",
      "Iter #459840:  Learning rate = 0.004247:   Batch Loss = 1.043493, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.996558666229248, Accuracy = 0.7267326712608337\n",
      "Iter #460320:  Learning rate = 0.004247:   Batch Loss = 1.068867, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9886353015899658, Accuracy = 0.7386138439178467\n",
      "Iter #460800:  Learning rate = 0.004247:   Batch Loss = 0.829847, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9923001527786255, Accuracy = 0.7227723002433777\n",
      "Iter #461280:  Learning rate = 0.004247:   Batch Loss = 1.059806, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0010225772857666, Accuracy = 0.7217822074890137\n",
      "Iter #461760:  Learning rate = 0.004247:   Batch Loss = 0.857013, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9949241876602173, Accuracy = 0.7306930422782898\n",
      "Iter #462240:  Learning rate = 0.004247:   Batch Loss = 1.000283, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.995559573173523, Accuracy = 0.7178217768669128\n",
      "Iter #462720:  Learning rate = 0.004247:   Batch Loss = 1.010606, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.996800422668457, Accuracy = 0.7277227640151978\n",
      "Iter #463200:  Learning rate = 0.004247:   Batch Loss = 0.827621, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9906607270240784, Accuracy = 0.7336633801460266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #463680:  Learning rate = 0.004247:   Batch Loss = 1.079061, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9908202290534973, Accuracy = 0.7336633801460266\n",
      "Iter #464160:  Learning rate = 0.004247:   Batch Loss = 0.845516, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9801931381225586, Accuracy = 0.7366336584091187\n",
      "Iter #464640:  Learning rate = 0.004247:   Batch Loss = 0.843353, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0122642517089844, Accuracy = 0.7267326712608337\n",
      "Iter #465120:  Learning rate = 0.004247:   Batch Loss = 0.927554, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9825475811958313, Accuracy = 0.7297029495239258\n",
      "Iter #465600:  Learning rate = 0.004247:   Batch Loss = 1.126423, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9775579571723938, Accuracy = 0.7475247383117676\n",
      "Iter #466080:  Learning rate = 0.004247:   Batch Loss = 0.881783, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9851255416870117, Accuracy = 0.7326732873916626\n",
      "Iter #466560:  Learning rate = 0.004247:   Batch Loss = 0.955829, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9828085899353027, Accuracy = 0.7396039366722107\n",
      "Iter #467040:  Learning rate = 0.004247:   Batch Loss = 0.866782, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9479135274887085, Accuracy = 0.7534653544425964\n",
      "Iter #467520:  Learning rate = 0.004247:   Batch Loss = 0.924880, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9607928991317749, Accuracy = 0.7435643672943115\n",
      "Iter #468000:  Learning rate = 0.004247:   Batch Loss = 1.261916, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9513207077980042, Accuracy = 0.7574257254600525\n",
      "Iter #468480:  Learning rate = 0.004247:   Batch Loss = 0.923795, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9997239112854004, Accuracy = 0.7306930422782898\n",
      "Iter #468960:  Learning rate = 0.004247:   Batch Loss = 0.910667, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9921965003013611, Accuracy = 0.7386138439178467\n",
      "Iter #469440:  Learning rate = 0.004247:   Batch Loss = 0.849260, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9900557398796082, Accuracy = 0.7207920551300049\n",
      "Iter #469920:  Learning rate = 0.004247:   Batch Loss = 1.085623, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9829367399215698, Accuracy = 0.7435643672943115\n",
      "Iter #470400:  Learning rate = 0.004247:   Batch Loss = 0.908203, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9398607015609741, Accuracy = 0.7584158182144165\n",
      "Iter #470880:  Learning rate = 0.004247:   Batch Loss = 0.908845, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9626139402389526, Accuracy = 0.7554455399513245\n",
      "Iter #471360:  Learning rate = 0.004247:   Batch Loss = 1.073086, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0523041486740112, Accuracy = 0.6910890936851501\n",
      "Iter #471840:  Learning rate = 0.004247:   Batch Loss = 0.941654, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9975281953811646, Accuracy = 0.7415841817855835\n",
      "Iter #472320:  Learning rate = 0.004247:   Batch Loss = 1.023792, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9958899617195129, Accuracy = 0.7267326712608337\n",
      "Iter #472800:  Learning rate = 0.004247:   Batch Loss = 0.964574, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9630815982818604, Accuracy = 0.7425742745399475\n",
      "Iter #473280:  Learning rate = 0.004247:   Batch Loss = 0.834342, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9711464643478394, Accuracy = 0.7405940890312195\n",
      "Iter #473760:  Learning rate = 0.004247:   Batch Loss = 0.687768, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.974616289138794, Accuracy = 0.7435643672943115\n",
      "Iter #474240:  Learning rate = 0.004247:   Batch Loss = 0.868012, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.971359372138977, Accuracy = 0.7405940890312195\n",
      "Iter #474720:  Learning rate = 0.004247:   Batch Loss = 1.035053, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9588298797607422, Accuracy = 0.7524752616882324\n",
      "Iter #475200:  Learning rate = 0.004247:   Batch Loss = 0.924181, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.971680223941803, Accuracy = 0.7425742745399475\n",
      "Iter #475680:  Learning rate = 0.004247:   Batch Loss = 0.936355, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9913452863693237, Accuracy = 0.7257425785064697\n",
      "Iter #476160:  Learning rate = 0.004247:   Batch Loss = 0.845747, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.004570484161377, Accuracy = 0.7475247383117676\n",
      "Iter #476640:  Learning rate = 0.004247:   Batch Loss = 0.936514, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.965222954750061, Accuracy = 0.7544554471969604\n",
      "Iter #477120:  Learning rate = 0.004247:   Batch Loss = 0.745693, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9607425332069397, Accuracy = 0.7495049238204956\n",
      "Iter #477600:  Learning rate = 0.004247:   Batch Loss = 0.768016, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9574816823005676, Accuracy = 0.7485148310661316\n",
      "Iter #478080:  Learning rate = 0.004247:   Batch Loss = 0.987288, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9584022164344788, Accuracy = 0.7643564343452454\n",
      "Iter #478560:  Learning rate = 0.004247:   Batch Loss = 0.983128, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9748716354370117, Accuracy = 0.7366336584091187\n",
      "Iter #479040:  Learning rate = 0.004247:   Batch Loss = 0.814519, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9582302570343018, Accuracy = 0.7544554471969604\n",
      "Iter #479520:  Learning rate = 0.004247:   Batch Loss = 1.036132, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9598166942596436, Accuracy = 0.7603960633277893\n",
      "Iter #480000:  Learning rate = 0.004247:   Batch Loss = 0.767304, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9662075638771057, Accuracy = 0.7326732873916626\n",
      "Iter #480480:  Learning rate = 0.004247:   Batch Loss = 0.858790, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9458142518997192, Accuracy = 0.7435643672943115\n",
      "Iter #480960:  Learning rate = 0.004247:   Batch Loss = 0.861918, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9570252895355225, Accuracy = 0.7504950761795044\n",
      "Iter #481440:  Learning rate = 0.004247:   Batch Loss = 0.834813, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9353044033050537, Accuracy = 0.7485148310661316\n",
      "Iter #481920:  Learning rate = 0.004247:   Batch Loss = 0.968267, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.94407057762146, Accuracy = 0.7455445528030396\n",
      "Iter #482400:  Learning rate = 0.004247:   Batch Loss = 0.871519, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9662750363349915, Accuracy = 0.7475247383117676\n",
      "Iter #482880:  Learning rate = 0.004247:   Batch Loss = 0.959334, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9441947937011719, Accuracy = 0.7475247383117676\n",
      "Iter #483360:  Learning rate = 0.004247:   Batch Loss = 0.956685, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.5237466096878052, Accuracy = 0.5069307088851929\n",
      "Iter #483840:  Learning rate = 0.004247:   Batch Loss = 0.890811, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9500268697738647, Accuracy = 0.7445544600486755\n",
      "Iter #484320:  Learning rate = 0.004247:   Batch Loss = 0.888615, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9230996370315552, Accuracy = 0.7653465270996094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #484800:  Learning rate = 0.004247:   Batch Loss = 0.925748, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9433282613754272, Accuracy = 0.7534653544425964\n",
      "Iter #485280:  Learning rate = 0.004247:   Batch Loss = 1.014620, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9536184668540955, Accuracy = 0.7544554471969604\n",
      "Iter #485760:  Learning rate = 0.004247:   Batch Loss = 0.931119, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9599331617355347, Accuracy = 0.7564356327056885\n",
      "Iter #486240:  Learning rate = 0.004247:   Batch Loss = 0.780972, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9472554922103882, Accuracy = 0.7514851689338684\n",
      "Iter #486720:  Learning rate = 0.004247:   Batch Loss = 0.692340, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9206124544143677, Accuracy = 0.7772276997566223\n",
      "Iter #487200:  Learning rate = 0.004247:   Batch Loss = 0.851673, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9391003847122192, Accuracy = 0.7584158182144165\n",
      "Iter #487680:  Learning rate = 0.004247:   Batch Loss = 0.839438, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9901740550994873, Accuracy = 0.7247524857521057\n",
      "Iter #488160:  Learning rate = 0.004247:   Batch Loss = 0.909444, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9498014450073242, Accuracy = 0.7485148310661316\n",
      "Iter #488640:  Learning rate = 0.004247:   Batch Loss = 0.800260, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.947075605392456, Accuracy = 0.7712871432304382\n",
      "Iter #489120:  Learning rate = 0.004247:   Batch Loss = 1.070339, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9585027098655701, Accuracy = 0.7534653544425964\n",
      "Iter #489600:  Learning rate = 0.004247:   Batch Loss = 0.787766, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9738314151763916, Accuracy = 0.7425742745399475\n",
      "Iter #490080:  Learning rate = 0.004247:   Batch Loss = 0.940568, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.952933669090271, Accuracy = 0.7435643672943115\n",
      "Iter #490560:  Learning rate = 0.004247:   Batch Loss = 0.935160, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9741661548614502, Accuracy = 0.7346534729003906\n",
      "Iter #491040:  Learning rate = 0.004247:   Batch Loss = 0.819263, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9879481792449951, Accuracy = 0.7356435656547546\n",
      "Iter #491520:  Learning rate = 0.004247:   Batch Loss = 1.092519, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0110903978347778, Accuracy = 0.7158415913581848\n",
      "Iter #492000:  Learning rate = 0.004247:   Batch Loss = 0.701747, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9757238030433655, Accuracy = 0.7386138439178467\n",
      "Iter #492480:  Learning rate = 0.004247:   Batch Loss = 0.908297, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9538573622703552, Accuracy = 0.7534653544425964\n",
      "Iter #492960:  Learning rate = 0.004247:   Batch Loss = 1.121245, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9603665471076965, Accuracy = 0.7455445528030396\n",
      "Iter #493440:  Learning rate = 0.004247:   Batch Loss = 0.878800, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9606837630271912, Accuracy = 0.7554455399513245\n",
      "Iter #493920:  Learning rate = 0.004247:   Batch Loss = 0.932930, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9574885964393616, Accuracy = 0.7376237511634827\n",
      "Iter #494400:  Learning rate = 0.004247:   Batch Loss = 0.920199, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9685144424438477, Accuracy = 0.7534653544425964\n",
      "Iter #494880:  Learning rate = 0.004247:   Batch Loss = 0.936095, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9330384731292725, Accuracy = 0.7712871432304382\n",
      "Iter #495360:  Learning rate = 0.004247:   Batch Loss = 0.816618, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9289053678512573, Accuracy = 0.7643564343452454\n",
      "Iter #495840:  Learning rate = 0.004247:   Batch Loss = 1.028706, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9776310920715332, Accuracy = 0.7415841817855835\n",
      "Iter #496320:  Learning rate = 0.004247:   Batch Loss = 0.865746, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.921959400177002, Accuracy = 0.7663366198539734\n",
      "Iter #496800:  Learning rate = 0.004247:   Batch Loss = 1.053325, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9916173815727234, Accuracy = 0.7514851689338684\n",
      "Iter #497280:  Learning rate = 0.004247:   Batch Loss = 0.848044, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9525346755981445, Accuracy = 0.7455445528030396\n",
      "Iter #497760:  Learning rate = 0.004247:   Batch Loss = 1.083672, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9598948955535889, Accuracy = 0.7574257254600525\n",
      "Iter #498240:  Learning rate = 0.004247:   Batch Loss = 0.724328, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9736554026603699, Accuracy = 0.7514851689338684\n",
      "Iter #498720:  Learning rate = 0.004247:   Batch Loss = 0.908623, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9619167447090149, Accuracy = 0.7435643672943115\n",
      "Iter #499200:  Learning rate = 0.004247:   Batch Loss = 1.084665, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9651350378990173, Accuracy = 0.7495049238204956\n",
      "Iter #499680:  Learning rate = 0.004247:   Batch Loss = 0.693500, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9580572247505188, Accuracy = 0.7485148310661316\n",
      "Iter #500160:  Learning rate = 0.004077:   Batch Loss = 0.815380, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9651063680648804, Accuracy = 0.7544554471969604\n",
      "Iter #500640:  Learning rate = 0.004077:   Batch Loss = 1.033262, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9801628589630127, Accuracy = 0.7356435656547546\n",
      "Iter #501120:  Learning rate = 0.004077:   Batch Loss = 1.013774, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0216686725616455, Accuracy = 0.7049505114555359\n",
      "Iter #501600:  Learning rate = 0.004077:   Batch Loss = 1.105128, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0224826335906982, Accuracy = 0.7217822074890137\n",
      "Iter #502080:  Learning rate = 0.004077:   Batch Loss = 0.791209, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.028673529624939, Accuracy = 0.7217822074890137\n",
      "Iter #502560:  Learning rate = 0.004077:   Batch Loss = 0.958742, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0540910959243774, Accuracy = 0.7089108824729919\n",
      "Iter #503040:  Learning rate = 0.004077:   Batch Loss = 1.071918, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0421696901321411, Accuracy = 0.7227723002433777\n",
      "Iter #503520:  Learning rate = 0.004077:   Batch Loss = 1.106920, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1561558246612549, Accuracy = 0.6524752378463745\n",
      "Iter #504000:  Learning rate = 0.004077:   Batch Loss = 1.168259, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1531338691711426, Accuracy = 0.6613861322402954\n",
      "Iter #504480:  Learning rate = 0.004077:   Batch Loss = 1.217381, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1816786527633667, Accuracy = 0.6524752378463745\n",
      "Iter #504960:  Learning rate = 0.004077:   Batch Loss = 1.231254, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0712145566940308, Accuracy = 0.699999988079071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #505440:  Learning rate = 0.004077:   Batch Loss = 1.029943, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0309702157974243, Accuracy = 0.7227723002433777\n",
      "Iter #505920:  Learning rate = 0.004077:   Batch Loss = 0.863120, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0495210886001587, Accuracy = 0.71089106798172\n",
      "Iter #506400:  Learning rate = 0.004077:   Batch Loss = 1.256336, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0483423471450806, Accuracy = 0.7227723002433777\n",
      "Iter #506880:  Learning rate = 0.004077:   Batch Loss = 1.001843, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0160746574401855, Accuracy = 0.7277227640151978\n",
      "Iter #507360:  Learning rate = 0.004077:   Batch Loss = 0.864028, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0101269483566284, Accuracy = 0.7277227640151978\n",
      "Iter #507840:  Learning rate = 0.004077:   Batch Loss = 0.886028, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0109549760818481, Accuracy = 0.7267326712608337\n",
      "Iter #508320:  Learning rate = 0.004077:   Batch Loss = 0.750252, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0474611520767212, Accuracy = 0.7237623929977417\n",
      "Iter #508800:  Learning rate = 0.004077:   Batch Loss = 1.014135, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0244455337524414, Accuracy = 0.7316831946372986\n",
      "Iter #509280:  Learning rate = 0.004077:   Batch Loss = 0.899327, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0780067443847656, Accuracy = 0.7049505114555359\n",
      "Iter #509760:  Learning rate = 0.004077:   Batch Loss = 0.939639, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.002864122390747, Accuracy = 0.7405940890312195\n",
      "Iter #510240:  Learning rate = 0.004077:   Batch Loss = 1.092583, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0143193006515503, Accuracy = 0.7207920551300049\n",
      "Iter #510720:  Learning rate = 0.004077:   Batch Loss = 1.035147, Accuracy = 0.6333333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9967750310897827, Accuracy = 0.7267326712608337\n",
      "Iter #511200:  Learning rate = 0.004077:   Batch Loss = 0.778338, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9602905511856079, Accuracy = 0.7663366198539734\n",
      "Iter #511680:  Learning rate = 0.004077:   Batch Loss = 0.802572, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9900460839271545, Accuracy = 0.7475247383117676\n",
      "Iter #512160:  Learning rate = 0.004077:   Batch Loss = 0.911413, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9804239273071289, Accuracy = 0.7405940890312195\n",
      "Iter #512640:  Learning rate = 0.004077:   Batch Loss = 0.777679, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9858208894729614, Accuracy = 0.7504950761795044\n",
      "Iter #513120:  Learning rate = 0.004077:   Batch Loss = 0.866700, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.976952075958252, Accuracy = 0.7465346455574036\n",
      "Iter #513600:  Learning rate = 0.004077:   Batch Loss = 0.957611, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9711454510688782, Accuracy = 0.7495049238204956\n",
      "Iter #514080:  Learning rate = 0.004077:   Batch Loss = 1.169762, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9894546866416931, Accuracy = 0.7425742745399475\n",
      "Iter #514560:  Learning rate = 0.004077:   Batch Loss = 1.059363, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9945489168167114, Accuracy = 0.7366336584091187\n",
      "Iter #515040:  Learning rate = 0.004077:   Batch Loss = 0.950086, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9840712547302246, Accuracy = 0.7465346455574036\n",
      "Iter #515520:  Learning rate = 0.004077:   Batch Loss = 0.804242, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9751394987106323, Accuracy = 0.7584158182144165\n",
      "Iter #516000:  Learning rate = 0.004077:   Batch Loss = 0.795313, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9673676490783691, Accuracy = 0.7574257254600525\n",
      "Iter #516480:  Learning rate = 0.004077:   Batch Loss = 0.830337, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9471513032913208, Accuracy = 0.7693069577217102\n",
      "Iter #516960:  Learning rate = 0.004077:   Batch Loss = 0.992509, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9583110809326172, Accuracy = 0.7386138439178467\n",
      "Iter #517440:  Learning rate = 0.004077:   Batch Loss = 0.807163, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9661029577255249, Accuracy = 0.7504950761795044\n",
      "Iter #517920:  Learning rate = 0.004077:   Batch Loss = 0.940751, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0179988145828247, Accuracy = 0.7207920551300049\n",
      "Iter #518400:  Learning rate = 0.004077:   Batch Loss = 0.883296, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9858093857765198, Accuracy = 0.7287128567695618\n",
      "Iter #518880:  Learning rate = 0.004077:   Batch Loss = 0.877886, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9791475534439087, Accuracy = 0.7415841817855835\n",
      "Iter #519360:  Learning rate = 0.004077:   Batch Loss = 1.157091, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0066137313842773, Accuracy = 0.7089108824729919\n",
      "Iter #519840:  Learning rate = 0.004077:   Batch Loss = 1.218240, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9836668372154236, Accuracy = 0.7207920551300049\n",
      "Iter #520320:  Learning rate = 0.004077:   Batch Loss = 1.036608, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9639713168144226, Accuracy = 0.7495049238204956\n",
      "Iter #520800:  Learning rate = 0.004077:   Batch Loss = 0.869393, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9604178667068481, Accuracy = 0.7534653544425964\n",
      "Iter #521280:  Learning rate = 0.004077:   Batch Loss = 0.979456, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9487329125404358, Accuracy = 0.7504950761795044\n",
      "Iter #521760:  Learning rate = 0.004077:   Batch Loss = 0.888440, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9623788595199585, Accuracy = 0.7514851689338684\n",
      "Iter #522240:  Learning rate = 0.004077:   Batch Loss = 0.876840, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0059738159179688, Accuracy = 0.7158415913581848\n",
      "Iter #522720:  Learning rate = 0.004077:   Batch Loss = 0.918770, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9521697759628296, Accuracy = 0.7396039366722107\n",
      "Iter #523200:  Learning rate = 0.004077:   Batch Loss = 0.887954, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9154291152954102, Accuracy = 0.7693069577217102\n",
      "Iter #523680:  Learning rate = 0.004077:   Batch Loss = 0.881945, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9451600909233093, Accuracy = 0.7594059109687805\n",
      "Iter #524160:  Learning rate = 0.004077:   Batch Loss = 1.090277, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9553775787353516, Accuracy = 0.7475247383117676\n",
      "Iter #524640:  Learning rate = 0.004077:   Batch Loss = 1.117759, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9234866499900818, Accuracy = 0.7633663415908813\n",
      "Iter #525120:  Learning rate = 0.004077:   Batch Loss = 0.642923, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9554942846298218, Accuracy = 0.7455445528030396\n",
      "Iter #525600:  Learning rate = 0.004077:   Batch Loss = 0.926841, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9462981820106506, Accuracy = 0.7514851689338684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #526080:  Learning rate = 0.004077:   Batch Loss = 1.009220, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9362272620201111, Accuracy = 0.7544554471969604\n",
      "Iter #526560:  Learning rate = 0.004077:   Batch Loss = 0.770483, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9288755655288696, Accuracy = 0.7653465270996094\n",
      "Iter #527040:  Learning rate = 0.004077:   Batch Loss = 0.815948, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9136466979980469, Accuracy = 0.7821782231330872\n",
      "Iter #527520:  Learning rate = 0.004077:   Batch Loss = 0.735404, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9267350435256958, Accuracy = 0.7782177925109863\n",
      "Iter #528000:  Learning rate = 0.004077:   Batch Loss = 0.929814, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9155569076538086, Accuracy = 0.7693069577217102\n",
      "Iter #528480:  Learning rate = 0.004077:   Batch Loss = 0.845691, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9091272950172424, Accuracy = 0.7633663415908813\n",
      "Iter #528960:  Learning rate = 0.004077:   Batch Loss = 0.911729, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9439066648483276, Accuracy = 0.7297029495239258\n",
      "Iter #529440:  Learning rate = 0.004077:   Batch Loss = 1.086181, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9857853651046753, Accuracy = 0.7356435656547546\n",
      "Iter #529920:  Learning rate = 0.004077:   Batch Loss = 1.315362, Accuracy = 0.5333333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0372834205627441, Accuracy = 0.7306930422782898\n",
      "Iter #530400:  Learning rate = 0.004077:   Batch Loss = 1.021468, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9852222204208374, Accuracy = 0.7425742745399475\n",
      "Iter #530880:  Learning rate = 0.004077:   Batch Loss = 1.017424, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9874495267868042, Accuracy = 0.7465346455574036\n",
      "Iter #531360:  Learning rate = 0.004077:   Batch Loss = 1.026990, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9662449955940247, Accuracy = 0.7524752616882324\n",
      "Iter #531840:  Learning rate = 0.004077:   Batch Loss = 0.864842, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9444243311882019, Accuracy = 0.7702970504760742\n",
      "Iter #532320:  Learning rate = 0.004077:   Batch Loss = 0.938025, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9676351547241211, Accuracy = 0.7574257254600525\n",
      "Iter #532800:  Learning rate = 0.004077:   Batch Loss = 0.912902, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9599590301513672, Accuracy = 0.7554455399513245\n",
      "Iter #533280:  Learning rate = 0.004077:   Batch Loss = 0.971374, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9395153522491455, Accuracy = 0.7633663415908813\n",
      "Iter #533760:  Learning rate = 0.004077:   Batch Loss = 0.916859, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9762332439422607, Accuracy = 0.7475247383117676\n",
      "Iter #534240:  Learning rate = 0.004077:   Batch Loss = 0.958220, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9789621829986572, Accuracy = 0.7415841817855835\n",
      "Iter #534720:  Learning rate = 0.004077:   Batch Loss = 1.052208, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9660650491714478, Accuracy = 0.7544554471969604\n",
      "Iter #535200:  Learning rate = 0.004077:   Batch Loss = 0.921732, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9500737190246582, Accuracy = 0.7396039366722107\n",
      "Iter #535680:  Learning rate = 0.004077:   Batch Loss = 1.241667, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0547053813934326, Accuracy = 0.7009900808334351\n",
      "Iter #536160:  Learning rate = 0.004077:   Batch Loss = 1.017545, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.048102855682373, Accuracy = 0.7148514986038208\n",
      "Iter #536640:  Learning rate = 0.004077:   Batch Loss = 0.882470, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9644739627838135, Accuracy = 0.7386138439178467\n",
      "Iter #537120:  Learning rate = 0.004077:   Batch Loss = 1.097138, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.961317777633667, Accuracy = 0.7405940890312195\n",
      "Iter #537600:  Learning rate = 0.004077:   Batch Loss = 1.034308, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9439014792442322, Accuracy = 0.7475247383117676\n",
      "Iter #538080:  Learning rate = 0.004077:   Batch Loss = 1.033188, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9461115598678589, Accuracy = 0.7524752616882324\n",
      "Iter #538560:  Learning rate = 0.004077:   Batch Loss = 1.087868, Accuracy = 0.6666666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9302785396575928, Accuracy = 0.7554455399513245\n",
      "Iter #539040:  Learning rate = 0.004077:   Batch Loss = 0.950673, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9081249833106995, Accuracy = 0.7742574214935303\n",
      "Iter #539520:  Learning rate = 0.004077:   Batch Loss = 0.868463, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9215916395187378, Accuracy = 0.7663366198539734\n",
      "Iter #540000:  Learning rate = 0.004077:   Batch Loss = 0.824414, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9230215549468994, Accuracy = 0.7574257254600525\n",
      "Iter #540480:  Learning rate = 0.004077:   Batch Loss = 1.025863, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9185184836387634, Accuracy = 0.7524752616882324\n",
      "Iter #540960:  Learning rate = 0.004077:   Batch Loss = 0.915324, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9302456378936768, Accuracy = 0.7495049238204956\n",
      "Iter #541440:  Learning rate = 0.004077:   Batch Loss = 0.917946, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9307816028594971, Accuracy = 0.7524752616882324\n",
      "Iter #541920:  Learning rate = 0.004077:   Batch Loss = 0.646192, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9391103982925415, Accuracy = 0.7544554471969604\n",
      "Iter #542400:  Learning rate = 0.004077:   Batch Loss = 0.720776, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9334909319877625, Accuracy = 0.7643564343452454\n",
      "Iter #542880:  Learning rate = 0.004077:   Batch Loss = 0.831348, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9343696236610413, Accuracy = 0.7574257254600525\n",
      "Iter #543360:  Learning rate = 0.004077:   Batch Loss = 1.034639, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9326590895652771, Accuracy = 0.7673267126083374\n",
      "Iter #543840:  Learning rate = 0.004077:   Batch Loss = 1.074800, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.925460696220398, Accuracy = 0.7722772359848022\n",
      "Iter #544320:  Learning rate = 0.004077:   Batch Loss = 0.696701, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9310872554779053, Accuracy = 0.7514851689338684\n",
      "Iter #544800:  Learning rate = 0.004077:   Batch Loss = 0.932867, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9276511669158936, Accuracy = 0.7712871432304382\n",
      "Iter #545280:  Learning rate = 0.004077:   Batch Loss = 0.783498, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9156055450439453, Accuracy = 0.7712871432304382\n",
      "Iter #545760:  Learning rate = 0.004077:   Batch Loss = 0.792669, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9480884075164795, Accuracy = 0.7435643672943115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #546240:  Learning rate = 0.004077:   Batch Loss = 0.803557, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9298794865608215, Accuracy = 0.7643564343452454\n",
      "Iter #546720:  Learning rate = 0.004077:   Batch Loss = 0.783444, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9296668767929077, Accuracy = 0.7485148310661316\n",
      "Iter #547200:  Learning rate = 0.004077:   Batch Loss = 0.916820, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9634020328521729, Accuracy = 0.7495049238204956\n",
      "Iter #547680:  Learning rate = 0.004077:   Batch Loss = 0.930829, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.949805498123169, Accuracy = 0.7554455399513245\n",
      "Iter #548160:  Learning rate = 0.004077:   Batch Loss = 0.989639, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9470725655555725, Accuracy = 0.7495049238204956\n",
      "Iter #548640:  Learning rate = 0.004077:   Batch Loss = 0.811805, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.939048707485199, Accuracy = 0.7613861560821533\n",
      "Iter #549120:  Learning rate = 0.004077:   Batch Loss = 0.824413, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9196785092353821, Accuracy = 0.7673267126083374\n",
      "Iter #549600:  Learning rate = 0.004077:   Batch Loss = 0.737376, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8897323608398438, Accuracy = 0.7841584086418152\n",
      "Iter #550080:  Learning rate = 0.004077:   Batch Loss = 0.935706, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9085684418678284, Accuracy = 0.7693069577217102\n",
      "Iter #550560:  Learning rate = 0.004077:   Batch Loss = 0.745149, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9083226919174194, Accuracy = 0.7811881303787231\n",
      "Iter #551040:  Learning rate = 0.004077:   Batch Loss = 0.883808, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9825310111045837, Accuracy = 0.7198019623756409\n",
      "Iter #551520:  Learning rate = 0.004077:   Batch Loss = 1.049656, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9576293230056763, Accuracy = 0.7386138439178467\n",
      "Iter #552000:  Learning rate = 0.004077:   Batch Loss = 0.698231, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9663470983505249, Accuracy = 0.7475247383117676\n",
      "Iter #552480:  Learning rate = 0.004077:   Batch Loss = 0.753236, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9539395570755005, Accuracy = 0.7524752616882324\n",
      "Iter #552960:  Learning rate = 0.004077:   Batch Loss = 0.731723, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9576207399368286, Accuracy = 0.7534653544425964\n",
      "Iter #553440:  Learning rate = 0.004077:   Batch Loss = 0.838062, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9058762192726135, Accuracy = 0.7851485013961792\n",
      "Iter #553920:  Learning rate = 0.004077:   Batch Loss = 0.912556, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8943196535110474, Accuracy = 0.7772276997566223\n",
      "Iter #554400:  Learning rate = 0.004077:   Batch Loss = 0.706731, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8878101110458374, Accuracy = 0.7841584086418152\n",
      "Iter #554880:  Learning rate = 0.004077:   Batch Loss = 0.938109, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8984489440917969, Accuracy = 0.7851485013961792\n",
      "Iter #555360:  Learning rate = 0.004077:   Batch Loss = 0.636170, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.894247829914093, Accuracy = 0.7861385941505432\n",
      "Iter #555840:  Learning rate = 0.004077:   Batch Loss = 0.852472, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9176041483879089, Accuracy = 0.7792079448699951\n",
      "Iter #556320:  Learning rate = 0.004077:   Batch Loss = 0.677300, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.911490797996521, Accuracy = 0.7752475142478943\n",
      "Iter #556800:  Learning rate = 0.004077:   Batch Loss = 0.899768, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9129015803337097, Accuracy = 0.7653465270996094\n",
      "Iter #557280:  Learning rate = 0.004077:   Batch Loss = 0.785736, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8720963597297668, Accuracy = 0.788118839263916\n",
      "Iter #557760:  Learning rate = 0.004077:   Batch Loss = 0.907328, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8844469785690308, Accuracy = 0.7940593957901001\n",
      "Iter #558240:  Learning rate = 0.004077:   Batch Loss = 0.798853, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.901024341583252, Accuracy = 0.7811881303787231\n",
      "Iter #558720:  Learning rate = 0.004077:   Batch Loss = 0.662769, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8989678025245667, Accuracy = 0.788118839263916\n",
      "Iter #559200:  Learning rate = 0.004077:   Batch Loss = 0.887788, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9078482985496521, Accuracy = 0.7851485013961792\n",
      "Iter #559680:  Learning rate = 0.004077:   Batch Loss = 0.715426, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.904336154460907, Accuracy = 0.7762376070022583\n",
      "Iter #560160:  Learning rate = 0.004077:   Batch Loss = 0.825839, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8981091380119324, Accuracy = 0.7821782231330872\n",
      "Iter #560640:  Learning rate = 0.004077:   Batch Loss = 0.769686, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8813905715942383, Accuracy = 0.7910891175270081\n",
      "Iter #561120:  Learning rate = 0.004077:   Batch Loss = 0.694249, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8878732919692993, Accuracy = 0.7851485013961792\n",
      "Iter #561600:  Learning rate = 0.004077:   Batch Loss = 0.795331, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8962385058403015, Accuracy = 0.7821782231330872\n",
      "Iter #562080:  Learning rate = 0.004077:   Batch Loss = 0.910450, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9041682481765747, Accuracy = 0.7772276997566223\n",
      "Iter #562560:  Learning rate = 0.004077:   Batch Loss = 1.012259, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8952528834342957, Accuracy = 0.7831683158874512\n",
      "Iter #563040:  Learning rate = 0.004077:   Batch Loss = 0.715992, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9127439260482788, Accuracy = 0.7683168053627014\n",
      "Iter #563520:  Learning rate = 0.004077:   Batch Loss = 0.793280, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9201223850250244, Accuracy = 0.7693069577217102\n",
      "Iter #564000:  Learning rate = 0.004077:   Batch Loss = 0.851468, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9684802889823914, Accuracy = 0.7574257254600525\n",
      "Iter #564480:  Learning rate = 0.004077:   Batch Loss = 0.805708, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9404900074005127, Accuracy = 0.7663366198539734\n",
      "Iter #564960:  Learning rate = 0.004077:   Batch Loss = 0.931397, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9282190203666687, Accuracy = 0.7792079448699951\n",
      "Iter #565440:  Learning rate = 0.004077:   Batch Loss = 0.794590, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9282480478286743, Accuracy = 0.7653465270996094\n",
      "Iter #565920:  Learning rate = 0.004077:   Batch Loss = 0.766471, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9062875509262085, Accuracy = 0.7772276997566223\n",
      "Iter #566400:  Learning rate = 0.004077:   Batch Loss = 0.701550, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9358007311820984, Accuracy = 0.7574257254600525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #566880:  Learning rate = 0.004077:   Batch Loss = 0.975162, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9156941771507263, Accuracy = 0.7712871432304382\n",
      "Iter #567360:  Learning rate = 0.004077:   Batch Loss = 0.854920, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9159091114997864, Accuracy = 0.7732673287391663\n",
      "Iter #567840:  Learning rate = 0.004077:   Batch Loss = 0.712999, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9037071466445923, Accuracy = 0.7623762488365173\n",
      "Iter #568320:  Learning rate = 0.004077:   Batch Loss = 0.820742, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9217090606689453, Accuracy = 0.7594059109687805\n",
      "Iter #568800:  Learning rate = 0.004077:   Batch Loss = 0.836711, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9426316022872925, Accuracy = 0.7603960633277893\n",
      "Iter #569280:  Learning rate = 0.004077:   Batch Loss = 0.706959, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9211958646774292, Accuracy = 0.7673267126083374\n",
      "Iter #569760:  Learning rate = 0.004077:   Batch Loss = 0.906985, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9425621032714844, Accuracy = 0.7485148310661316\n",
      "Iter #570240:  Learning rate = 0.004077:   Batch Loss = 1.031514, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9010109305381775, Accuracy = 0.7792079448699951\n",
      "Iter #570720:  Learning rate = 0.004077:   Batch Loss = 0.823388, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9465564489364624, Accuracy = 0.7366336584091187\n",
      "Iter #571200:  Learning rate = 0.004077:   Batch Loss = 0.931986, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9497218728065491, Accuracy = 0.7653465270996094\n",
      "Iter #571680:  Learning rate = 0.004077:   Batch Loss = 0.886990, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9153516888618469, Accuracy = 0.7792079448699951\n",
      "Iter #572160:  Learning rate = 0.004077:   Batch Loss = 0.875738, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9057604670524597, Accuracy = 0.7762376070022583\n",
      "Iter #572640:  Learning rate = 0.004077:   Batch Loss = 0.853820, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9390915632247925, Accuracy = 0.7594059109687805\n",
      "Iter #573120:  Learning rate = 0.004077:   Batch Loss = 1.001676, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.916944682598114, Accuracy = 0.7712871432304382\n",
      "Iter #573600:  Learning rate = 0.004077:   Batch Loss = 0.774808, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9523563385009766, Accuracy = 0.7405940890312195\n",
      "Iter #574080:  Learning rate = 0.004077:   Batch Loss = 0.760039, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9800624847412109, Accuracy = 0.7475247383117676\n",
      "Iter #574560:  Learning rate = 0.004077:   Batch Loss = 0.897056, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9484144449234009, Accuracy = 0.7534653544425964\n",
      "Iter #575040:  Learning rate = 0.004077:   Batch Loss = 0.798196, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9435778856277466, Accuracy = 0.7564356327056885\n",
      "Iter #575520:  Learning rate = 0.004077:   Batch Loss = 0.812007, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9139423966407776, Accuracy = 0.7702970504760742\n",
      "Iter #576000:  Learning rate = 0.004077:   Batch Loss = 0.739782, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8973032832145691, Accuracy = 0.7821782231330872\n",
      "Iter #576480:  Learning rate = 0.004077:   Batch Loss = 0.763783, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9008378386497498, Accuracy = 0.7821782231330872\n",
      "Iter #576960:  Learning rate = 0.004077:   Batch Loss = 0.760274, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9047843217849731, Accuracy = 0.7841584086418152\n",
      "Iter #577440:  Learning rate = 0.004077:   Batch Loss = 0.839814, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8995765447616577, Accuracy = 0.7752475142478943\n",
      "Iter #577920:  Learning rate = 0.004077:   Batch Loss = 0.848181, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8967402577400208, Accuracy = 0.7811881303787231\n",
      "Iter #578400:  Learning rate = 0.004077:   Batch Loss = 0.860016, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8701452016830444, Accuracy = 0.7851485013961792\n",
      "Iter #578880:  Learning rate = 0.004077:   Batch Loss = 0.856862, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8930984735488892, Accuracy = 0.7772276997566223\n",
      "Iter #579360:  Learning rate = 0.004077:   Batch Loss = 0.688966, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9131585955619812, Accuracy = 0.7673267126083374\n",
      "Iter #579840:  Learning rate = 0.004077:   Batch Loss = 0.778360, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9291074275970459, Accuracy = 0.7603960633277893\n",
      "Iter #580320:  Learning rate = 0.004077:   Batch Loss = 0.773591, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8979679346084595, Accuracy = 0.7841584086418152\n",
      "Iter #580800:  Learning rate = 0.004077:   Batch Loss = 0.728470, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9046193361282349, Accuracy = 0.7673267126083374\n",
      "Iter #581280:  Learning rate = 0.004077:   Batch Loss = 0.932084, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9292232990264893, Accuracy = 0.7653465270996094\n",
      "Iter #581760:  Learning rate = 0.004077:   Batch Loss = 0.809932, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.965457558631897, Accuracy = 0.7504950761795044\n",
      "Iter #582240:  Learning rate = 0.004077:   Batch Loss = 0.867648, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9534066915512085, Accuracy = 0.7564356327056885\n",
      "Iter #582720:  Learning rate = 0.004077:   Batch Loss = 0.667089, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9397149682044983, Accuracy = 0.7594059109687805\n",
      "Iter #583200:  Learning rate = 0.004077:   Batch Loss = 0.817275, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9533267021179199, Accuracy = 0.7524752616882324\n",
      "Iter #583680:  Learning rate = 0.004077:   Batch Loss = 0.907952, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9354568719863892, Accuracy = 0.7564356327056885\n",
      "Iter #584160:  Learning rate = 0.004077:   Batch Loss = 0.774701, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9697378873825073, Accuracy = 0.7514851689338684\n",
      "Iter #584640:  Learning rate = 0.004077:   Batch Loss = 0.698980, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9449039101600647, Accuracy = 0.7534653544425964\n",
      "Iter #585120:  Learning rate = 0.004077:   Batch Loss = 0.901799, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9496884942054749, Accuracy = 0.7534653544425964\n",
      "Iter #585600:  Learning rate = 0.004077:   Batch Loss = 0.588962, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9418882727622986, Accuracy = 0.7683168053627014\n",
      "Iter #586080:  Learning rate = 0.004077:   Batch Loss = 0.770141, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9423086047172546, Accuracy = 0.7504950761795044\n",
      "Iter #586560:  Learning rate = 0.004077:   Batch Loss = 0.742418, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9442768096923828, Accuracy = 0.7643564343452454\n",
      "Iter #587040:  Learning rate = 0.004077:   Batch Loss = 0.780048, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9437155723571777, Accuracy = 0.7594059109687805\n",
      "Iter #587520:  Learning rate = 0.004077:   Batch Loss = 0.701413, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.928497314453125, Accuracy = 0.7653465270996094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #588000:  Learning rate = 0.004077:   Batch Loss = 0.840491, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9300996661186218, Accuracy = 0.7534653544425964\n",
      "Iter #588480:  Learning rate = 0.004077:   Batch Loss = 0.876682, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9309290647506714, Accuracy = 0.7534653544425964\n",
      "Iter #588960:  Learning rate = 0.004077:   Batch Loss = 0.947167, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9573103189468384, Accuracy = 0.7455445528030396\n",
      "Iter #589440:  Learning rate = 0.004077:   Batch Loss = 0.886899, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9416120648384094, Accuracy = 0.7564356327056885\n",
      "Iter #589920:  Learning rate = 0.004077:   Batch Loss = 0.781199, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9201357364654541, Accuracy = 0.7623762488365173\n",
      "Iter #590400:  Learning rate = 0.004077:   Batch Loss = 0.927181, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9220821261405945, Accuracy = 0.7742574214935303\n",
      "Iter #590880:  Learning rate = 0.004077:   Batch Loss = 0.738681, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9467895030975342, Accuracy = 0.7643564343452454\n",
      "Iter #591360:  Learning rate = 0.004077:   Batch Loss = 0.587413, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9701962471008301, Accuracy = 0.7326732873916626\n",
      "Iter #591840:  Learning rate = 0.004077:   Batch Loss = 0.672722, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9564548134803772, Accuracy = 0.7643564343452454\n",
      "Iter #592320:  Learning rate = 0.004077:   Batch Loss = 0.928928, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9715097546577454, Accuracy = 0.7148514986038208\n",
      "Iter #592800:  Learning rate = 0.004077:   Batch Loss = 0.890104, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9849990010261536, Accuracy = 0.7514851689338684\n",
      "Iter #593280:  Learning rate = 0.004077:   Batch Loss = 0.765449, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9707743525505066, Accuracy = 0.7485148310661316\n",
      "Iter #593760:  Learning rate = 0.004077:   Batch Loss = 0.841392, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9391152858734131, Accuracy = 0.7603960633277893\n",
      "Iter #594240:  Learning rate = 0.004077:   Batch Loss = 0.891812, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9268678426742554, Accuracy = 0.7722772359848022\n",
      "Iter #594720:  Learning rate = 0.004077:   Batch Loss = 0.769695, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9153280854225159, Accuracy = 0.7752475142478943\n",
      "Iter #595200:  Learning rate = 0.004077:   Batch Loss = 0.652597, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9266408085823059, Accuracy = 0.7702970504760742\n",
      "Iter #595680:  Learning rate = 0.004077:   Batch Loss = 0.750487, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0360586643218994, Accuracy = 0.7267326712608337\n",
      "Iter #596160:  Learning rate = 0.004077:   Batch Loss = 0.934273, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9679265022277832, Accuracy = 0.7485148310661316\n",
      "Iter #596640:  Learning rate = 0.004077:   Batch Loss = 0.684427, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9533661007881165, Accuracy = 0.7485148310661316\n",
      "Iter #597120:  Learning rate = 0.004077:   Batch Loss = 0.903772, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9538276791572571, Accuracy = 0.7445544600486755\n",
      "Iter #597600:  Learning rate = 0.004077:   Batch Loss = 0.919574, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9656342267990112, Accuracy = 0.7415841817855835\n",
      "Iter #598080:  Learning rate = 0.004077:   Batch Loss = 0.728001, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9434835910797119, Accuracy = 0.7534653544425964\n",
      "Iter #598560:  Learning rate = 0.004077:   Batch Loss = 0.715987, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.907293438911438, Accuracy = 0.7742574214935303\n",
      "Iter #599040:  Learning rate = 0.004077:   Batch Loss = 0.723736, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.894546389579773, Accuracy = 0.7920792102813721\n",
      "Iter #599520:  Learning rate = 0.004077:   Batch Loss = 0.809695, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9454555511474609, Accuracy = 0.7574257254600525\n",
      "Iter #600000:  Learning rate = 0.003914:   Batch Loss = 0.841667, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9318349361419678, Accuracy = 0.7584158182144165\n",
      "Iter #600480:  Learning rate = 0.003914:   Batch Loss = 0.640534, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9218760132789612, Accuracy = 0.7683168053627014\n",
      "Iter #600960:  Learning rate = 0.003914:   Batch Loss = 0.851822, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.90953129529953, Accuracy = 0.7732673287391663\n",
      "Iter #601440:  Learning rate = 0.003914:   Batch Loss = 0.755419, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9055209159851074, Accuracy = 0.7851485013961792\n",
      "Iter #601920:  Learning rate = 0.003914:   Batch Loss = 0.686867, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9009473919868469, Accuracy = 0.7683168053627014\n",
      "Iter #602400:  Learning rate = 0.003914:   Batch Loss = 0.816576, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9095473885536194, Accuracy = 0.7722772359848022\n",
      "Iter #602880:  Learning rate = 0.003914:   Batch Loss = 0.734585, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.903113603591919, Accuracy = 0.7752475142478943\n",
      "Iter #603360:  Learning rate = 0.003914:   Batch Loss = 0.698857, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.920382559299469, Accuracy = 0.7495049238204956\n",
      "Iter #603840:  Learning rate = 0.003914:   Batch Loss = 0.626313, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9305095672607422, Accuracy = 0.7663366198539734\n",
      "Iter #604320:  Learning rate = 0.003914:   Batch Loss = 0.789202, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9161674380302429, Accuracy = 0.7693069577217102\n",
      "Iter #604800:  Learning rate = 0.003914:   Batch Loss = 0.577689, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9030656218528748, Accuracy = 0.7722772359848022\n",
      "Iter #605280:  Learning rate = 0.003914:   Batch Loss = 0.918427, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9101829528808594, Accuracy = 0.7693069577217102\n",
      "Iter #605760:  Learning rate = 0.003914:   Batch Loss = 0.958408, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9244364500045776, Accuracy = 0.7653465270996094\n",
      "Iter #606240:  Learning rate = 0.003914:   Batch Loss = 0.790199, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9273591637611389, Accuracy = 0.7722772359848022\n",
      "Iter #606720:  Learning rate = 0.003914:   Batch Loss = 0.770459, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9373065233230591, Accuracy = 0.7584158182144165\n",
      "Iter #607200:  Learning rate = 0.003914:   Batch Loss = 0.626590, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9156467318534851, Accuracy = 0.7752475142478943\n",
      "Iter #607680:  Learning rate = 0.003914:   Batch Loss = 0.971116, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9268083572387695, Accuracy = 0.7712871432304382\n",
      "Iter #608160:  Learning rate = 0.003914:   Batch Loss = 0.920327, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9190173745155334, Accuracy = 0.7752475142478943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #608640:  Learning rate = 0.003914:   Batch Loss = 0.613992, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9226985573768616, Accuracy = 0.7861385941505432\n",
      "Iter #609120:  Learning rate = 0.003914:   Batch Loss = 0.830625, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9202243685722351, Accuracy = 0.7722772359848022\n",
      "Iter #609600:  Learning rate = 0.003914:   Batch Loss = 0.684653, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9177485704421997, Accuracy = 0.7772276997566223\n",
      "Iter #610080:  Learning rate = 0.003914:   Batch Loss = 0.828041, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9099208116531372, Accuracy = 0.7851485013961792\n",
      "Iter #610560:  Learning rate = 0.003914:   Batch Loss = 0.734543, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9021756052970886, Accuracy = 0.7831683158874512\n",
      "Iter #611040:  Learning rate = 0.003914:   Batch Loss = 0.912250, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9054460525512695, Accuracy = 0.7782177925109863\n",
      "Iter #611520:  Learning rate = 0.003914:   Batch Loss = 0.707076, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9204994440078735, Accuracy = 0.7772276997566223\n",
      "Iter #612000:  Learning rate = 0.003914:   Batch Loss = 0.720312, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9131646156311035, Accuracy = 0.7792079448699951\n",
      "Iter #612480:  Learning rate = 0.003914:   Batch Loss = 0.768085, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9475138187408447, Accuracy = 0.7722772359848022\n",
      "Iter #612960:  Learning rate = 0.003914:   Batch Loss = 0.741624, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.916661262512207, Accuracy = 0.78910893201828\n",
      "Iter #613440:  Learning rate = 0.003914:   Batch Loss = 0.821290, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9590541124343872, Accuracy = 0.7405940890312195\n",
      "Iter #613920:  Learning rate = 0.003914:   Batch Loss = 0.882581, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9325993657112122, Accuracy = 0.7603960633277893\n",
      "Iter #614400:  Learning rate = 0.003914:   Batch Loss = 0.770582, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9350460767745972, Accuracy = 0.7495049238204956\n",
      "Iter #614880:  Learning rate = 0.003914:   Batch Loss = 0.580080, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9401661157608032, Accuracy = 0.7574257254600525\n",
      "Iter #615360:  Learning rate = 0.003914:   Batch Loss = 0.928886, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9314112663269043, Accuracy = 0.7633663415908813\n",
      "Iter #615840:  Learning rate = 0.003914:   Batch Loss = 0.697827, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9385501146316528, Accuracy = 0.7574257254600525\n",
      "Iter #616320:  Learning rate = 0.003914:   Batch Loss = 0.713984, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9142541885375977, Accuracy = 0.7673267126083374\n",
      "Iter #616800:  Learning rate = 0.003914:   Batch Loss = 0.919839, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9095779657363892, Accuracy = 0.7712871432304382\n",
      "Iter #617280:  Learning rate = 0.003914:   Batch Loss = 1.015425, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9691132307052612, Accuracy = 0.7425742745399475\n",
      "Iter #617760:  Learning rate = 0.003914:   Batch Loss = 0.805288, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.896349310874939, Accuracy = 0.7831683158874512\n",
      "Iter #618240:  Learning rate = 0.003914:   Batch Loss = 0.629424, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8913917541503906, Accuracy = 0.7841584086418152\n",
      "Iter #618720:  Learning rate = 0.003914:   Batch Loss = 1.017798, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9269382357597351, Accuracy = 0.7693069577217102\n",
      "Iter #619200:  Learning rate = 0.003914:   Batch Loss = 0.848762, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.90338534116745, Accuracy = 0.7752475142478943\n",
      "Iter #619680:  Learning rate = 0.003914:   Batch Loss = 0.846552, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.887570321559906, Accuracy = 0.7871286869049072\n",
      "Iter #620160:  Learning rate = 0.003914:   Batch Loss = 0.640048, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9453727006912231, Accuracy = 0.7643564343452454\n",
      "Iter #620640:  Learning rate = 0.003914:   Batch Loss = 0.786744, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9013774394989014, Accuracy = 0.7673267126083374\n",
      "Iter #621120:  Learning rate = 0.003914:   Batch Loss = 0.869700, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9174122214317322, Accuracy = 0.7584158182144165\n",
      "Iter #621600:  Learning rate = 0.003914:   Batch Loss = 0.588766, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9120999574661255, Accuracy = 0.7792079448699951\n",
      "Iter #622080:  Learning rate = 0.003914:   Batch Loss = 0.965871, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9088578820228577, Accuracy = 0.7851485013961792\n",
      "Iter #622560:  Learning rate = 0.003914:   Batch Loss = 0.750491, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9205433130264282, Accuracy = 0.7712871432304382\n",
      "Iter #623040:  Learning rate = 0.003914:   Batch Loss = 0.593475, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9188841581344604, Accuracy = 0.7782177925109863\n",
      "Iter #623520:  Learning rate = 0.003914:   Batch Loss = 0.977540, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9359283447265625, Accuracy = 0.7653465270996094\n",
      "Iter #624000:  Learning rate = 0.003914:   Batch Loss = 0.731857, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9365466237068176, Accuracy = 0.7643564343452454\n",
      "Iter #624480:  Learning rate = 0.003914:   Batch Loss = 0.897849, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.949627697467804, Accuracy = 0.7584158182144165\n",
      "Iter #624960:  Learning rate = 0.003914:   Batch Loss = 0.930091, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9173712730407715, Accuracy = 0.7712871432304382\n",
      "Iter #625440:  Learning rate = 0.003914:   Batch Loss = 0.848241, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9152089357376099, Accuracy = 0.7712871432304382\n",
      "Iter #625920:  Learning rate = 0.003914:   Batch Loss = 0.645581, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9075093865394592, Accuracy = 0.7762376070022583\n",
      "Iter #626400:  Learning rate = 0.003914:   Batch Loss = 0.902141, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9074400663375854, Accuracy = 0.7811881303787231\n",
      "Iter #626880:  Learning rate = 0.003914:   Batch Loss = 0.990756, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9054831266403198, Accuracy = 0.7722772359848022\n",
      "Iter #627360:  Learning rate = 0.003914:   Batch Loss = 0.769126, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8973447680473328, Accuracy = 0.7732673287391663\n",
      "Iter #627840:  Learning rate = 0.003914:   Batch Loss = 0.720080, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9000879526138306, Accuracy = 0.7831683158874512\n",
      "Iter #628320:  Learning rate = 0.003914:   Batch Loss = 0.847359, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9067444205284119, Accuracy = 0.7633663415908813\n",
      "Iter #628800:  Learning rate = 0.003914:   Batch Loss = 0.791418, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9139969944953918, Accuracy = 0.7712871432304382\n",
      "Iter #629280:  Learning rate = 0.003914:   Batch Loss = 0.747731, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9337654113769531, Accuracy = 0.7603960633277893\n",
      "Iter #629760:  Learning rate = 0.003914:   Batch Loss = 0.922614, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9164403676986694, Accuracy = 0.7613861560821533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #630240:  Learning rate = 0.003914:   Batch Loss = 0.848608, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9370213747024536, Accuracy = 0.7643564343452454\n",
      "Iter #630720:  Learning rate = 0.003914:   Batch Loss = 0.704939, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9497804641723633, Accuracy = 0.7544554471969604\n",
      "Iter #631200:  Learning rate = 0.003914:   Batch Loss = 0.828856, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9272054433822632, Accuracy = 0.7633663415908813\n",
      "Iter #631680:  Learning rate = 0.003914:   Batch Loss = 0.904414, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8961178660392761, Accuracy = 0.788118839263916\n",
      "Iter #632160:  Learning rate = 0.003914:   Batch Loss = 0.757885, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.888197124004364, Accuracy = 0.788118839263916\n",
      "Iter #632640:  Learning rate = 0.003914:   Batch Loss = 0.635385, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8940526843070984, Accuracy = 0.7732673287391663\n",
      "Iter #633120:  Learning rate = 0.003914:   Batch Loss = 0.743388, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9109386801719666, Accuracy = 0.7762376070022583\n",
      "Iter #633600:  Learning rate = 0.003914:   Batch Loss = 0.893969, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9128178358078003, Accuracy = 0.7752475142478943\n",
      "Iter #634080:  Learning rate = 0.003914:   Batch Loss = 0.823482, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8891785740852356, Accuracy = 0.7811881303787231\n",
      "Iter #634560:  Learning rate = 0.003914:   Batch Loss = 0.749520, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8894139528274536, Accuracy = 0.7851485013961792\n",
      "Iter #635040:  Learning rate = 0.003914:   Batch Loss = 0.661391, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8870840072631836, Accuracy = 0.7782177925109863\n",
      "Iter #635520:  Learning rate = 0.003914:   Batch Loss = 0.632366, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8791660070419312, Accuracy = 0.7801980376243591\n",
      "Iter #636000:  Learning rate = 0.003914:   Batch Loss = 0.742109, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8890644311904907, Accuracy = 0.7792079448699951\n",
      "Iter #636480:  Learning rate = 0.003914:   Batch Loss = 0.756504, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8979722261428833, Accuracy = 0.7772276997566223\n",
      "Iter #636960:  Learning rate = 0.003914:   Batch Loss = 0.678639, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.903171181678772, Accuracy = 0.7752475142478943\n",
      "Iter #637440:  Learning rate = 0.003914:   Batch Loss = 0.663057, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8888355493545532, Accuracy = 0.7831683158874512\n",
      "Iter #637920:  Learning rate = 0.003914:   Batch Loss = 0.912238, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8857840299606323, Accuracy = 0.7792079448699951\n",
      "Iter #638400:  Learning rate = 0.003914:   Batch Loss = 0.539052, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9087400436401367, Accuracy = 0.7831683158874512\n",
      "Iter #638880:  Learning rate = 0.003914:   Batch Loss = 0.642444, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9241852164268494, Accuracy = 0.7722772359848022\n",
      "Iter #639360:  Learning rate = 0.003914:   Batch Loss = 0.870670, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9620172381401062, Accuracy = 0.7376237511634827\n",
      "Iter #639840:  Learning rate = 0.003914:   Batch Loss = 0.895268, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.926378607749939, Accuracy = 0.7643564343452454\n",
      "Iter #640320:  Learning rate = 0.003914:   Batch Loss = 0.851294, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9408316612243652, Accuracy = 0.7683168053627014\n",
      "Iter #640800:  Learning rate = 0.003914:   Batch Loss = 1.011923, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9150314927101135, Accuracy = 0.7801980376243591\n",
      "Iter #641280:  Learning rate = 0.003914:   Batch Loss = 1.044275, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9138696789741516, Accuracy = 0.7772276997566223\n",
      "Iter #641760:  Learning rate = 0.003914:   Batch Loss = 0.878326, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9367697238922119, Accuracy = 0.7564356327056885\n",
      "Iter #642240:  Learning rate = 0.003914:   Batch Loss = 0.654684, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9393265247344971, Accuracy = 0.7544554471969604\n",
      "Iter #642720:  Learning rate = 0.003914:   Batch Loss = 0.900626, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9421105980873108, Accuracy = 0.7564356327056885\n",
      "Iter #643200:  Learning rate = 0.003914:   Batch Loss = 0.704684, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9423906803131104, Accuracy = 0.7465346455574036\n",
      "Iter #643680:  Learning rate = 0.003914:   Batch Loss = 0.770979, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9327173233032227, Accuracy = 0.7544554471969604\n",
      "Iter #644160:  Learning rate = 0.003914:   Batch Loss = 0.820472, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9257347583770752, Accuracy = 0.7683168053627014\n",
      "Iter #644640:  Learning rate = 0.003914:   Batch Loss = 1.137993, Accuracy = 0.6166666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0052436590194702, Accuracy = 0.7574257254600525\n",
      "Iter #645120:  Learning rate = 0.003914:   Batch Loss = 0.837351, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0206623077392578, Accuracy = 0.7306930422782898\n",
      "Iter #645600:  Learning rate = 0.003914:   Batch Loss = 0.978667, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9831333160400391, Accuracy = 0.7445544600486755\n",
      "Iter #646080:  Learning rate = 0.003914:   Batch Loss = 0.768390, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9514304995536804, Accuracy = 0.7584158182144165\n",
      "Iter #646560:  Learning rate = 0.003914:   Batch Loss = 0.698652, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9262759685516357, Accuracy = 0.7663366198539734\n",
      "Iter #647040:  Learning rate = 0.003914:   Batch Loss = 0.753799, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9282112717628479, Accuracy = 0.7693069577217102\n",
      "Iter #647520:  Learning rate = 0.003914:   Batch Loss = 0.858296, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9342188835144043, Accuracy = 0.7673267126083374\n",
      "Iter #648000:  Learning rate = 0.003914:   Batch Loss = 0.706634, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9465903639793396, Accuracy = 0.7514851689338684\n",
      "Iter #648480:  Learning rate = 0.003914:   Batch Loss = 0.700646, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9461604356765747, Accuracy = 0.7693069577217102\n",
      "Iter #648960:  Learning rate = 0.003914:   Batch Loss = 0.740915, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9466923475265503, Accuracy = 0.7534653544425964\n",
      "Iter #649440:  Learning rate = 0.003914:   Batch Loss = 0.787869, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9355984330177307, Accuracy = 0.7386138439178467\n",
      "Iter #649920:  Learning rate = 0.003914:   Batch Loss = 0.731337, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9518229961395264, Accuracy = 0.7465346455574036\n",
      "Iter #650400:  Learning rate = 0.003914:   Batch Loss = 0.727795, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9346346855163574, Accuracy = 0.7594059109687805\n",
      "Iter #650880:  Learning rate = 0.003914:   Batch Loss = 0.697562, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9353570342063904, Accuracy = 0.7693069577217102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #651360:  Learning rate = 0.003914:   Batch Loss = 0.766774, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9379625916481018, Accuracy = 0.7603960633277893\n",
      "Iter #651840:  Learning rate = 0.003914:   Batch Loss = 0.926668, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9324791431427002, Accuracy = 0.7623762488365173\n",
      "Iter #652320:  Learning rate = 0.003914:   Batch Loss = 0.710166, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9316738247871399, Accuracy = 0.7564356327056885\n",
      "Iter #652800:  Learning rate = 0.003914:   Batch Loss = 1.169544, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9721028804779053, Accuracy = 0.7455445528030396\n",
      "Iter #653280:  Learning rate = 0.003914:   Batch Loss = 0.827804, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0102170705795288, Accuracy = 0.7138614058494568\n",
      "Iter #653760:  Learning rate = 0.003914:   Batch Loss = 0.689835, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0662384033203125, Accuracy = 0.7089108824729919\n",
      "Iter #654240:  Learning rate = 0.003914:   Batch Loss = 0.913587, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0239360332489014, Accuracy = 0.7217822074890137\n",
      "Iter #654720:  Learning rate = 0.003914:   Batch Loss = 0.869471, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0388140678405762, Accuracy = 0.6900990009307861\n",
      "Iter #655200:  Learning rate = 0.003914:   Batch Loss = 0.944198, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9967422485351562, Accuracy = 0.7326732873916626\n",
      "Iter #655680:  Learning rate = 0.003914:   Batch Loss = 0.880230, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9675345420837402, Accuracy = 0.7346534729003906\n",
      "Iter #656160:  Learning rate = 0.003914:   Batch Loss = 0.782974, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9294754862785339, Accuracy = 0.7673267126083374\n",
      "Iter #656640:  Learning rate = 0.003914:   Batch Loss = 0.723501, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9162965416908264, Accuracy = 0.7722772359848022\n",
      "Iter #657120:  Learning rate = 0.003914:   Batch Loss = 0.780803, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9102829694747925, Accuracy = 0.7792079448699951\n",
      "Iter #657600:  Learning rate = 0.003914:   Batch Loss = 0.632807, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9182714223861694, Accuracy = 0.7673267126083374\n",
      "Iter #658080:  Learning rate = 0.003914:   Batch Loss = 0.673752, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.938823938369751, Accuracy = 0.7653465270996094\n",
      "Iter #658560:  Learning rate = 0.003914:   Batch Loss = 0.916880, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9064780473709106, Accuracy = 0.7683168053627014\n",
      "Iter #659040:  Learning rate = 0.003914:   Batch Loss = 0.755888, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9036833047866821, Accuracy = 0.7782177925109863\n",
      "Iter #659520:  Learning rate = 0.003914:   Batch Loss = 0.977762, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9248747229576111, Accuracy = 0.7613861560821533\n",
      "Iter #660000:  Learning rate = 0.003914:   Batch Loss = 0.727431, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9120913147926331, Accuracy = 0.7504950761795044\n",
      "Iter #660480:  Learning rate = 0.003914:   Batch Loss = 0.810472, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9132755994796753, Accuracy = 0.7693069577217102\n",
      "Iter #660960:  Learning rate = 0.003914:   Batch Loss = 0.670066, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9148495197296143, Accuracy = 0.7693069577217102\n",
      "Iter #661440:  Learning rate = 0.003914:   Batch Loss = 0.817290, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9348299503326416, Accuracy = 0.7663366198539734\n",
      "Iter #661920:  Learning rate = 0.003914:   Batch Loss = 0.786255, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9088425636291504, Accuracy = 0.7762376070022583\n",
      "Iter #662400:  Learning rate = 0.003914:   Batch Loss = 0.858893, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9066718816757202, Accuracy = 0.7792079448699951\n",
      "Iter #662880:  Learning rate = 0.003914:   Batch Loss = 0.554388, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9122127890586853, Accuracy = 0.7683168053627014\n",
      "Iter #663360:  Learning rate = 0.003914:   Batch Loss = 0.745110, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9201731085777283, Accuracy = 0.7663366198539734\n",
      "Iter #663840:  Learning rate = 0.003914:   Batch Loss = 0.788850, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9305981993675232, Accuracy = 0.7663366198539734\n",
      "Iter #664320:  Learning rate = 0.003914:   Batch Loss = 0.736970, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9077506065368652, Accuracy = 0.7712871432304382\n",
      "Iter #664800:  Learning rate = 0.003914:   Batch Loss = 0.788430, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8850129842758179, Accuracy = 0.7871286869049072\n",
      "Iter #665280:  Learning rate = 0.003914:   Batch Loss = 0.693648, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8899640440940857, Accuracy = 0.7792079448699951\n",
      "Iter #665760:  Learning rate = 0.003914:   Batch Loss = 0.742896, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9203758239746094, Accuracy = 0.7663366198539734\n",
      "Iter #666240:  Learning rate = 0.003914:   Batch Loss = 0.922801, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.916671633720398, Accuracy = 0.7613861560821533\n",
      "Iter #666720:  Learning rate = 0.003914:   Batch Loss = 0.989915, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9459819793701172, Accuracy = 0.7495049238204956\n",
      "Iter #667200:  Learning rate = 0.003914:   Batch Loss = 0.606863, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9794216752052307, Accuracy = 0.7356435656547546\n",
      "Iter #667680:  Learning rate = 0.003914:   Batch Loss = 0.921602, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9527173042297363, Accuracy = 0.7396039366722107\n",
      "Iter #668160:  Learning rate = 0.003914:   Batch Loss = 0.960919, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.96156907081604, Accuracy = 0.7376237511634827\n",
      "Iter #668640:  Learning rate = 0.003914:   Batch Loss = 0.740480, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9392632246017456, Accuracy = 0.7564356327056885\n",
      "Iter #669120:  Learning rate = 0.003914:   Batch Loss = 0.885431, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9214115738868713, Accuracy = 0.7594059109687805\n",
      "Iter #669600:  Learning rate = 0.003914:   Batch Loss = 0.931693, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9299407005310059, Accuracy = 0.7504950761795044\n",
      "Iter #670080:  Learning rate = 0.003914:   Batch Loss = 0.729856, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9264484643936157, Accuracy = 0.7603960633277893\n",
      "Iter #670560:  Learning rate = 0.003914:   Batch Loss = 0.518030, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9167808294296265, Accuracy = 0.7702970504760742\n",
      "Iter #671040:  Learning rate = 0.003914:   Batch Loss = 0.546488, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9198031425476074, Accuracy = 0.7752475142478943\n",
      "Iter #671520:  Learning rate = 0.003914:   Batch Loss = 1.019400, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9238202571868896, Accuracy = 0.7683168053627014\n",
      "Iter #672000:  Learning rate = 0.003914:   Batch Loss = 0.764043, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9209098815917969, Accuracy = 0.7732673287391663\n",
      "Iter #672480:  Learning rate = 0.003914:   Batch Loss = 0.752473, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.940494954586029, Accuracy = 0.7574257254600525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #672960:  Learning rate = 0.003914:   Batch Loss = 0.794323, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9428651332855225, Accuracy = 0.7643564343452454\n",
      "Iter #673440:  Learning rate = 0.003914:   Batch Loss = 0.802138, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9192956686019897, Accuracy = 0.7633663415908813\n",
      "Iter #673920:  Learning rate = 0.003914:   Batch Loss = 0.721576, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9354826807975769, Accuracy = 0.7455445528030396\n",
      "Iter #674400:  Learning rate = 0.003914:   Batch Loss = 0.642261, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9531600475311279, Accuracy = 0.7356435656547546\n",
      "Iter #674880:  Learning rate = 0.003914:   Batch Loss = 0.722677, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9442131519317627, Accuracy = 0.7495049238204956\n",
      "Iter #675360:  Learning rate = 0.003914:   Batch Loss = 0.887465, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9476442337036133, Accuracy = 0.7544554471969604\n",
      "Iter #675840:  Learning rate = 0.003914:   Batch Loss = 0.691701, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9244101643562317, Accuracy = 0.7623762488365173\n",
      "Iter #676320:  Learning rate = 0.003914:   Batch Loss = 1.022375, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0183672904968262, Accuracy = 0.71089106798172\n",
      "Iter #676800:  Learning rate = 0.003914:   Batch Loss = 0.999340, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9590138792991638, Accuracy = 0.7415841817855835\n",
      "Iter #677280:  Learning rate = 0.003914:   Batch Loss = 0.816030, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9615616202354431, Accuracy = 0.7396039366722107\n",
      "Iter #677760:  Learning rate = 0.003914:   Batch Loss = 0.784504, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9465222954750061, Accuracy = 0.7376237511634827\n",
      "Iter #678240:  Learning rate = 0.003914:   Batch Loss = 0.982824, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9084383845329285, Accuracy = 0.7683168053627014\n",
      "Iter #678720:  Learning rate = 0.003914:   Batch Loss = 0.949275, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9332351684570312, Accuracy = 0.7514851689338684\n",
      "Iter #679200:  Learning rate = 0.003914:   Batch Loss = 0.947767, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9149113893508911, Accuracy = 0.7574257254600525\n",
      "Iter #679680:  Learning rate = 0.003914:   Batch Loss = 0.733861, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9277076125144958, Accuracy = 0.7613861560821533\n",
      "Iter #680160:  Learning rate = 0.003914:   Batch Loss = 0.797325, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.902661919593811, Accuracy = 0.7594059109687805\n",
      "Iter #680640:  Learning rate = 0.003914:   Batch Loss = 1.012089, Accuracy = 0.6499999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9262721538543701, Accuracy = 0.7514851689338684\n",
      "Iter #681120:  Learning rate = 0.003914:   Batch Loss = 0.584273, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9220808744430542, Accuracy = 0.7584158182144165\n",
      "Iter #681600:  Learning rate = 0.003914:   Batch Loss = 0.730785, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8923476338386536, Accuracy = 0.7643564343452454\n",
      "Iter #682080:  Learning rate = 0.003914:   Batch Loss = 0.741366, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.893082320690155, Accuracy = 0.7702970504760742\n",
      "Iter #682560:  Learning rate = 0.003914:   Batch Loss = 0.728280, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8992100358009338, Accuracy = 0.7673267126083374\n",
      "Iter #683040:  Learning rate = 0.003914:   Batch Loss = 0.694436, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8947566151618958, Accuracy = 0.7732673287391663\n",
      "Iter #683520:  Learning rate = 0.003914:   Batch Loss = 0.817924, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9006574153900146, Accuracy = 0.7752475142478943\n",
      "Iter #684000:  Learning rate = 0.003914:   Batch Loss = 0.867128, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9085054993629456, Accuracy = 0.7673267126083374\n",
      "Iter #684480:  Learning rate = 0.003914:   Batch Loss = 0.773801, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9247909188270569, Accuracy = 0.7514851689338684\n",
      "Iter #684960:  Learning rate = 0.003914:   Batch Loss = 0.899970, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.891179621219635, Accuracy = 0.7702970504760742\n",
      "Iter #685440:  Learning rate = 0.003914:   Batch Loss = 0.790288, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9171760082244873, Accuracy = 0.7594059109687805\n",
      "Iter #685920:  Learning rate = 0.003914:   Batch Loss = 0.653909, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9045289158821106, Accuracy = 0.7732673287391663\n",
      "Iter #686400:  Learning rate = 0.003914:   Batch Loss = 0.709796, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9100197553634644, Accuracy = 0.7613861560821533\n",
      "Iter #686880:  Learning rate = 0.003914:   Batch Loss = 0.979870, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8863857984542847, Accuracy = 0.7851485013961792\n",
      "Iter #687360:  Learning rate = 0.003914:   Batch Loss = 0.836324, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8768206238746643, Accuracy = 0.7762376070022583\n",
      "Iter #687840:  Learning rate = 0.003914:   Batch Loss = 0.768964, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8947317004203796, Accuracy = 0.7772276997566223\n",
      "Iter #688320:  Learning rate = 0.003914:   Batch Loss = 0.696182, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9101114869117737, Accuracy = 0.7782177925109863\n",
      "Iter #688800:  Learning rate = 0.003914:   Batch Loss = 0.708015, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.87913578748703, Accuracy = 0.7801980376243591\n",
      "Iter #689280:  Learning rate = 0.003914:   Batch Loss = 0.694162, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8940282464027405, Accuracy = 0.7841584086418152\n",
      "Iter #689760:  Learning rate = 0.003914:   Batch Loss = 0.675810, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.876157283782959, Accuracy = 0.7910891175270081\n",
      "Iter #690240:  Learning rate = 0.003914:   Batch Loss = 0.849054, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9060850739479065, Accuracy = 0.7811881303787231\n",
      "Iter #690720:  Learning rate = 0.003914:   Batch Loss = 0.887435, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.898147463798523, Accuracy = 0.7801980376243591\n",
      "Iter #691200:  Learning rate = 0.003914:   Batch Loss = 0.814381, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8970540761947632, Accuracy = 0.7683168053627014\n",
      "Iter #691680:  Learning rate = 0.003914:   Batch Loss = 0.795956, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9059483408927917, Accuracy = 0.7722772359848022\n",
      "Iter #692160:  Learning rate = 0.003914:   Batch Loss = 0.649744, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9061371088027954, Accuracy = 0.7712871432304382\n",
      "Iter #692640:  Learning rate = 0.003914:   Batch Loss = 0.947504, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8983691930770874, Accuracy = 0.7752475142478943\n",
      "Iter #693120:  Learning rate = 0.003914:   Batch Loss = 0.849555, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9277368783950806, Accuracy = 0.7564356327056885\n",
      "Iter #693600:  Learning rate = 0.003914:   Batch Loss = 0.868591, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9744556546211243, Accuracy = 0.7316831946372986\n",
      "Iter #694080:  Learning rate = 0.003914:   Batch Loss = 0.828342, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9413797855377197, Accuracy = 0.7544554471969604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #694560:  Learning rate = 0.003914:   Batch Loss = 0.698860, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9262597560882568, Accuracy = 0.7663366198539734\n",
      "Iter #695040:  Learning rate = 0.003914:   Batch Loss = 0.752308, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9198161363601685, Accuracy = 0.7693069577217102\n",
      "Iter #695520:  Learning rate = 0.003914:   Batch Loss = 0.813441, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9191203117370605, Accuracy = 0.7633663415908813\n",
      "Iter #696000:  Learning rate = 0.003914:   Batch Loss = 0.724064, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9106467366218567, Accuracy = 0.7712871432304382\n",
      "Iter #696480:  Learning rate = 0.003914:   Batch Loss = 0.699087, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9064912796020508, Accuracy = 0.7712871432304382\n",
      "Iter #696960:  Learning rate = 0.003914:   Batch Loss = 0.720680, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9009708166122437, Accuracy = 0.7792079448699951\n",
      "Iter #697440:  Learning rate = 0.003914:   Batch Loss = 0.598470, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9052464962005615, Accuracy = 0.7811881303787231\n",
      "Iter #697920:  Learning rate = 0.003914:   Batch Loss = 0.770120, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9262265563011169, Accuracy = 0.7623762488365173\n",
      "Iter #698400:  Learning rate = 0.003914:   Batch Loss = 0.775013, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9132252335548401, Accuracy = 0.7653465270996094\n",
      "Iter #698880:  Learning rate = 0.003914:   Batch Loss = 0.768520, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9037545919418335, Accuracy = 0.7643564343452454\n",
      "Iter #699360:  Learning rate = 0.003914:   Batch Loss = 0.738976, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9251419901847839, Accuracy = 0.7574257254600525\n",
      "Iter #699840:  Learning rate = 0.003914:   Batch Loss = 0.840901, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9032677412033081, Accuracy = 0.7643564343452454\n",
      "Iter #700320:  Learning rate = 0.003757:   Batch Loss = 0.846627, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8826184272766113, Accuracy = 0.7772276997566223\n",
      "Iter #700800:  Learning rate = 0.003757:   Batch Loss = 0.712239, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8693965077400208, Accuracy = 0.7871286869049072\n",
      "Iter #701280:  Learning rate = 0.003757:   Batch Loss = 0.730674, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8834589719772339, Accuracy = 0.7821782231330872\n",
      "Iter #701760:  Learning rate = 0.003757:   Batch Loss = 0.544155, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9308512210845947, Accuracy = 0.7554455399513245\n",
      "Iter #702240:  Learning rate = 0.003757:   Batch Loss = 0.669737, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9212036728858948, Accuracy = 0.7613861560821533\n",
      "Iter #702720:  Learning rate = 0.003757:   Batch Loss = 0.872797, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9420673847198486, Accuracy = 0.7603960633277893\n",
      "Iter #703200:  Learning rate = 0.003757:   Batch Loss = 0.812277, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9524468183517456, Accuracy = 0.7504950761795044\n",
      "Iter #703680:  Learning rate = 0.003757:   Batch Loss = 0.899672, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9339190721511841, Accuracy = 0.7574257254600525\n",
      "Iter #704160:  Learning rate = 0.003757:   Batch Loss = 0.865905, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9358623623847961, Accuracy = 0.7574257254600525\n",
      "Iter #704640:  Learning rate = 0.003757:   Batch Loss = 0.756329, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9392569065093994, Accuracy = 0.7613861560821533\n",
      "Iter #705120:  Learning rate = 0.003757:   Batch Loss = 0.733063, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9104119539260864, Accuracy = 0.7702970504760742\n",
      "Iter #705600:  Learning rate = 0.003757:   Batch Loss = 0.841567, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9175493717193604, Accuracy = 0.7742574214935303\n",
      "Iter #706080:  Learning rate = 0.003757:   Batch Loss = 0.701444, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9140666127204895, Accuracy = 0.7683168053627014\n",
      "Iter #706560:  Learning rate = 0.003757:   Batch Loss = 0.876428, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9283690452575684, Accuracy = 0.7613861560821533\n",
      "Iter #707040:  Learning rate = 0.003757:   Batch Loss = 0.868290, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9285826086997986, Accuracy = 0.7613861560821533\n",
      "Iter #707520:  Learning rate = 0.003757:   Batch Loss = 0.734658, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.92119300365448, Accuracy = 0.7673267126083374\n",
      "Iter #708000:  Learning rate = 0.003757:   Batch Loss = 0.700382, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9399755001068115, Accuracy = 0.7603960633277893\n",
      "Iter #708480:  Learning rate = 0.003757:   Batch Loss = 0.761203, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.946060299873352, Accuracy = 0.7603960633277893\n",
      "Iter #708960:  Learning rate = 0.003757:   Batch Loss = 0.933594, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9148286581039429, Accuracy = 0.7732673287391663\n",
      "Iter #709440:  Learning rate = 0.003757:   Batch Loss = 0.586471, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9024909734725952, Accuracy = 0.7742574214935303\n",
      "Iter #709920:  Learning rate = 0.003757:   Batch Loss = 0.807989, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9026414155960083, Accuracy = 0.7663366198539734\n",
      "Iter #710400:  Learning rate = 0.003757:   Batch Loss = 0.700788, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9118011593818665, Accuracy = 0.7702970504760742\n",
      "Iter #710880:  Learning rate = 0.003757:   Batch Loss = 0.762813, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9172226190567017, Accuracy = 0.7623762488365173\n",
      "Iter #711360:  Learning rate = 0.003757:   Batch Loss = 0.739274, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8925330638885498, Accuracy = 0.7841584086418152\n",
      "Iter #711840:  Learning rate = 0.003757:   Batch Loss = 0.738801, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8881455659866333, Accuracy = 0.7861385941505432\n",
      "Iter #712320:  Learning rate = 0.003757:   Batch Loss = 0.862860, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8862297534942627, Accuracy = 0.7782177925109863\n",
      "Iter #712800:  Learning rate = 0.003757:   Batch Loss = 0.805849, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9100516438484192, Accuracy = 0.7623762488365173\n",
      "Iter #713280:  Learning rate = 0.003757:   Batch Loss = 0.797190, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8725501298904419, Accuracy = 0.788118839263916\n",
      "Iter #713760:  Learning rate = 0.003757:   Batch Loss = 0.780146, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.903764009475708, Accuracy = 0.7792079448699951\n",
      "Iter #714240:  Learning rate = 0.003757:   Batch Loss = 0.675263, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8895437121391296, Accuracy = 0.7772276997566223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #714720:  Learning rate = 0.003757:   Batch Loss = 0.746400, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9288188219070435, Accuracy = 0.7524752616882324\n",
      "Iter #715200:  Learning rate = 0.003757:   Batch Loss = 0.768654, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.91189044713974, Accuracy = 0.7683168053627014\n",
      "Iter #715680:  Learning rate = 0.003757:   Batch Loss = 0.847931, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.926292896270752, Accuracy = 0.7504950761795044\n",
      "Iter #716160:  Learning rate = 0.003757:   Batch Loss = 1.027952, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9123813509941101, Accuracy = 0.7673267126083374\n",
      "Iter #716640:  Learning rate = 0.003757:   Batch Loss = 0.842028, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9133137464523315, Accuracy = 0.7564356327056885\n",
      "Iter #717120:  Learning rate = 0.003757:   Batch Loss = 0.825012, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9045654535293579, Accuracy = 0.7752475142478943\n",
      "Iter #717600:  Learning rate = 0.003757:   Batch Loss = 0.981047, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9177597165107727, Accuracy = 0.7544554471969604\n",
      "Iter #718080:  Learning rate = 0.003757:   Batch Loss = 0.772694, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9187589287757874, Accuracy = 0.7613861560821533\n",
      "Iter #718560:  Learning rate = 0.003757:   Batch Loss = 0.775882, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9046136140823364, Accuracy = 0.7782177925109863\n",
      "Iter #719040:  Learning rate = 0.003757:   Batch Loss = 0.867662, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.914425253868103, Accuracy = 0.7782177925109863\n",
      "Iter #719520:  Learning rate = 0.003757:   Batch Loss = 0.758780, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9013310670852661, Accuracy = 0.7742574214935303\n",
      "Iter #720000:  Learning rate = 0.003757:   Batch Loss = 0.718248, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8912885189056396, Accuracy = 0.7742574214935303\n",
      "Iter #720480:  Learning rate = 0.003757:   Batch Loss = 0.891898, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8968847990036011, Accuracy = 0.7712871432304382\n",
      "Iter #720960:  Learning rate = 0.003757:   Batch Loss = 0.698340, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8997377157211304, Accuracy = 0.7693069577217102\n",
      "Iter #721440:  Learning rate = 0.003757:   Batch Loss = 0.842107, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8992514610290527, Accuracy = 0.7732673287391663\n",
      "Iter #721920:  Learning rate = 0.003757:   Batch Loss = 0.791660, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9066975712776184, Accuracy = 0.7702970504760742\n",
      "Iter #722400:  Learning rate = 0.003757:   Batch Loss = 0.757550, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8902654647827148, Accuracy = 0.7841584086418152\n",
      "Iter #722880:  Learning rate = 0.003757:   Batch Loss = 0.863296, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9044114947319031, Accuracy = 0.7851485013961792\n",
      "Iter #723360:  Learning rate = 0.003757:   Batch Loss = 0.809217, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8967370390892029, Accuracy = 0.7762376070022583\n",
      "Iter #723840:  Learning rate = 0.003757:   Batch Loss = 0.601523, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9419912099838257, Accuracy = 0.7405940890312195\n",
      "Iter #724320:  Learning rate = 0.003757:   Batch Loss = 0.587873, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9268504977226257, Accuracy = 0.7683168053627014\n",
      "Iter #724800:  Learning rate = 0.003757:   Batch Loss = 0.764734, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.947284996509552, Accuracy = 0.7564356327056885\n",
      "Iter #725280:  Learning rate = 0.003757:   Batch Loss = 0.828588, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9035861492156982, Accuracy = 0.7752475142478943\n",
      "Iter #725760:  Learning rate = 0.003757:   Batch Loss = 0.569540, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9168988466262817, Accuracy = 0.7653465270996094\n",
      "Iter #726240:  Learning rate = 0.003757:   Batch Loss = 0.688051, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9197930693626404, Accuracy = 0.7633663415908813\n",
      "Iter #726720:  Learning rate = 0.003757:   Batch Loss = 0.929190, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9218800067901611, Accuracy = 0.7594059109687805\n",
      "Iter #727200:  Learning rate = 0.003757:   Batch Loss = 0.716776, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8962215781211853, Accuracy = 0.7722772359848022\n",
      "Iter #727680:  Learning rate = 0.003757:   Batch Loss = 0.781286, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8991527557373047, Accuracy = 0.7841584086418152\n",
      "Iter #728160:  Learning rate = 0.003757:   Batch Loss = 0.620193, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.900155782699585, Accuracy = 0.7693069577217102\n",
      "Iter #728640:  Learning rate = 0.003757:   Batch Loss = 0.772065, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9027292728424072, Accuracy = 0.7653465270996094\n",
      "Iter #729120:  Learning rate = 0.003757:   Batch Loss = 0.653501, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9041170477867126, Accuracy = 0.7633663415908813\n",
      "Iter #729600:  Learning rate = 0.003757:   Batch Loss = 0.685925, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9012171626091003, Accuracy = 0.7702970504760742\n",
      "Iter #730080:  Learning rate = 0.003757:   Batch Loss = 0.765811, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9067471027374268, Accuracy = 0.7663366198539734\n",
      "Iter #730560:  Learning rate = 0.003757:   Batch Loss = 0.813761, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9032682180404663, Accuracy = 0.7693069577217102\n",
      "Iter #731040:  Learning rate = 0.003757:   Batch Loss = 0.893781, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.912019670009613, Accuracy = 0.7782177925109863\n",
      "Iter #731520:  Learning rate = 0.003757:   Batch Loss = 0.824347, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9159870743751526, Accuracy = 0.7742574214935303\n",
      "Iter #732000:  Learning rate = 0.003757:   Batch Loss = 0.635472, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8899277448654175, Accuracy = 0.7752475142478943\n",
      "Iter #732480:  Learning rate = 0.003757:   Batch Loss = 0.832382, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9126580953598022, Accuracy = 0.7742574214935303\n",
      "Iter #732960:  Learning rate = 0.003757:   Batch Loss = 0.802126, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.898868203163147, Accuracy = 0.7831683158874512\n",
      "Iter #733440:  Learning rate = 0.003757:   Batch Loss = 0.848852, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9023589491844177, Accuracy = 0.7742574214935303\n",
      "Iter #733920:  Learning rate = 0.003757:   Batch Loss = 0.684020, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.921217143535614, Accuracy = 0.7643564343452454\n",
      "Iter #734400:  Learning rate = 0.003757:   Batch Loss = 0.844686, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9176061153411865, Accuracy = 0.7772276997566223\n",
      "Iter #734880:  Learning rate = 0.003757:   Batch Loss = 0.614335, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9070422649383545, Accuracy = 0.7683168053627014\n",
      "Iter #735360:  Learning rate = 0.003757:   Batch Loss = 0.642857, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.924240231513977, Accuracy = 0.7673267126083374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #735840:  Learning rate = 0.003757:   Batch Loss = 0.730737, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8796144723892212, Accuracy = 0.7722772359848022\n",
      "Iter #736320:  Learning rate = 0.003757:   Batch Loss = 0.869610, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8852652311325073, Accuracy = 0.7792079448699951\n",
      "Iter #736800:  Learning rate = 0.003757:   Batch Loss = 0.674930, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8808825612068176, Accuracy = 0.7801980376243591\n",
      "Iter #737280:  Learning rate = 0.003757:   Batch Loss = 0.612974, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8925408124923706, Accuracy = 0.7683168053627014\n",
      "Iter #737760:  Learning rate = 0.003757:   Batch Loss = 0.872575, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9028956890106201, Accuracy = 0.7722772359848022\n",
      "Iter #738240:  Learning rate = 0.003757:   Batch Loss = 0.646349, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8764011859893799, Accuracy = 0.7861385941505432\n",
      "Iter #738720:  Learning rate = 0.003757:   Batch Loss = 0.699395, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.933367908000946, Accuracy = 0.7623762488365173\n",
      "Iter #739200:  Learning rate = 0.003757:   Batch Loss = 0.842464, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9643315076828003, Accuracy = 0.7435643672943115\n",
      "Iter #739680:  Learning rate = 0.003757:   Batch Loss = 0.813234, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9309858679771423, Accuracy = 0.7623762488365173\n",
      "Iter #740160:  Learning rate = 0.003757:   Batch Loss = 0.827534, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9102749228477478, Accuracy = 0.7742574214935303\n",
      "Iter #740640:  Learning rate = 0.003757:   Batch Loss = 0.672371, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9134359359741211, Accuracy = 0.7782177925109863\n",
      "Iter #741120:  Learning rate = 0.003757:   Batch Loss = 0.725273, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9254775047302246, Accuracy = 0.7722772359848022\n",
      "Iter #741600:  Learning rate = 0.003757:   Batch Loss = 0.800079, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9218466281890869, Accuracy = 0.7722772359848022\n",
      "Iter #742080:  Learning rate = 0.003757:   Batch Loss = 0.666471, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0081194639205933, Accuracy = 0.7603960633277893\n",
      "Iter #742560:  Learning rate = 0.003757:   Batch Loss = 0.913778, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9657425880432129, Accuracy = 0.7633663415908813\n",
      "Iter #743040:  Learning rate = 0.003757:   Batch Loss = 0.766751, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8825485110282898, Accuracy = 0.7871286869049072\n",
      "Iter #743520:  Learning rate = 0.003757:   Batch Loss = 0.956963, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9034510850906372, Accuracy = 0.7742574214935303\n",
      "Iter #744000:  Learning rate = 0.003757:   Batch Loss = 0.565607, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8744345903396606, Accuracy = 0.7841584086418152\n",
      "Iter #744480:  Learning rate = 0.003757:   Batch Loss = 0.720838, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8550046682357788, Accuracy = 0.7861385941505432\n",
      "Iter #744960:  Learning rate = 0.003757:   Batch Loss = 0.608998, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8785154223442078, Accuracy = 0.7861385941505432\n",
      "Iter #745440:  Learning rate = 0.003757:   Batch Loss = 0.816007, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9229210615158081, Accuracy = 0.7752475142478943\n",
      "Iter #745920:  Learning rate = 0.003757:   Batch Loss = 0.902472, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8984395861625671, Accuracy = 0.7772276997566223\n",
      "Iter #746400:  Learning rate = 0.003757:   Batch Loss = 0.665742, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9027485847473145, Accuracy = 0.7841584086418152\n",
      "Iter #746880:  Learning rate = 0.003757:   Batch Loss = 0.870535, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9007314443588257, Accuracy = 0.7712871432304382\n",
      "Iter #747360:  Learning rate = 0.003757:   Batch Loss = 0.907781, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.892017126083374, Accuracy = 0.7831683158874512\n",
      "Iter #747840:  Learning rate = 0.003757:   Batch Loss = 0.543865, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8880065679550171, Accuracy = 0.788118839263916\n",
      "Iter #748320:  Learning rate = 0.003757:   Batch Loss = 0.777299, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8838174343109131, Accuracy = 0.7831683158874512\n",
      "Iter #748800:  Learning rate = 0.003757:   Batch Loss = 0.729500, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8768597841262817, Accuracy = 0.7861385941505432\n",
      "Iter #749280:  Learning rate = 0.003757:   Batch Loss = 0.634476, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8748873472213745, Accuracy = 0.7821782231330872\n",
      "Iter #749760:  Learning rate = 0.003757:   Batch Loss = 0.718795, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8673306703567505, Accuracy = 0.7920792102813721\n",
      "Iter #750240:  Learning rate = 0.003757:   Batch Loss = 0.847616, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8987656235694885, Accuracy = 0.7762376070022583\n",
      "Iter #750720:  Learning rate = 0.003757:   Batch Loss = 0.734292, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8997681140899658, Accuracy = 0.7643564343452454\n",
      "Iter #751200:  Learning rate = 0.003757:   Batch Loss = 0.818927, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9112948179244995, Accuracy = 0.7772276997566223\n",
      "Iter #751680:  Learning rate = 0.003757:   Batch Loss = 0.593560, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9039753675460815, Accuracy = 0.7712871432304382\n",
      "Iter #752160:  Learning rate = 0.003757:   Batch Loss = 0.687605, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9119434952735901, Accuracy = 0.7584158182144165\n",
      "Iter #752640:  Learning rate = 0.003757:   Batch Loss = 0.884789, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0011461973190308, Accuracy = 0.7257425785064697\n",
      "Iter #753120:  Learning rate = 0.003757:   Batch Loss = 1.076872, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9368348121643066, Accuracy = 0.7663366198539734\n",
      "Iter #753600:  Learning rate = 0.003757:   Batch Loss = 0.868147, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9420282244682312, Accuracy = 0.7445544600486755\n",
      "Iter #754080:  Learning rate = 0.003757:   Batch Loss = 0.896868, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.913303554058075, Accuracy = 0.7752475142478943\n",
      "Iter #754560:  Learning rate = 0.003757:   Batch Loss = 0.852905, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9040238261222839, Accuracy = 0.7613861560821533\n",
      "Iter #755040:  Learning rate = 0.003757:   Batch Loss = 0.672688, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9043387770652771, Accuracy = 0.7594059109687805\n",
      "Iter #755520:  Learning rate = 0.003757:   Batch Loss = 0.829213, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9046194553375244, Accuracy = 0.7693069577217102\n",
      "Iter #756000:  Learning rate = 0.003757:   Batch Loss = 0.669438, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8950382471084595, Accuracy = 0.7772276997566223\n",
      "Iter #756480:  Learning rate = 0.003757:   Batch Loss = 0.804916, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8898157477378845, Accuracy = 0.7821782231330872\n",
      "Iter #756960:  Learning rate = 0.003757:   Batch Loss = 0.729664, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9044464230537415, Accuracy = 0.7732673287391663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #757440:  Learning rate = 0.003757:   Batch Loss = 0.663444, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9103919267654419, Accuracy = 0.7762376070022583\n",
      "Iter #757920:  Learning rate = 0.003757:   Batch Loss = 0.714902, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9340158700942993, Accuracy = 0.7455445528030396\n",
      "Iter #758400:  Learning rate = 0.003757:   Batch Loss = 0.804296, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8960227966308594, Accuracy = 0.7772276997566223\n",
      "Iter #758880:  Learning rate = 0.003757:   Batch Loss = 0.684717, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8863461017608643, Accuracy = 0.7831683158874512\n",
      "Iter #759360:  Learning rate = 0.003757:   Batch Loss = 0.634064, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8797285556793213, Accuracy = 0.7851485013961792\n",
      "Iter #759840:  Learning rate = 0.003757:   Batch Loss = 0.693176, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8686769604682922, Accuracy = 0.7851485013961792\n",
      "Iter #760320:  Learning rate = 0.003757:   Batch Loss = 0.764594, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8707103729248047, Accuracy = 0.78910893201828\n",
      "Iter #760800:  Learning rate = 0.003757:   Batch Loss = 0.712082, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.880801796913147, Accuracy = 0.7841584086418152\n",
      "Iter #761280:  Learning rate = 0.003757:   Batch Loss = 0.812169, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8999843001365662, Accuracy = 0.7732673287391663\n",
      "Iter #761760:  Learning rate = 0.003757:   Batch Loss = 0.715359, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8702772259712219, Accuracy = 0.7792079448699951\n",
      "Iter #762240:  Learning rate = 0.003757:   Batch Loss = 0.672258, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8935500383377075, Accuracy = 0.7762376070022583\n",
      "Iter #762720:  Learning rate = 0.003757:   Batch Loss = 0.642913, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9070789813995361, Accuracy = 0.7722772359848022\n",
      "Iter #763200:  Learning rate = 0.003757:   Batch Loss = 0.689860, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8953414559364319, Accuracy = 0.7762376070022583\n",
      "Iter #763680:  Learning rate = 0.003757:   Batch Loss = 0.701163, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8799154758453369, Accuracy = 0.7841584086418152\n",
      "Iter #764160:  Learning rate = 0.003757:   Batch Loss = 0.850314, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8932288289070129, Accuracy = 0.7772276997566223\n",
      "Iter #764640:  Learning rate = 0.003757:   Batch Loss = 0.791415, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.90172278881073, Accuracy = 0.7722772359848022\n",
      "Iter #765120:  Learning rate = 0.003757:   Batch Loss = 0.706437, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9207473993301392, Accuracy = 0.7594059109687805\n",
      "Iter #765600:  Learning rate = 0.003757:   Batch Loss = 0.782672, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9065865874290466, Accuracy = 0.7732673287391663\n",
      "Iter #766080:  Learning rate = 0.003757:   Batch Loss = 0.614062, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8879669904708862, Accuracy = 0.7702970504760742\n",
      "Iter #766560:  Learning rate = 0.003757:   Batch Loss = 0.760571, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9143882393836975, Accuracy = 0.7643564343452454\n",
      "Iter #767040:  Learning rate = 0.003757:   Batch Loss = 0.615689, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.933659553527832, Accuracy = 0.7554455399513245\n",
      "Iter #767520:  Learning rate = 0.003757:   Batch Loss = 0.832186, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9011991620063782, Accuracy = 0.7752475142478943\n",
      "Iter #768000:  Learning rate = 0.003757:   Batch Loss = 0.654493, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9219174981117249, Accuracy = 0.7643564343452454\n",
      "Iter #768480:  Learning rate = 0.003757:   Batch Loss = 0.849437, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9252809882164001, Accuracy = 0.7584158182144165\n",
      "Iter #768960:  Learning rate = 0.003757:   Batch Loss = 0.726866, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9199151396751404, Accuracy = 0.7603960633277893\n",
      "Iter #769440:  Learning rate = 0.003757:   Batch Loss = 0.813355, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9114550352096558, Accuracy = 0.7732673287391663\n",
      "Iter #769920:  Learning rate = 0.003757:   Batch Loss = 0.738448, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9358628988265991, Accuracy = 0.7534653544425964\n",
      "Iter #770400:  Learning rate = 0.003757:   Batch Loss = 0.917052, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9180541038513184, Accuracy = 0.7544554471969604\n",
      "Iter #770880:  Learning rate = 0.003757:   Batch Loss = 0.689576, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9400393962860107, Accuracy = 0.7584158182144165\n",
      "Iter #771360:  Learning rate = 0.003757:   Batch Loss = 0.781701, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8944994211196899, Accuracy = 0.7851485013961792\n",
      "Iter #771840:  Learning rate = 0.003757:   Batch Loss = 0.745082, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9237287044525146, Accuracy = 0.7762376070022583\n",
      "Iter #772320:  Learning rate = 0.003757:   Batch Loss = 0.631038, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.906259298324585, Accuracy = 0.7712871432304382\n",
      "Iter #772800:  Learning rate = 0.003757:   Batch Loss = 0.576282, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8984441757202148, Accuracy = 0.7772276997566223\n",
      "Iter #773280:  Learning rate = 0.003757:   Batch Loss = 0.807670, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8992076516151428, Accuracy = 0.7772276997566223\n",
      "Iter #773760:  Learning rate = 0.003757:   Batch Loss = 0.721128, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9287096858024597, Accuracy = 0.7643564343452454\n",
      "Iter #774240:  Learning rate = 0.003757:   Batch Loss = 0.736388, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8929162621498108, Accuracy = 0.7821782231330872\n",
      "Iter #774720:  Learning rate = 0.003757:   Batch Loss = 0.735403, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8954795598983765, Accuracy = 0.7732673287391663\n",
      "Iter #775200:  Learning rate = 0.003757:   Batch Loss = 0.588192, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8963769674301147, Accuracy = 0.7782177925109863\n",
      "Iter #775680:  Learning rate = 0.003757:   Batch Loss = 0.752525, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9148939847946167, Accuracy = 0.7762376070022583\n",
      "Iter #776160:  Learning rate = 0.003757:   Batch Loss = 0.691375, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8961825370788574, Accuracy = 0.7762376070022583\n",
      "Iter #776640:  Learning rate = 0.003757:   Batch Loss = 0.730588, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8850361704826355, Accuracy = 0.7920792102813721\n",
      "Iter #777120:  Learning rate = 0.003757:   Batch Loss = 0.731002, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9146687984466553, Accuracy = 0.7722772359848022\n",
      "Iter #777600:  Learning rate = 0.003757:   Batch Loss = 0.527402, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9119042158126831, Accuracy = 0.7623762488365173\n",
      "Iter #778080:  Learning rate = 0.003757:   Batch Loss = 0.730797, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9034969210624695, Accuracy = 0.7683168053627014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #778560:  Learning rate = 0.003757:   Batch Loss = 0.653136, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9070547819137573, Accuracy = 0.7792079448699951\n",
      "Iter #779040:  Learning rate = 0.003757:   Batch Loss = 0.838682, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9186570644378662, Accuracy = 0.7623762488365173\n",
      "Iter #779520:  Learning rate = 0.003757:   Batch Loss = 0.700968, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9162499308586121, Accuracy = 0.7782177925109863\n",
      "Iter #780000:  Learning rate = 0.003757:   Batch Loss = 0.622056, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8947420716285706, Accuracy = 0.7772276997566223\n",
      "Iter #780480:  Learning rate = 0.003757:   Batch Loss = 0.647928, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9011402130126953, Accuracy = 0.7762376070022583\n",
      "Iter #780960:  Learning rate = 0.003757:   Batch Loss = 0.582135, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9010754227638245, Accuracy = 0.7712871432304382\n",
      "Iter #781440:  Learning rate = 0.003757:   Batch Loss = 0.635234, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8838040828704834, Accuracy = 0.7960395812988281\n",
      "Iter #781920:  Learning rate = 0.003757:   Batch Loss = 0.514627, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8896204233169556, Accuracy = 0.7801980376243591\n",
      "Iter #782400:  Learning rate = 0.003757:   Batch Loss = 0.527560, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8940188884735107, Accuracy = 0.7871286869049072\n",
      "Iter #782880:  Learning rate = 0.003757:   Batch Loss = 0.666396, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9037656188011169, Accuracy = 0.7811881303787231\n",
      "Iter #783360:  Learning rate = 0.003757:   Batch Loss = 0.789928, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9352623820304871, Accuracy = 0.7574257254600525\n",
      "Iter #783840:  Learning rate = 0.003757:   Batch Loss = 0.801919, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8880000710487366, Accuracy = 0.7811881303787231\n",
      "Iter #784320:  Learning rate = 0.003757:   Batch Loss = 0.607199, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8829928636550903, Accuracy = 0.7831683158874512\n",
      "Iter #784800:  Learning rate = 0.003757:   Batch Loss = 0.753992, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8770807981491089, Accuracy = 0.7930693030357361\n",
      "Iter #785280:  Learning rate = 0.003757:   Batch Loss = 0.645214, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9015740156173706, Accuracy = 0.7554455399513245\n",
      "Iter #785760:  Learning rate = 0.003757:   Batch Loss = 0.724503, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8796160817146301, Accuracy = 0.7871286869049072\n",
      "Iter #786240:  Learning rate = 0.003757:   Batch Loss = 0.659079, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8740848302841187, Accuracy = 0.7851485013961792\n",
      "Iter #786720:  Learning rate = 0.003757:   Batch Loss = 0.792985, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8922741413116455, Accuracy = 0.7782177925109863\n",
      "Iter #787200:  Learning rate = 0.003757:   Batch Loss = 0.676470, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8642873764038086, Accuracy = 0.788118839263916\n",
      "Iter #787680:  Learning rate = 0.003757:   Batch Loss = 0.597589, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9031462669372559, Accuracy = 0.7841584086418152\n",
      "Iter #788160:  Learning rate = 0.003757:   Batch Loss = 0.669503, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.877856433391571, Accuracy = 0.7930693030357361\n",
      "Iter #788640:  Learning rate = 0.003757:   Batch Loss = 0.572802, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.887723445892334, Accuracy = 0.7732673287391663\n",
      "Iter #789120:  Learning rate = 0.003757:   Batch Loss = 0.544726, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8749694228172302, Accuracy = 0.78910893201828\n",
      "Iter #789600:  Learning rate = 0.003757:   Batch Loss = 0.800730, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8538013696670532, Accuracy = 0.803960382938385\n",
      "Iter #790080:  Learning rate = 0.003757:   Batch Loss = 0.900771, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8677881956100464, Accuracy = 0.7980198264122009\n",
      "Iter #790560:  Learning rate = 0.003757:   Batch Loss = 0.667779, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8661214113235474, Accuracy = 0.78910893201828\n",
      "Iter #791040:  Learning rate = 0.003757:   Batch Loss = 0.713071, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8658151626586914, Accuracy = 0.790099024772644\n",
      "Iter #791520:  Learning rate = 0.003757:   Batch Loss = 0.701957, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.866341233253479, Accuracy = 0.7861385941505432\n",
      "Iter #792000:  Learning rate = 0.003757:   Batch Loss = 0.543062, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8989447951316833, Accuracy = 0.7801980376243591\n",
      "Iter #792480:  Learning rate = 0.003757:   Batch Loss = 0.668931, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8656284809112549, Accuracy = 0.7861385941505432\n",
      "Iter #792960:  Learning rate = 0.003757:   Batch Loss = 0.751394, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8729575276374817, Accuracy = 0.7950494885444641\n",
      "Iter #793440:  Learning rate = 0.003757:   Batch Loss = 0.558362, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9246997237205505, Accuracy = 0.7693069577217102\n",
      "Iter #793920:  Learning rate = 0.003757:   Batch Loss = 0.634265, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9074451923370361, Accuracy = 0.7821782231330872\n",
      "Iter #794400:  Learning rate = 0.003757:   Batch Loss = 0.688570, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8875886797904968, Accuracy = 0.7782177925109863\n",
      "Iter #794880:  Learning rate = 0.003757:   Batch Loss = 0.631354, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9100162386894226, Accuracy = 0.7762376070022583\n",
      "Iter #795360:  Learning rate = 0.003757:   Batch Loss = 0.730567, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8928768038749695, Accuracy = 0.7930693030357361\n",
      "Iter #795840:  Learning rate = 0.003757:   Batch Loss = 0.740473, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8988388776779175, Accuracy = 0.7762376070022583\n",
      "Iter #796320:  Learning rate = 0.003757:   Batch Loss = 0.621521, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8938358426094055, Accuracy = 0.7782177925109863\n",
      "Iter #796800:  Learning rate = 0.003757:   Batch Loss = 0.628926, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9114699959754944, Accuracy = 0.7712871432304382\n",
      "Iter #797280:  Learning rate = 0.003757:   Batch Loss = 0.773322, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8836613297462463, Accuracy = 0.7801980376243591\n",
      "Iter #797760:  Learning rate = 0.003757:   Batch Loss = 0.487415, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8843377232551575, Accuracy = 0.78910893201828\n",
      "Iter #798240:  Learning rate = 0.003757:   Batch Loss = 0.853968, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8901500701904297, Accuracy = 0.7871286869049072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #798720:  Learning rate = 0.003757:   Batch Loss = 0.612880, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8997642993927002, Accuracy = 0.7752475142478943\n",
      "Iter #799200:  Learning rate = 0.003757:   Batch Loss = 0.576698, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9131243228912354, Accuracy = 0.7811881303787231\n",
      "Iter #799680:  Learning rate = 0.003757:   Batch Loss = 0.604139, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8909115195274353, Accuracy = 0.7841584086418152\n",
      "Iter #800160:  Learning rate = 0.003607:   Batch Loss = 0.794725, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8822252750396729, Accuracy = 0.7821782231330872\n",
      "Iter #800640:  Learning rate = 0.003607:   Batch Loss = 0.665295, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8828980922698975, Accuracy = 0.7801980376243591\n",
      "Iter #801120:  Learning rate = 0.003607:   Batch Loss = 0.788317, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8931475877761841, Accuracy = 0.7841584086418152\n",
      "Iter #801600:  Learning rate = 0.003607:   Batch Loss = 0.789897, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8698973655700684, Accuracy = 0.7920792102813721\n",
      "Iter #802080:  Learning rate = 0.003607:   Batch Loss = 0.505986, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.870375394821167, Accuracy = 0.800990104675293\n",
      "Iter #802560:  Learning rate = 0.003607:   Batch Loss = 0.679648, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8806720972061157, Accuracy = 0.7910891175270081\n",
      "Iter #803040:  Learning rate = 0.003607:   Batch Loss = 0.994898, Accuracy = 0.6833333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8805221319198608, Accuracy = 0.7871286869049072\n",
      "Iter #803520:  Learning rate = 0.003607:   Batch Loss = 0.548169, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8916746377944946, Accuracy = 0.7762376070022583\n",
      "Iter #804000:  Learning rate = 0.003607:   Batch Loss = 0.563396, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8940644264221191, Accuracy = 0.7801980376243591\n",
      "Iter #804480:  Learning rate = 0.003607:   Batch Loss = 0.557281, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.900738000869751, Accuracy = 0.7792079448699951\n",
      "Iter #804960:  Learning rate = 0.003607:   Batch Loss = 0.682711, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8808313608169556, Accuracy = 0.7910891175270081\n",
      "Iter #805440:  Learning rate = 0.003607:   Batch Loss = 0.563897, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8950710892677307, Accuracy = 0.7851485013961792\n",
      "Iter #805920:  Learning rate = 0.003607:   Batch Loss = 0.725447, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9151561856269836, Accuracy = 0.7821782231330872\n",
      "Iter #806400:  Learning rate = 0.003607:   Batch Loss = 0.755062, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9149414300918579, Accuracy = 0.7732673287391663\n",
      "Iter #806880:  Learning rate = 0.003607:   Batch Loss = 0.656170, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9071782827377319, Accuracy = 0.7752475142478943\n",
      "Iter #807360:  Learning rate = 0.003607:   Batch Loss = 0.835953, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.876206636428833, Accuracy = 0.7742574214935303\n",
      "Iter #807840:  Learning rate = 0.003607:   Batch Loss = 0.737908, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8590049743652344, Accuracy = 0.7990099191665649\n",
      "Iter #808320:  Learning rate = 0.003607:   Batch Loss = 0.601392, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8685198426246643, Accuracy = 0.7950494885444641\n",
      "Iter #808800:  Learning rate = 0.003607:   Batch Loss = 0.674243, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8793672919273376, Accuracy = 0.7811881303787231\n",
      "Iter #809280:  Learning rate = 0.003607:   Batch Loss = 0.724383, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8666890263557434, Accuracy = 0.7930693030357361\n",
      "Iter #809760:  Learning rate = 0.003607:   Batch Loss = 0.557423, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8952119946479797, Accuracy = 0.7792079448699951\n",
      "Iter #810240:  Learning rate = 0.003607:   Batch Loss = 0.547429, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8917186260223389, Accuracy = 0.7732673287391663\n",
      "Iter #810720:  Learning rate = 0.003607:   Batch Loss = 0.700296, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.893831193447113, Accuracy = 0.7752475142478943\n",
      "Iter #811200:  Learning rate = 0.003607:   Batch Loss = 0.635701, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8765130043029785, Accuracy = 0.7841584086418152\n",
      "Iter #811680:  Learning rate = 0.003607:   Batch Loss = 0.631283, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8552077412605286, Accuracy = 0.801980197429657\n",
      "Iter #812160:  Learning rate = 0.003607:   Batch Loss = 0.630231, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8676983118057251, Accuracy = 0.7821782231330872\n",
      "Iter #812640:  Learning rate = 0.003607:   Batch Loss = 0.841966, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8537451028823853, Accuracy = 0.7910891175270081\n",
      "Iter #813120:  Learning rate = 0.003607:   Batch Loss = 0.676258, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8572093844413757, Accuracy = 0.7950494885444641\n",
      "Iter #813600:  Learning rate = 0.003607:   Batch Loss = 0.792978, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.852250337600708, Accuracy = 0.801980197429657\n",
      "Iter #814080:  Learning rate = 0.003607:   Batch Loss = 0.627339, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.865308940410614, Accuracy = 0.7940593957901001\n",
      "Iter #814560:  Learning rate = 0.003607:   Batch Loss = 0.528991, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8554618954658508, Accuracy = 0.7940593957901001\n",
      "Iter #815040:  Learning rate = 0.003607:   Batch Loss = 0.517593, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8586243391036987, Accuracy = 0.7831683158874512\n",
      "Iter #815520:  Learning rate = 0.003607:   Batch Loss = 0.727194, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8549510836601257, Accuracy = 0.790099024772644\n",
      "Iter #816000:  Learning rate = 0.003607:   Batch Loss = 0.710135, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8908039331436157, Accuracy = 0.7792079448699951\n",
      "Iter #816480:  Learning rate = 0.003607:   Batch Loss = 0.694981, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9047973155975342, Accuracy = 0.7752475142478943\n",
      "Iter #816960:  Learning rate = 0.003607:   Batch Loss = 0.599118, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8936947584152222, Accuracy = 0.7732673287391663\n",
      "Iter #817440:  Learning rate = 0.003607:   Batch Loss = 0.577549, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8637679219245911, Accuracy = 0.78910893201828\n",
      "Iter #817920:  Learning rate = 0.003607:   Batch Loss = 0.703389, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8542306423187256, Accuracy = 0.7980198264122009\n",
      "Iter #818400:  Learning rate = 0.003607:   Batch Loss = 0.663926, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8796967267990112, Accuracy = 0.7910891175270081\n",
      "Iter #818880:  Learning rate = 0.003607:   Batch Loss = 0.689061, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8490290641784668, Accuracy = 0.7940593957901001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #819360:  Learning rate = 0.003607:   Batch Loss = 0.694214, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8567498326301575, Accuracy = 0.7990099191665649\n",
      "Iter #819840:  Learning rate = 0.003607:   Batch Loss = 0.669327, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8597581386566162, Accuracy = 0.788118839263916\n",
      "Iter #820320:  Learning rate = 0.003607:   Batch Loss = 0.615335, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8911970257759094, Accuracy = 0.7742574214935303\n",
      "Iter #820800:  Learning rate = 0.003607:   Batch Loss = 0.624844, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9027290344238281, Accuracy = 0.7801980376243591\n",
      "Iter #821280:  Learning rate = 0.003607:   Batch Loss = 0.714872, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8880010843276978, Accuracy = 0.7861385941505432\n",
      "Iter #821760:  Learning rate = 0.003607:   Batch Loss = 0.664065, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8780947923660278, Accuracy = 0.7980198264122009\n",
      "Iter #822240:  Learning rate = 0.003607:   Batch Loss = 0.609977, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8788728713989258, Accuracy = 0.7930693030357361\n",
      "Iter #822720:  Learning rate = 0.003607:   Batch Loss = 0.619234, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8757473230361938, Accuracy = 0.790099024772644\n",
      "Iter #823200:  Learning rate = 0.003607:   Batch Loss = 0.624990, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8624405860900879, Accuracy = 0.7970296740531921\n",
      "Iter #823680:  Learning rate = 0.003607:   Batch Loss = 0.623005, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8662865161895752, Accuracy = 0.801980197429657\n",
      "Iter #824160:  Learning rate = 0.003607:   Batch Loss = 0.738420, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.878052830696106, Accuracy = 0.7920792102813721\n",
      "Iter #824640:  Learning rate = 0.003607:   Batch Loss = 0.641408, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.904890775680542, Accuracy = 0.7742574214935303\n",
      "Iter #825120:  Learning rate = 0.003607:   Batch Loss = 0.712375, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9221824407577515, Accuracy = 0.7732673287391663\n",
      "Iter #825600:  Learning rate = 0.003607:   Batch Loss = 0.693499, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9029838442802429, Accuracy = 0.7831683158874512\n",
      "Iter #826080:  Learning rate = 0.003607:   Batch Loss = 0.493993, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8766473531723022, Accuracy = 0.790099024772644\n",
      "Iter #826560:  Learning rate = 0.003607:   Batch Loss = 0.688570, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9054030179977417, Accuracy = 0.7762376070022583\n",
      "Iter #827040:  Learning rate = 0.003607:   Batch Loss = 0.729644, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9004601836204529, Accuracy = 0.7752475142478943\n",
      "Iter #827520:  Learning rate = 0.003607:   Batch Loss = 0.618213, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8982312083244324, Accuracy = 0.788118839263916\n",
      "Iter #828000:  Learning rate = 0.003607:   Batch Loss = 0.618886, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9006810188293457, Accuracy = 0.7772276997566223\n",
      "Iter #828480:  Learning rate = 0.003607:   Batch Loss = 0.652913, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.878166913986206, Accuracy = 0.7920792102813721\n",
      "Iter #828960:  Learning rate = 0.003607:   Batch Loss = 0.738144, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8760164976119995, Accuracy = 0.7871286869049072\n",
      "Iter #829440:  Learning rate = 0.003607:   Batch Loss = 0.652454, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8743754029273987, Accuracy = 0.7752475142478943\n",
      "Iter #829920:  Learning rate = 0.003607:   Batch Loss = 0.846528, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0446321964263916, Accuracy = 0.7237623929977417\n",
      "Iter #830400:  Learning rate = 0.003607:   Batch Loss = 0.784170, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9461331963539124, Accuracy = 0.7693069577217102\n",
      "Iter #830880:  Learning rate = 0.003607:   Batch Loss = 0.718801, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9086737036705017, Accuracy = 0.7752475142478943\n",
      "Iter #831360:  Learning rate = 0.003607:   Batch Loss = 0.701527, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8737012147903442, Accuracy = 0.801980197429657\n",
      "Iter #831840:  Learning rate = 0.003607:   Batch Loss = 0.948857, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0913561582565308, Accuracy = 0.7287128567695618\n",
      "Iter #832320:  Learning rate = 0.003607:   Batch Loss = 0.751636, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9055068492889404, Accuracy = 0.7742574214935303\n",
      "Iter #832800:  Learning rate = 0.003607:   Batch Loss = 0.644325, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9068963527679443, Accuracy = 0.7712871432304382\n",
      "Iter #833280:  Learning rate = 0.003607:   Batch Loss = 0.785426, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9454647302627563, Accuracy = 0.7683168053627014\n",
      "Iter #833760:  Learning rate = 0.003607:   Batch Loss = 0.567466, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9302992820739746, Accuracy = 0.7762376070022583\n",
      "Iter #834240:  Learning rate = 0.003607:   Batch Loss = 0.563529, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8955440521240234, Accuracy = 0.7782177925109863\n",
      "Iter #834720:  Learning rate = 0.003607:   Batch Loss = 0.717469, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8793678283691406, Accuracy = 0.7732673287391663\n",
      "Iter #835200:  Learning rate = 0.003607:   Batch Loss = 0.579456, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8720799088478088, Accuracy = 0.7861385941505432\n",
      "Iter #835680:  Learning rate = 0.003607:   Batch Loss = 0.601065, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9089294672012329, Accuracy = 0.7821782231330872\n",
      "Iter #836160:  Learning rate = 0.003607:   Batch Loss = 0.911915, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9085099697113037, Accuracy = 0.7851485013961792\n",
      "Iter #836640:  Learning rate = 0.003607:   Batch Loss = 0.743902, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9277265071868896, Accuracy = 0.7564356327056885\n",
      "Iter #837120:  Learning rate = 0.003607:   Batch Loss = 0.793688, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9243416786193848, Accuracy = 0.7594059109687805\n",
      "Iter #837600:  Learning rate = 0.003607:   Batch Loss = 0.738386, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9341545104980469, Accuracy = 0.7574257254600525\n",
      "Iter #838080:  Learning rate = 0.003607:   Batch Loss = 0.610138, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8888168931007385, Accuracy = 0.7742574214935303\n",
      "Iter #838560:  Learning rate = 0.003607:   Batch Loss = 0.752934, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8851869106292725, Accuracy = 0.788118839263916\n",
      "Iter #839040:  Learning rate = 0.003607:   Batch Loss = 0.687718, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8727263808250427, Accuracy = 0.7930693030357361\n",
      "Iter #839520:  Learning rate = 0.003607:   Batch Loss = 0.737584, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8781505823135376, Accuracy = 0.7861385941505432\n",
      "Iter #840000:  Learning rate = 0.003607:   Batch Loss = 0.704386, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8811224102973938, Accuracy = 0.7792079448699951\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #840480:  Learning rate = 0.003607:   Batch Loss = 0.696866, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8781582713127136, Accuracy = 0.7811881303787231\n",
      "Iter #840960:  Learning rate = 0.003607:   Batch Loss = 0.757415, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9124671220779419, Accuracy = 0.7722772359848022\n",
      "Iter #841440:  Learning rate = 0.003607:   Batch Loss = 0.788197, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9686174988746643, Accuracy = 0.7495049238204956\n",
      "Iter #841920:  Learning rate = 0.003607:   Batch Loss = 0.666980, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9194424748420715, Accuracy = 0.7752475142478943\n",
      "Iter #842400:  Learning rate = 0.003607:   Batch Loss = 0.628188, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9285684823989868, Accuracy = 0.7465346455574036\n",
      "Iter #842880:  Learning rate = 0.003607:   Batch Loss = 0.503194, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9593742489814758, Accuracy = 0.7445544600486755\n",
      "Iter #843360:  Learning rate = 0.003607:   Batch Loss = 0.581588, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9227278232574463, Accuracy = 0.7722772359848022\n",
      "Iter #843840:  Learning rate = 0.003607:   Batch Loss = 0.734953, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9148966670036316, Accuracy = 0.7702970504760742\n",
      "Iter #844320:  Learning rate = 0.003607:   Batch Loss = 0.909062, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9695665836334229, Accuracy = 0.7475247383117676\n",
      "Iter #844800:  Learning rate = 0.003607:   Batch Loss = 0.572665, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.871687114238739, Accuracy = 0.7851485013961792\n",
      "Iter #845280:  Learning rate = 0.003607:   Batch Loss = 0.750990, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9415301084518433, Accuracy = 0.7564356327056885\n",
      "Iter #845760:  Learning rate = 0.003607:   Batch Loss = 0.589269, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9288053512573242, Accuracy = 0.7594059109687805\n",
      "Iter #846240:  Learning rate = 0.003607:   Batch Loss = 0.683854, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9431634545326233, Accuracy = 0.7663366198539734\n",
      "Iter #846720:  Learning rate = 0.003607:   Batch Loss = 0.736497, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0198906660079956, Accuracy = 0.7376237511634827\n",
      "Iter #847200:  Learning rate = 0.003607:   Batch Loss = 0.822577, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9418414235115051, Accuracy = 0.7405940890312195\n",
      "Iter #847680:  Learning rate = 0.003607:   Batch Loss = 0.760081, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9315229058265686, Accuracy = 0.7524752616882324\n",
      "Iter #848160:  Learning rate = 0.003607:   Batch Loss = 0.665384, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9414395689964294, Accuracy = 0.7534653544425964\n",
      "Iter #848640:  Learning rate = 0.003607:   Batch Loss = 0.785043, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9021866917610168, Accuracy = 0.7772276997566223\n",
      "Iter #849120:  Learning rate = 0.003607:   Batch Loss = 0.852331, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8978826999664307, Accuracy = 0.7752475142478943\n",
      "Iter #849600:  Learning rate = 0.003607:   Batch Loss = 0.885658, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8918646574020386, Accuracy = 0.7811881303787231\n",
      "Iter #850080:  Learning rate = 0.003607:   Batch Loss = 0.780619, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8632906079292297, Accuracy = 0.790099024772644\n",
      "Iter #850560:  Learning rate = 0.003607:   Batch Loss = 0.773684, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8913027048110962, Accuracy = 0.7801980376243591\n",
      "Iter #851040:  Learning rate = 0.003607:   Batch Loss = 0.749315, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8895032405853271, Accuracy = 0.7821782231330872\n",
      "Iter #851520:  Learning rate = 0.003607:   Batch Loss = 0.827769, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8838797807693481, Accuracy = 0.7950494885444641\n",
      "Iter #852000:  Learning rate = 0.003607:   Batch Loss = 0.718613, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8708942532539368, Accuracy = 0.788118839263916\n",
      "Iter #852480:  Learning rate = 0.003607:   Batch Loss = 0.509275, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8770961761474609, Accuracy = 0.7861385941505432\n",
      "Iter #852960:  Learning rate = 0.003607:   Batch Loss = 0.609634, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8968410491943359, Accuracy = 0.7861385941505432\n",
      "Iter #853440:  Learning rate = 0.003607:   Batch Loss = 0.572397, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.864690899848938, Accuracy = 0.802970290184021\n",
      "Iter #853920:  Learning rate = 0.003607:   Batch Loss = 0.582391, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8745002746582031, Accuracy = 0.7930693030357361\n",
      "Iter #854400:  Learning rate = 0.003607:   Batch Loss = 0.716922, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8920641541481018, Accuracy = 0.7841584086418152\n",
      "Iter #854880:  Learning rate = 0.003607:   Batch Loss = 0.611756, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9135220050811768, Accuracy = 0.7722772359848022\n",
      "Iter #855360:  Learning rate = 0.003607:   Batch Loss = 0.549205, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8919738531112671, Accuracy = 0.7801980376243591\n",
      "Iter #855840:  Learning rate = 0.003607:   Batch Loss = 0.620976, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8725612163543701, Accuracy = 0.7861385941505432\n",
      "Iter #856320:  Learning rate = 0.003607:   Batch Loss = 0.799716, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8831992745399475, Accuracy = 0.788118839263916\n",
      "Iter #856800:  Learning rate = 0.003607:   Batch Loss = 0.656489, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9039134383201599, Accuracy = 0.7831683158874512\n",
      "Iter #857280:  Learning rate = 0.003607:   Batch Loss = 0.818672, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8839775323867798, Accuracy = 0.7831683158874512\n",
      "Iter #857760:  Learning rate = 0.003607:   Batch Loss = 0.743946, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9197914004325867, Accuracy = 0.7732673287391663\n",
      "Iter #858240:  Learning rate = 0.003607:   Batch Loss = 0.583951, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9240567684173584, Accuracy = 0.7732673287391663\n",
      "Iter #858720:  Learning rate = 0.003607:   Batch Loss = 1.043166, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9367693066596985, Accuracy = 0.7564356327056885\n",
      "Iter #859200:  Learning rate = 0.003607:   Batch Loss = 0.696868, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9396907091140747, Accuracy = 0.7386138439178467\n",
      "Iter #859680:  Learning rate = 0.003607:   Batch Loss = 0.887326, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9347963929176331, Accuracy = 0.7633663415908813\n",
      "Iter #860160:  Learning rate = 0.003607:   Batch Loss = 0.771896, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8997257947921753, Accuracy = 0.7801980376243591\n",
      "Iter #860640:  Learning rate = 0.003607:   Batch Loss = 0.621740, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9007034301757812, Accuracy = 0.7732673287391663\n",
      "Iter #861120:  Learning rate = 0.003607:   Batch Loss = 0.633413, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9141283631324768, Accuracy = 0.7742574214935303\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #861600:  Learning rate = 0.003607:   Batch Loss = 0.644058, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8870500922203064, Accuracy = 0.7752475142478943\n",
      "Iter #862080:  Learning rate = 0.003607:   Batch Loss = 0.756041, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9016899466514587, Accuracy = 0.7712871432304382\n",
      "Iter #862560:  Learning rate = 0.003607:   Batch Loss = 0.643151, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9111102819442749, Accuracy = 0.7742574214935303\n",
      "Iter #863040:  Learning rate = 0.003607:   Batch Loss = 0.612436, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8876011967658997, Accuracy = 0.7851485013961792\n",
      "Iter #863520:  Learning rate = 0.003607:   Batch Loss = 0.696537, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9048870801925659, Accuracy = 0.7693069577217102\n",
      "Iter #864000:  Learning rate = 0.003607:   Batch Loss = 0.826868, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.881088137626648, Accuracy = 0.7980198264122009\n",
      "Iter #864480:  Learning rate = 0.003607:   Batch Loss = 0.832712, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9174058437347412, Accuracy = 0.7732673287391663\n",
      "Iter #864960:  Learning rate = 0.003607:   Batch Loss = 0.882681, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9380820393562317, Accuracy = 0.7712871432304382\n",
      "Iter #865440:  Learning rate = 0.003607:   Batch Loss = 0.839364, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9551537036895752, Accuracy = 0.7613861560821533\n",
      "Iter #865920:  Learning rate = 0.003607:   Batch Loss = 0.675565, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9478946924209595, Accuracy = 0.7673267126083374\n",
      "Iter #866400:  Learning rate = 0.003607:   Batch Loss = 0.750504, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0009912252426147, Accuracy = 0.7495049238204956\n",
      "Iter #866880:  Learning rate = 0.003607:   Batch Loss = 0.643244, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9491366147994995, Accuracy = 0.7544554471969604\n",
      "Iter #867360:  Learning rate = 0.003607:   Batch Loss = 0.854277, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9238362908363342, Accuracy = 0.7722772359848022\n",
      "Iter #867840:  Learning rate = 0.003607:   Batch Loss = 0.862575, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9062793254852295, Accuracy = 0.7950494885444641\n",
      "Iter #868320:  Learning rate = 0.003607:   Batch Loss = 0.915366, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9334972500801086, Accuracy = 0.7801980376243591\n",
      "Iter #868800:  Learning rate = 0.003607:   Batch Loss = 0.669837, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9111188054084778, Accuracy = 0.7772276997566223\n",
      "Iter #869280:  Learning rate = 0.003607:   Batch Loss = 0.742517, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9184756875038147, Accuracy = 0.7792079448699951\n",
      "Iter #869760:  Learning rate = 0.003607:   Batch Loss = 0.672337, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8723890781402588, Accuracy = 0.801980197429657\n",
      "Iter #870240:  Learning rate = 0.003607:   Batch Loss = 0.837206, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9001134634017944, Accuracy = 0.788118839263916\n",
      "Iter #870720:  Learning rate = 0.003607:   Batch Loss = 0.578771, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8997742533683777, Accuracy = 0.7801980376243591\n",
      "Iter #871200:  Learning rate = 0.003607:   Batch Loss = 0.801075, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8624081611633301, Accuracy = 0.7990099191665649\n",
      "Iter #871680:  Learning rate = 0.003607:   Batch Loss = 0.678698, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8624971508979797, Accuracy = 0.7980198264122009\n",
      "Iter #872160:  Learning rate = 0.003607:   Batch Loss = 0.757025, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8880077600479126, Accuracy = 0.7811881303787231\n",
      "Iter #872640:  Learning rate = 0.003607:   Batch Loss = 0.689381, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9210899472236633, Accuracy = 0.7732673287391663\n",
      "Iter #873120:  Learning rate = 0.003607:   Batch Loss = 0.608897, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9045236706733704, Accuracy = 0.7821782231330872\n",
      "Iter #873600:  Learning rate = 0.003607:   Batch Loss = 0.609451, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.903027355670929, Accuracy = 0.7861385941505432\n",
      "Iter #874080:  Learning rate = 0.003607:   Batch Loss = 0.505856, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8950690031051636, Accuracy = 0.7772276997566223\n",
      "Iter #874560:  Learning rate = 0.003607:   Batch Loss = 0.781480, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8871271014213562, Accuracy = 0.7782177925109863\n",
      "Iter #875040:  Learning rate = 0.003607:   Batch Loss = 0.697094, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9016304016113281, Accuracy = 0.7693069577217102\n",
      "Iter #875520:  Learning rate = 0.003607:   Batch Loss = 0.679636, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9466629028320312, Accuracy = 0.7455445528030396\n",
      "Iter #876000:  Learning rate = 0.003607:   Batch Loss = 0.759439, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.885942816734314, Accuracy = 0.7801980376243591\n",
      "Iter #876480:  Learning rate = 0.003607:   Batch Loss = 0.630859, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8703089356422424, Accuracy = 0.7762376070022583\n",
      "Iter #876960:  Learning rate = 0.003607:   Batch Loss = 0.693212, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8504696488380432, Accuracy = 0.7871286869049072\n",
      "Iter #877440:  Learning rate = 0.003607:   Batch Loss = 0.739831, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.862139105796814, Accuracy = 0.801980197429657\n",
      "Iter #877920:  Learning rate = 0.003607:   Batch Loss = 0.624213, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8764498233795166, Accuracy = 0.788118839263916\n",
      "Iter #878400:  Learning rate = 0.003607:   Batch Loss = 0.566892, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.869794487953186, Accuracy = 0.7980198264122009\n",
      "Iter #878880:  Learning rate = 0.003607:   Batch Loss = 0.642712, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.884942889213562, Accuracy = 0.7910891175270081\n",
      "Iter #879360:  Learning rate = 0.003607:   Batch Loss = 0.793650, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9096711277961731, Accuracy = 0.7722772359848022\n",
      "Iter #879840:  Learning rate = 0.003607:   Batch Loss = 0.568370, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9136197566986084, Accuracy = 0.7762376070022583\n",
      "Iter #880320:  Learning rate = 0.003607:   Batch Loss = 0.715830, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9269146919250488, Accuracy = 0.7712871432304382\n",
      "Iter #880800:  Learning rate = 0.003607:   Batch Loss = 0.648183, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8995932936668396, Accuracy = 0.7871286869049072\n",
      "Iter #881280:  Learning rate = 0.003607:   Batch Loss = 0.718067, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8889037370681763, Accuracy = 0.788118839263916\n",
      "Iter #881760:  Learning rate = 0.003607:   Batch Loss = 0.590775, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8716941475868225, Accuracy = 0.790099024772644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #882240:  Learning rate = 0.003607:   Batch Loss = 0.654386, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8659355640411377, Accuracy = 0.7821782231330872\n",
      "Iter #882720:  Learning rate = 0.003607:   Batch Loss = 0.560155, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9027307629585266, Accuracy = 0.7732673287391663\n",
      "Iter #883200:  Learning rate = 0.003607:   Batch Loss = 0.630487, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8690294027328491, Accuracy = 0.788118839263916\n",
      "Iter #883680:  Learning rate = 0.003607:   Batch Loss = 0.540200, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8664118647575378, Accuracy = 0.7940593957901001\n",
      "Iter #884160:  Learning rate = 0.003607:   Batch Loss = 0.634167, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8873710632324219, Accuracy = 0.7841584086418152\n",
      "Iter #884640:  Learning rate = 0.003607:   Batch Loss = 0.680804, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9091448783874512, Accuracy = 0.7762376070022583\n",
      "Iter #885120:  Learning rate = 0.003607:   Batch Loss = 0.825154, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8840308785438538, Accuracy = 0.78910893201828\n",
      "Iter #885600:  Learning rate = 0.003607:   Batch Loss = 0.529885, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9023759365081787, Accuracy = 0.7930693030357361\n",
      "Iter #886080:  Learning rate = 0.003607:   Batch Loss = 0.726478, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8566235303878784, Accuracy = 0.7970296740531921\n",
      "Iter #886560:  Learning rate = 0.003607:   Batch Loss = 0.584724, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8640981316566467, Accuracy = 0.7990099191665649\n",
      "Iter #887040:  Learning rate = 0.003607:   Batch Loss = 0.565626, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8831698298454285, Accuracy = 0.7811881303787231\n",
      "Iter #887520:  Learning rate = 0.003607:   Batch Loss = 0.657919, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9101343154907227, Accuracy = 0.7653465270996094\n",
      "Iter #888000:  Learning rate = 0.003607:   Batch Loss = 0.528953, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8878579139709473, Accuracy = 0.7851485013961792\n",
      "Iter #888480:  Learning rate = 0.003607:   Batch Loss = 0.563133, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9196272492408752, Accuracy = 0.7762376070022583\n",
      "Iter #888960:  Learning rate = 0.003607:   Batch Loss = 0.675836, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9197500348091125, Accuracy = 0.7603960633277893\n",
      "Iter #889440:  Learning rate = 0.003607:   Batch Loss = 0.704271, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9333751797676086, Accuracy = 0.7495049238204956\n",
      "Iter #889920:  Learning rate = 0.003607:   Batch Loss = 0.755999, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9396542310714722, Accuracy = 0.7613861560821533\n",
      "Iter #890400:  Learning rate = 0.003607:   Batch Loss = 0.755046, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9020280838012695, Accuracy = 0.7831683158874512\n",
      "Iter #890880:  Learning rate = 0.003607:   Batch Loss = 0.781102, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9257470369338989, Accuracy = 0.7712871432304382\n",
      "Iter #891360:  Learning rate = 0.003607:   Batch Loss = 0.570178, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9209251999855042, Accuracy = 0.7712871432304382\n",
      "Iter #891840:  Learning rate = 0.003607:   Batch Loss = 0.693362, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9048663973808289, Accuracy = 0.7732673287391663\n",
      "Iter #892320:  Learning rate = 0.003607:   Batch Loss = 0.669013, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8914708495140076, Accuracy = 0.7742574214935303\n",
      "Iter #892800:  Learning rate = 0.003607:   Batch Loss = 0.589321, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.938457727432251, Accuracy = 0.7594059109687805\n",
      "Iter #893280:  Learning rate = 0.003607:   Batch Loss = 0.828659, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9620789289474487, Accuracy = 0.7673267126083374\n",
      "Iter #893760:  Learning rate = 0.003607:   Batch Loss = 0.957561, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0459812879562378, Accuracy = 0.7405940890312195\n",
      "Iter #894240:  Learning rate = 0.003607:   Batch Loss = 0.635946, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9723072648048401, Accuracy = 0.7603960633277893\n",
      "Iter #894720:  Learning rate = 0.003607:   Batch Loss = 0.739912, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9713846445083618, Accuracy = 0.7425742745399475\n",
      "Iter #895200:  Learning rate = 0.003607:   Batch Loss = 0.821421, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9965507388114929, Accuracy = 0.7247524857521057\n",
      "Iter #895680:  Learning rate = 0.003607:   Batch Loss = 0.957122, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9763203859329224, Accuracy = 0.7207920551300049\n",
      "Iter #896160:  Learning rate = 0.003607:   Batch Loss = 0.780557, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9563369750976562, Accuracy = 0.7495049238204956\n",
      "Iter #896640:  Learning rate = 0.003607:   Batch Loss = 0.735393, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9504461288452148, Accuracy = 0.7544554471969604\n",
      "Iter #897120:  Learning rate = 0.003607:   Batch Loss = 0.698276, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9215220212936401, Accuracy = 0.7653465270996094\n",
      "Iter #897600:  Learning rate = 0.003607:   Batch Loss = 0.725671, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8997015953063965, Accuracy = 0.7653465270996094\n",
      "Iter #898080:  Learning rate = 0.003607:   Batch Loss = 0.693864, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.878054141998291, Accuracy = 0.7752475142478943\n",
      "Iter #898560:  Learning rate = 0.003607:   Batch Loss = 0.677372, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8940845727920532, Accuracy = 0.7702970504760742\n",
      "Iter #899040:  Learning rate = 0.003607:   Batch Loss = 0.917805, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9080507159233093, Accuracy = 0.7742574214935303\n",
      "Iter #899520:  Learning rate = 0.003607:   Batch Loss = 0.578241, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8916388154029846, Accuracy = 0.7722772359848022\n",
      "Iter #900000:  Learning rate = 0.003463:   Batch Loss = 0.662561, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9102859497070312, Accuracy = 0.7693069577217102\n",
      "Iter #900480:  Learning rate = 0.003463:   Batch Loss = 0.666197, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8700397610664368, Accuracy = 0.7841584086418152\n",
      "Iter #900960:  Learning rate = 0.003463:   Batch Loss = 0.814503, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8886564373970032, Accuracy = 0.7782177925109863\n",
      "Iter #901440:  Learning rate = 0.003463:   Batch Loss = 0.614452, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8831663727760315, Accuracy = 0.7732673287391663\n",
      "Iter #901920:  Learning rate = 0.003463:   Batch Loss = 0.651769, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8964990973472595, Accuracy = 0.7574257254600525\n",
      "Iter #902400:  Learning rate = 0.003463:   Batch Loss = 0.702953, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8832024335861206, Accuracy = 0.7782177925109863\n",
      "Iter #902880:  Learning rate = 0.003463:   Batch Loss = 0.734061, Accuracy = 0.7666666507720947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9093233942985535, Accuracy = 0.7742574214935303\n",
      "Iter #903360:  Learning rate = 0.003463:   Batch Loss = 0.669972, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9226819276809692, Accuracy = 0.7722772359848022\n",
      "Iter #903840:  Learning rate = 0.003463:   Batch Loss = 0.710961, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9569790959358215, Accuracy = 0.7534653544425964\n",
      "Iter #904320:  Learning rate = 0.003463:   Batch Loss = 0.865299, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9668017029762268, Accuracy = 0.7227723002433777\n",
      "Iter #904800:  Learning rate = 0.003463:   Batch Loss = 0.722629, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0154831409454346, Accuracy = 0.7158415913581848\n",
      "Iter #905280:  Learning rate = 0.003463:   Batch Loss = 0.724394, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9582459926605225, Accuracy = 0.7544554471969604\n",
      "Iter #905760:  Learning rate = 0.003463:   Batch Loss = 0.882465, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9725722670555115, Accuracy = 0.7415841817855835\n",
      "Iter #906240:  Learning rate = 0.003463:   Batch Loss = 0.622601, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9266571402549744, Accuracy = 0.7643564343452454\n",
      "Iter #906720:  Learning rate = 0.003463:   Batch Loss = 0.674673, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9328362345695496, Accuracy = 0.7485148310661316\n",
      "Iter #907200:  Learning rate = 0.003463:   Batch Loss = 0.684401, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9719405174255371, Accuracy = 0.7633663415908813\n",
      "Iter #907680:  Learning rate = 0.003463:   Batch Loss = 0.633297, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9175276160240173, Accuracy = 0.7722772359848022\n",
      "Iter #908160:  Learning rate = 0.003463:   Batch Loss = 0.728588, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.943307638168335, Accuracy = 0.7673267126083374\n",
      "Iter #908640:  Learning rate = 0.003463:   Batch Loss = 0.743614, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9322378635406494, Accuracy = 0.7653465270996094\n",
      "Iter #909120:  Learning rate = 0.003463:   Batch Loss = 0.617898, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9365139603614807, Accuracy = 0.7683168053627014\n",
      "Iter #909600:  Learning rate = 0.003463:   Batch Loss = 0.778775, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.930433988571167, Accuracy = 0.7693069577217102\n",
      "Iter #910080:  Learning rate = 0.003463:   Batch Loss = 0.824856, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9015294909477234, Accuracy = 0.7752475142478943\n",
      "Iter #910560:  Learning rate = 0.003463:   Batch Loss = 0.654293, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.903743326663971, Accuracy = 0.7762376070022583\n",
      "Iter #911040:  Learning rate = 0.003463:   Batch Loss = 0.613932, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9096760153770447, Accuracy = 0.7722772359848022\n",
      "Iter #911520:  Learning rate = 0.003463:   Batch Loss = 0.587332, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8996274471282959, Accuracy = 0.7831683158874512\n",
      "Iter #912000:  Learning rate = 0.003463:   Batch Loss = 0.571194, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8903417587280273, Accuracy = 0.7762376070022583\n",
      "Iter #912480:  Learning rate = 0.003463:   Batch Loss = 0.742390, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9048372507095337, Accuracy = 0.7772276997566223\n",
      "Iter #912960:  Learning rate = 0.003463:   Batch Loss = 0.734843, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9075442552566528, Accuracy = 0.7722772359848022\n",
      "Iter #913440:  Learning rate = 0.003463:   Batch Loss = 0.667691, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8949933052062988, Accuracy = 0.7772276997566223\n",
      "Iter #913920:  Learning rate = 0.003463:   Batch Loss = 0.741594, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8992074131965637, Accuracy = 0.7792079448699951\n",
      "Iter #914400:  Learning rate = 0.003463:   Batch Loss = 0.623151, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8955659866333008, Accuracy = 0.7801980376243591\n",
      "Iter #914880:  Learning rate = 0.003463:   Batch Loss = 0.857842, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.916541337966919, Accuracy = 0.7752475142478943\n",
      "Iter #915360:  Learning rate = 0.003463:   Batch Loss = 0.484646, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9270706176757812, Accuracy = 0.7722772359848022\n",
      "Iter #915840:  Learning rate = 0.003463:   Batch Loss = 0.681094, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9192677736282349, Accuracy = 0.7683168053627014\n",
      "Iter #916320:  Learning rate = 0.003463:   Batch Loss = 0.608265, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9216560125350952, Accuracy = 0.7693069577217102\n",
      "Iter #916800:  Learning rate = 0.003463:   Batch Loss = 0.707443, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9138975143432617, Accuracy = 0.7702970504760742\n",
      "Iter #917280:  Learning rate = 0.003463:   Batch Loss = 0.689445, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9119670391082764, Accuracy = 0.7673267126083374\n",
      "Iter #917760:  Learning rate = 0.003463:   Batch Loss = 0.594448, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8969352841377258, Accuracy = 0.7782177925109863\n",
      "Iter #918240:  Learning rate = 0.003463:   Batch Loss = 0.909015, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9173151850700378, Accuracy = 0.7702970504760742\n",
      "Iter #918720:  Learning rate = 0.003463:   Batch Loss = 0.572073, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9089211225509644, Accuracy = 0.7732673287391663\n",
      "Iter #919200:  Learning rate = 0.003463:   Batch Loss = 0.784723, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8931915760040283, Accuracy = 0.7811881303787231\n",
      "Iter #919680:  Learning rate = 0.003463:   Batch Loss = 0.784481, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.1670405864715576, Accuracy = 0.6831682920455933\n",
      "Iter #920160:  Learning rate = 0.003463:   Batch Loss = 0.811492, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9588149785995483, Accuracy = 0.7554455399513245\n",
      "Iter #920640:  Learning rate = 0.003463:   Batch Loss = 0.658178, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9800091981887817, Accuracy = 0.7504950761795044\n",
      "Iter #921120:  Learning rate = 0.003463:   Batch Loss = 0.688074, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9597646594047546, Accuracy = 0.7584158182144165\n",
      "Iter #921600:  Learning rate = 0.003463:   Batch Loss = 0.838523, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9860426187515259, Accuracy = 0.7405940890312195\n",
      "Iter #922080:  Learning rate = 0.003463:   Batch Loss = 0.760301, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9363088011741638, Accuracy = 0.7485148310661316\n",
      "Iter #922560:  Learning rate = 0.003463:   Batch Loss = 0.584210, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9703100323677063, Accuracy = 0.7435643672943115\n",
      "Iter #923040:  Learning rate = 0.003463:   Batch Loss = 0.772215, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9773351550102234, Accuracy = 0.7316831946372986\n",
      "Iter #923520:  Learning rate = 0.003463:   Batch Loss = 0.604244, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9676686525344849, Accuracy = 0.7386138439178467\n",
      "Iter #924000:  Learning rate = 0.003463:   Batch Loss = 0.709459, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9516857862472534, Accuracy = 0.7534653544425964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #924480:  Learning rate = 0.003463:   Batch Loss = 0.629769, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9230054616928101, Accuracy = 0.7752475142478943\n",
      "Iter #924960:  Learning rate = 0.003463:   Batch Loss = 0.559876, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9308261871337891, Accuracy = 0.7564356327056885\n",
      "Iter #925440:  Learning rate = 0.003463:   Batch Loss = 0.620503, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9019761085510254, Accuracy = 0.7693069577217102\n",
      "Iter #925920:  Learning rate = 0.003463:   Batch Loss = 0.713646, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9486069679260254, Accuracy = 0.7534653544425964\n",
      "Iter #926400:  Learning rate = 0.003463:   Batch Loss = 0.822914, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9445556998252869, Accuracy = 0.7386138439178467\n",
      "Iter #926880:  Learning rate = 0.003463:   Batch Loss = 0.696576, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9527924060821533, Accuracy = 0.7475247383117676\n",
      "Iter #927360:  Learning rate = 0.003463:   Batch Loss = 0.650900, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9496944546699524, Accuracy = 0.7613861560821533\n",
      "Iter #927840:  Learning rate = 0.003463:   Batch Loss = 0.621172, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.95047527551651, Accuracy = 0.7594059109687805\n",
      "Iter #928320:  Learning rate = 0.003463:   Batch Loss = 0.830602, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9636245369911194, Accuracy = 0.7594059109687805\n",
      "Iter #928800:  Learning rate = 0.003463:   Batch Loss = 0.718197, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9706982374191284, Accuracy = 0.7643564343452454\n",
      "Iter #929280:  Learning rate = 0.003463:   Batch Loss = 0.733636, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9590761065483093, Accuracy = 0.7485148310661316\n",
      "Iter #929760:  Learning rate = 0.003463:   Batch Loss = 0.810911, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.937556266784668, Accuracy = 0.7514851689338684\n",
      "Iter #930240:  Learning rate = 0.003463:   Batch Loss = 0.656076, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9375560283660889, Accuracy = 0.7564356327056885\n",
      "Iter #930720:  Learning rate = 0.003463:   Batch Loss = 0.722947, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9439266324043274, Accuracy = 0.7603960633277893\n",
      "Iter #931200:  Learning rate = 0.003463:   Batch Loss = 0.642945, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9511681795120239, Accuracy = 0.7584158182144165\n",
      "Iter #931680:  Learning rate = 0.003463:   Batch Loss = 0.759844, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9606497287750244, Accuracy = 0.7594059109687805\n",
      "Iter #932160:  Learning rate = 0.003463:   Batch Loss = 0.494191, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9354419708251953, Accuracy = 0.7722772359848022\n",
      "Iter #932640:  Learning rate = 0.003463:   Batch Loss = 0.703312, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.931800365447998, Accuracy = 0.7623762488365173\n",
      "Iter #933120:  Learning rate = 0.003463:   Batch Loss = 0.735332, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.934779167175293, Accuracy = 0.7673267126083374\n",
      "Iter #933600:  Learning rate = 0.003463:   Batch Loss = 0.808282, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.92679762840271, Accuracy = 0.7643564343452454\n",
      "Iter #934080:  Learning rate = 0.003463:   Batch Loss = 0.763652, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9514431357383728, Accuracy = 0.7564356327056885\n",
      "Iter #934560:  Learning rate = 0.003463:   Batch Loss = 0.698953, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9072337746620178, Accuracy = 0.7871286869049072\n",
      "Iter #935040:  Learning rate = 0.003463:   Batch Loss = 0.703589, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9273583292961121, Accuracy = 0.7653465270996094\n",
      "Iter #935520:  Learning rate = 0.003463:   Batch Loss = 0.711937, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9208609461784363, Accuracy = 0.7811881303787231\n",
      "Iter #936000:  Learning rate = 0.003463:   Batch Loss = 0.736017, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9079174995422363, Accuracy = 0.7782177925109863\n",
      "Iter #936480:  Learning rate = 0.003463:   Batch Loss = 0.691571, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8955389857292175, Accuracy = 0.7792079448699951\n",
      "Iter #936960:  Learning rate = 0.003463:   Batch Loss = 0.728490, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9037424325942993, Accuracy = 0.7831683158874512\n",
      "Iter #937440:  Learning rate = 0.003463:   Batch Loss = 0.676182, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9055191874504089, Accuracy = 0.7841584086418152\n",
      "Iter #937920:  Learning rate = 0.003463:   Batch Loss = 0.786126, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9281613230705261, Accuracy = 0.7811881303787231\n",
      "Iter #938400:  Learning rate = 0.003463:   Batch Loss = 0.666607, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9171104431152344, Accuracy = 0.7772276997566223\n",
      "Iter #938880:  Learning rate = 0.003463:   Batch Loss = 0.745679, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9112273454666138, Accuracy = 0.7871286869049072\n",
      "Iter #939360:  Learning rate = 0.003463:   Batch Loss = 0.735184, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9143638610839844, Accuracy = 0.7841584086418152\n",
      "Iter #939840:  Learning rate = 0.003463:   Batch Loss = 0.687583, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9104130268096924, Accuracy = 0.7841584086418152\n",
      "Iter #940320:  Learning rate = 0.003463:   Batch Loss = 0.546922, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9061780571937561, Accuracy = 0.7792079448699951\n",
      "Iter #940800:  Learning rate = 0.003463:   Batch Loss = 0.708616, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9552755355834961, Accuracy = 0.7712871432304382\n",
      "Iter #941280:  Learning rate = 0.003463:   Batch Loss = 0.445603, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9332886934280396, Accuracy = 0.7732673287391663\n",
      "Iter #941760:  Learning rate = 0.003463:   Batch Loss = 0.618894, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9074615240097046, Accuracy = 0.7782177925109863\n",
      "Iter #942240:  Learning rate = 0.003463:   Batch Loss = 0.650604, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9256000518798828, Accuracy = 0.7633663415908813\n",
      "Iter #942720:  Learning rate = 0.003463:   Batch Loss = 0.623294, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8995512127876282, Accuracy = 0.7811881303787231\n",
      "Iter #943200:  Learning rate = 0.003463:   Batch Loss = 0.662127, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8959960341453552, Accuracy = 0.7732673287391663\n",
      "Iter #943680:  Learning rate = 0.003463:   Batch Loss = 0.749821, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9391130208969116, Accuracy = 0.7623762488365173\n",
      "Iter #944160:  Learning rate = 0.003463:   Batch Loss = 0.651783, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9059537649154663, Accuracy = 0.7673267126083374\n",
      "Iter #944640:  Learning rate = 0.003463:   Batch Loss = 0.488439, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9920948147773743, Accuracy = 0.7386138439178467\n",
      "Iter #945120:  Learning rate = 0.003463:   Batch Loss = 0.531473, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.936902642250061, Accuracy = 0.7623762488365173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #945600:  Learning rate = 0.003463:   Batch Loss = 0.726265, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8957462310791016, Accuracy = 0.7693069577217102\n",
      "Iter #946080:  Learning rate = 0.003463:   Batch Loss = 0.814437, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9223760366439819, Accuracy = 0.7762376070022583\n",
      "Iter #946560:  Learning rate = 0.003463:   Batch Loss = 0.711896, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9463011622428894, Accuracy = 0.7643564343452454\n",
      "Iter #947040:  Learning rate = 0.003463:   Batch Loss = 0.626473, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9484990239143372, Accuracy = 0.7623762488365173\n",
      "Iter #947520:  Learning rate = 0.003463:   Batch Loss = 0.672001, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9339750409126282, Accuracy = 0.7584158182144165\n",
      "Iter #948000:  Learning rate = 0.003463:   Batch Loss = 0.692153, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9176758527755737, Accuracy = 0.7712871432304382\n",
      "Iter #948480:  Learning rate = 0.003463:   Batch Loss = 0.687977, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9079212546348572, Accuracy = 0.7712871432304382\n",
      "Iter #948960:  Learning rate = 0.003463:   Batch Loss = 0.736605, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.919948935508728, Accuracy = 0.7653465270996094\n",
      "Iter #949440:  Learning rate = 0.003463:   Batch Loss = 0.569695, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9327424764633179, Accuracy = 0.7653465270996094\n",
      "Iter #949920:  Learning rate = 0.003463:   Batch Loss = 0.659812, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0096495151519775, Accuracy = 0.7366336584091187\n",
      "Iter #950400:  Learning rate = 0.003463:   Batch Loss = 0.637495, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9787800312042236, Accuracy = 0.7574257254600525\n",
      "Iter #950880:  Learning rate = 0.003463:   Batch Loss = 0.590256, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.998569130897522, Accuracy = 0.7376237511634827\n",
      "Iter #951360:  Learning rate = 0.003463:   Batch Loss = 0.604022, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.949500322341919, Accuracy = 0.7643564343452454\n",
      "Iter #951840:  Learning rate = 0.003463:   Batch Loss = 0.773554, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9396321773529053, Accuracy = 0.7643564343452454\n",
      "Iter #952320:  Learning rate = 0.003463:   Batch Loss = 0.648338, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.928061842918396, Accuracy = 0.7752475142478943\n",
      "Iter #952800:  Learning rate = 0.003463:   Batch Loss = 0.771984, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9113081693649292, Accuracy = 0.7782177925109863\n",
      "Iter #953280:  Learning rate = 0.003463:   Batch Loss = 0.692398, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9402608871459961, Accuracy = 0.7673267126083374\n",
      "Iter #953760:  Learning rate = 0.003463:   Batch Loss = 0.624816, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9520029425621033, Accuracy = 0.7564356327056885\n",
      "Iter #954240:  Learning rate = 0.003463:   Batch Loss = 0.614280, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.95458984375, Accuracy = 0.7752475142478943\n",
      "Iter #954720:  Learning rate = 0.003463:   Batch Loss = 0.637149, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.940290093421936, Accuracy = 0.7653465270996094\n",
      "Iter #955200:  Learning rate = 0.003463:   Batch Loss = 0.582031, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9941474199295044, Accuracy = 0.7475247383117676\n",
      "Iter #955680:  Learning rate = 0.003463:   Batch Loss = 0.689149, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.988932728767395, Accuracy = 0.7613861560821533\n",
      "Iter #956160:  Learning rate = 0.003463:   Batch Loss = 0.639670, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9308639764785767, Accuracy = 0.7851485013961792\n",
      "Iter #956640:  Learning rate = 0.003463:   Batch Loss = 0.704106, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9653267860412598, Accuracy = 0.7584158182144165\n",
      "Iter #957120:  Learning rate = 0.003463:   Batch Loss = 0.591820, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9296944737434387, Accuracy = 0.7762376070022583\n",
      "Iter #957600:  Learning rate = 0.003463:   Batch Loss = 0.514570, Accuracy = 0.949999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9275881052017212, Accuracy = 0.7702970504760742\n",
      "Iter #958080:  Learning rate = 0.003463:   Batch Loss = 0.611301, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9225866198539734, Accuracy = 0.7643564343452454\n",
      "Iter #958560:  Learning rate = 0.003463:   Batch Loss = 0.615680, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.908141016960144, Accuracy = 0.7841584086418152\n",
      "Iter #959040:  Learning rate = 0.003463:   Batch Loss = 0.554952, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9229912757873535, Accuracy = 0.7762376070022583\n",
      "Iter #959520:  Learning rate = 0.003463:   Batch Loss = 0.565620, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9047531485557556, Accuracy = 0.7910891175270081\n",
      "Iter #960000:  Learning rate = 0.003463:   Batch Loss = 0.664639, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8941506743431091, Accuracy = 0.7811881303787231\n",
      "Iter #960480:  Learning rate = 0.003463:   Batch Loss = 0.703612, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8884279727935791, Accuracy = 0.7950494885444641\n",
      "Iter #960960:  Learning rate = 0.003463:   Batch Loss = 0.687560, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9016113877296448, Accuracy = 0.7851485013961792\n",
      "Iter #961440:  Learning rate = 0.003463:   Batch Loss = 0.580736, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9298492074012756, Accuracy = 0.7613861560821533\n",
      "Iter #961920:  Learning rate = 0.003463:   Batch Loss = 0.541056, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9724661111831665, Accuracy = 0.7574257254600525\n",
      "Iter #962400:  Learning rate = 0.003463:   Batch Loss = 0.754957, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9447387456893921, Accuracy = 0.7693069577217102\n",
      "Iter #962880:  Learning rate = 0.003463:   Batch Loss = 0.813172, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9590638279914856, Accuracy = 0.7574257254600525\n",
      "Iter #963360:  Learning rate = 0.003463:   Batch Loss = 0.649564, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9875390529632568, Accuracy = 0.7386138439178467\n",
      "Iter #963840:  Learning rate = 0.003463:   Batch Loss = 0.862929, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9975939989089966, Accuracy = 0.7386138439178467\n",
      "Iter #964320:  Learning rate = 0.003463:   Batch Loss = 0.782022, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9923414587974548, Accuracy = 0.7455445528030396\n",
      "Iter #964800:  Learning rate = 0.003463:   Batch Loss = 0.629867, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9635383486747742, Accuracy = 0.7623762488365173\n",
      "Iter #965280:  Learning rate = 0.003463:   Batch Loss = 0.687241, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9678196907043457, Accuracy = 0.7673267126083374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #965760:  Learning rate = 0.003463:   Batch Loss = 0.791671, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9679271578788757, Accuracy = 0.7722772359848022\n",
      "Iter #966240:  Learning rate = 0.003463:   Batch Loss = 0.568765, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9692633152008057, Accuracy = 0.7495049238204956\n",
      "Iter #966720:  Learning rate = 0.003463:   Batch Loss = 0.645987, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9360421299934387, Accuracy = 0.7742574214935303\n",
      "Iter #967200:  Learning rate = 0.003463:   Batch Loss = 0.628516, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9238708019256592, Accuracy = 0.7782177925109863\n",
      "Iter #967680:  Learning rate = 0.003463:   Batch Loss = 0.631209, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9133619070053101, Accuracy = 0.7851485013961792\n",
      "Iter #968160:  Learning rate = 0.003463:   Batch Loss = 0.612652, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8986551761627197, Accuracy = 0.7782177925109863\n",
      "Iter #968640:  Learning rate = 0.003463:   Batch Loss = 0.646780, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9179179668426514, Accuracy = 0.7762376070022583\n",
      "Iter #969120:  Learning rate = 0.003463:   Batch Loss = 0.634345, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.927646279335022, Accuracy = 0.7851485013961792\n",
      "Iter #969600:  Learning rate = 0.003463:   Batch Loss = 0.622127, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9134370684623718, Accuracy = 0.7841584086418152\n",
      "Iter #970080:  Learning rate = 0.003463:   Batch Loss = 0.803369, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9159822463989258, Accuracy = 0.7841584086418152\n",
      "Iter #970560:  Learning rate = 0.003463:   Batch Loss = 0.736402, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9219327569007874, Accuracy = 0.7851485013961792\n",
      "Iter #971040:  Learning rate = 0.003463:   Batch Loss = 0.536018, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9407035112380981, Accuracy = 0.7821782231330872\n",
      "Iter #971520:  Learning rate = 0.003463:   Batch Loss = 0.682939, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.969219982624054, Accuracy = 0.7683168053627014\n",
      "Iter #972000:  Learning rate = 0.003463:   Batch Loss = 0.786802, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9575322866439819, Accuracy = 0.7623762488365173\n",
      "Iter #972480:  Learning rate = 0.003463:   Batch Loss = 0.602099, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9664099812507629, Accuracy = 0.7584158182144165\n",
      "Iter #972960:  Learning rate = 0.003463:   Batch Loss = 0.675232, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9688536524772644, Accuracy = 0.7623762488365173\n",
      "Iter #973440:  Learning rate = 0.003463:   Batch Loss = 0.563430, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9889870882034302, Accuracy = 0.7544554471969604\n",
      "Iter #973920:  Learning rate = 0.003463:   Batch Loss = 0.656427, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9580526947975159, Accuracy = 0.7722772359848022\n",
      "Iter #974400:  Learning rate = 0.003463:   Batch Loss = 0.606305, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9500946998596191, Accuracy = 0.7801980376243591\n",
      "Iter #974880:  Learning rate = 0.003463:   Batch Loss = 0.609059, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.927711009979248, Accuracy = 0.7801980376243591\n",
      "Iter #975360:  Learning rate = 0.003463:   Batch Loss = 0.790307, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.935390830039978, Accuracy = 0.7702970504760742\n",
      "Iter #975840:  Learning rate = 0.003463:   Batch Loss = 0.674271, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.941382646560669, Accuracy = 0.7613861560821533\n",
      "Iter #976320:  Learning rate = 0.003463:   Batch Loss = 0.849402, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9582734107971191, Accuracy = 0.7613861560821533\n",
      "Iter #976800:  Learning rate = 0.003463:   Batch Loss = 0.719910, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.968969464302063, Accuracy = 0.7455445528030396\n",
      "Iter #977280:  Learning rate = 0.003463:   Batch Loss = 0.712648, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0086826086044312, Accuracy = 0.7415841817855835\n",
      "Iter #977760:  Learning rate = 0.003463:   Batch Loss = 0.710630, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9427805542945862, Accuracy = 0.7702970504760742\n",
      "Iter #978240:  Learning rate = 0.003463:   Batch Loss = 0.537195, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.914175271987915, Accuracy = 0.7732673287391663\n",
      "Iter #978720:  Learning rate = 0.003463:   Batch Loss = 0.786381, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9332745671272278, Accuracy = 0.7693069577217102\n",
      "Iter #979200:  Learning rate = 0.003463:   Batch Loss = 0.747902, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.970444917678833, Accuracy = 0.7623762488365173\n",
      "Iter #979680:  Learning rate = 0.003463:   Batch Loss = 0.674508, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9532175660133362, Accuracy = 0.7554455399513245\n",
      "Iter #980160:  Learning rate = 0.003463:   Batch Loss = 0.882376, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9786953330039978, Accuracy = 0.7524752616882324\n",
      "Iter #980640:  Learning rate = 0.003463:   Batch Loss = 0.941056, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9924583435058594, Accuracy = 0.7465346455574036\n",
      "Iter #981120:  Learning rate = 0.003463:   Batch Loss = 0.632860, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.004591464996338, Accuracy = 0.7356435656547546\n",
      "Iter #981600:  Learning rate = 0.003463:   Batch Loss = 0.846677, Accuracy = 0.7166666388511658\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9702061414718628, Accuracy = 0.7524752616882324\n",
      "Iter #982080:  Learning rate = 0.003463:   Batch Loss = 0.739912, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9416517615318298, Accuracy = 0.7683168053627014\n",
      "Iter #982560:  Learning rate = 0.003463:   Batch Loss = 0.748565, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.912365734577179, Accuracy = 0.7732673287391663\n",
      "Iter #983040:  Learning rate = 0.003463:   Batch Loss = 0.666487, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9146500825881958, Accuracy = 0.7801980376243591\n",
      "Iter #983520:  Learning rate = 0.003463:   Batch Loss = 0.642891, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.921913206577301, Accuracy = 0.7732673287391663\n",
      "Iter #984000:  Learning rate = 0.003463:   Batch Loss = 0.779580, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9089797735214233, Accuracy = 0.7851485013961792\n",
      "Iter #984480:  Learning rate = 0.003463:   Batch Loss = 0.563554, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9044873714447021, Accuracy = 0.7752475142478943\n",
      "Iter #984960:  Learning rate = 0.003463:   Batch Loss = 0.695216, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9203253984451294, Accuracy = 0.7831683158874512\n",
      "Iter #985440:  Learning rate = 0.003463:   Batch Loss = 0.633484, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9009250402450562, Accuracy = 0.7861385941505432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #985920:  Learning rate = 0.003463:   Batch Loss = 0.673166, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9122677445411682, Accuracy = 0.7811881303787231\n",
      "Iter #986400:  Learning rate = 0.003463:   Batch Loss = 0.702547, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9246393442153931, Accuracy = 0.7801980376243591\n",
      "Iter #986880:  Learning rate = 0.003463:   Batch Loss = 0.848325, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.93058180809021, Accuracy = 0.7673267126083374\n",
      "Iter #987360:  Learning rate = 0.003463:   Batch Loss = 0.517826, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9440370798110962, Accuracy = 0.7623762488365173\n",
      "Iter #987840:  Learning rate = 0.003463:   Batch Loss = 0.643927, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9809258580207825, Accuracy = 0.7594059109687805\n",
      "Iter #988320:  Learning rate = 0.003463:   Batch Loss = 0.840131, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9857133030891418, Accuracy = 0.7544554471969604\n",
      "Iter #988800:  Learning rate = 0.003463:   Batch Loss = 0.711053, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9602327942848206, Accuracy = 0.7683168053627014\n",
      "Iter #989280:  Learning rate = 0.003463:   Batch Loss = 0.538214, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9496275186538696, Accuracy = 0.7772276997566223\n",
      "Iter #989760:  Learning rate = 0.003463:   Batch Loss = 0.641967, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9728054404258728, Accuracy = 0.7742574214935303\n",
      "Iter #990240:  Learning rate = 0.003463:   Batch Loss = 0.748340, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.0144639015197754, Accuracy = 0.7465346455574036\n",
      "Iter #990720:  Learning rate = 0.003463:   Batch Loss = 0.738306, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9937942028045654, Accuracy = 0.7584158182144165\n",
      "Iter #991200:  Learning rate = 0.003463:   Batch Loss = 0.862219, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9595298767089844, Accuracy = 0.7613861560821533\n",
      "Iter #991680:  Learning rate = 0.003463:   Batch Loss = 0.800078, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9452515244483948, Accuracy = 0.7653465270996094\n",
      "Iter #992160:  Learning rate = 0.003463:   Batch Loss = 0.747884, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9424134492874146, Accuracy = 0.7683168053627014\n",
      "Iter #992640:  Learning rate = 0.003463:   Batch Loss = 0.680438, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9768686890602112, Accuracy = 0.7564356327056885\n",
      "Iter #993120:  Learning rate = 0.003463:   Batch Loss = 0.590821, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9521312713623047, Accuracy = 0.7594059109687805\n",
      "Iter #993600:  Learning rate = 0.003463:   Batch Loss = 0.798978, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 1.006558895111084, Accuracy = 0.7277227640151978\n",
      "Iter #994080:  Learning rate = 0.003463:   Batch Loss = 0.622368, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9638051986694336, Accuracy = 0.7386138439178467\n",
      "Iter #994560:  Learning rate = 0.003463:   Batch Loss = 0.754574, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9358681440353394, Accuracy = 0.7683168053627014\n",
      "Iter #995040:  Learning rate = 0.003463:   Batch Loss = 0.672786, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.924869179725647, Accuracy = 0.7663366198539734\n",
      "Iter #995520:  Learning rate = 0.003463:   Batch Loss = 0.769645, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.890733540058136, Accuracy = 0.7663366198539734\n",
      "Iter #996000:  Learning rate = 0.003463:   Batch Loss = 0.742337, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.869716227054596, Accuracy = 0.7920792102813721\n",
      "Iter #996480:  Learning rate = 0.003463:   Batch Loss = 0.556218, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8748442530632019, Accuracy = 0.7841584086418152\n",
      "Iter #996960:  Learning rate = 0.003463:   Batch Loss = 0.813647, Accuracy = 0.7666666507720947\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8952481746673584, Accuracy = 0.7722772359848022\n",
      "Iter #997440:  Learning rate = 0.003463:   Batch Loss = 0.665627, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9067485928535461, Accuracy = 0.7732673287391663\n",
      "Iter #997920:  Learning rate = 0.003463:   Batch Loss = 0.766543, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9142828583717346, Accuracy = 0.7782177925109863\n",
      "Iter #998400:  Learning rate = 0.003463:   Batch Loss = 0.920888, Accuracy = 0.7333333492279053\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8866180777549744, Accuracy = 0.7732673287391663\n",
      "Iter #998880:  Learning rate = 0.003463:   Batch Loss = 0.801364, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8941613435745239, Accuracy = 0.7801980376243591\n",
      "Iter #999360:  Learning rate = 0.003463:   Batch Loss = 0.669129, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8853437900543213, Accuracy = 0.7811881303787231\n",
      "Iter #999840:  Learning rate = 0.003463:   Batch Loss = 0.731592, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.860430121421814, Accuracy = 0.7970296740531921\n",
      "Iter #1000320:  Learning rate = 0.003324:   Batch Loss = 0.765150, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8827614784240723, Accuracy = 0.7851485013961792\n",
      "Iter #1000800:  Learning rate = 0.003324:   Batch Loss = 0.536519, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8881010413169861, Accuracy = 0.7950494885444641\n",
      "Iter #1001280:  Learning rate = 0.003324:   Batch Loss = 0.763167, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9070448279380798, Accuracy = 0.7861385941505432\n",
      "Iter #1001760:  Learning rate = 0.003324:   Batch Loss = 0.706097, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9086337685585022, Accuracy = 0.7831683158874512\n",
      "Iter #1002240:  Learning rate = 0.003324:   Batch Loss = 0.762139, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8908274173736572, Accuracy = 0.7841584086418152\n",
      "Iter #1002720:  Learning rate = 0.003324:   Batch Loss = 0.605189, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9123238325119019, Accuracy = 0.7702970504760742\n",
      "Iter #1003200:  Learning rate = 0.003324:   Batch Loss = 0.646852, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8942320942878723, Accuracy = 0.7752475142478943\n",
      "Iter #1003680:  Learning rate = 0.003324:   Batch Loss = 0.671947, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8620915412902832, Accuracy = 0.7920792102813721\n",
      "Iter #1004160:  Learning rate = 0.003324:   Batch Loss = 0.607002, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8752973079681396, Accuracy = 0.7851485013961792\n",
      "Iter #1004640:  Learning rate = 0.003324:   Batch Loss = 0.543809, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8732475638389587, Accuracy = 0.7851485013961792\n",
      "Iter #1005120:  Learning rate = 0.003324:   Batch Loss = 0.813583, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8842653632164001, Accuracy = 0.7871286869049072\n",
      "Iter #1005600:  Learning rate = 0.003324:   Batch Loss = 0.661870, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8946976661682129, Accuracy = 0.7871286869049072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #1006080:  Learning rate = 0.003324:   Batch Loss = 0.684607, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8984155654907227, Accuracy = 0.7851485013961792\n",
      "Iter #1006560:  Learning rate = 0.003324:   Batch Loss = 0.684434, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9091714024543762, Accuracy = 0.7633663415908813\n",
      "Iter #1007040:  Learning rate = 0.003324:   Batch Loss = 0.799636, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9377602338790894, Accuracy = 0.7683168053627014\n",
      "Iter #1007520:  Learning rate = 0.003324:   Batch Loss = 0.602497, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9309872984886169, Accuracy = 0.7623762488365173\n",
      "Iter #1008000:  Learning rate = 0.003324:   Batch Loss = 0.767994, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9358844757080078, Accuracy = 0.7534653544425964\n",
      "Iter #1008480:  Learning rate = 0.003324:   Batch Loss = 0.795842, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.911819577217102, Accuracy = 0.7623762488365173\n",
      "Iter #1008960:  Learning rate = 0.003324:   Batch Loss = 0.896441, Accuracy = 0.699999988079071\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8970286846160889, Accuracy = 0.7831683158874512\n",
      "Iter #1009440:  Learning rate = 0.003324:   Batch Loss = 0.467569, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8584986329078674, Accuracy = 0.7970296740531921\n",
      "Iter #1009920:  Learning rate = 0.003324:   Batch Loss = 0.599607, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8740006685256958, Accuracy = 0.788118839263916\n",
      "Iter #1010400:  Learning rate = 0.003324:   Batch Loss = 0.702615, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8927879333496094, Accuracy = 0.7732673287391663\n",
      "Iter #1010880:  Learning rate = 0.003324:   Batch Loss = 0.707835, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8864225149154663, Accuracy = 0.78910893201828\n",
      "Iter #1011360:  Learning rate = 0.003324:   Batch Loss = 0.666583, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.90789395570755, Accuracy = 0.7841584086418152\n",
      "Iter #1011840:  Learning rate = 0.003324:   Batch Loss = 0.602971, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8696368932723999, Accuracy = 0.7831683158874512\n",
      "Iter #1012320:  Learning rate = 0.003324:   Batch Loss = 0.747513, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.895071804523468, Accuracy = 0.7732673287391663\n",
      "Iter #1012800:  Learning rate = 0.003324:   Batch Loss = 0.785079, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9049110412597656, Accuracy = 0.7653465270996094\n",
      "Iter #1013280:  Learning rate = 0.003324:   Batch Loss = 0.604251, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.909062385559082, Accuracy = 0.7693069577217102\n",
      "Iter #1013760:  Learning rate = 0.003324:   Batch Loss = 0.571305, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8888264894485474, Accuracy = 0.7792079448699951\n",
      "Iter #1014240:  Learning rate = 0.003324:   Batch Loss = 0.650863, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8742249608039856, Accuracy = 0.7752475142478943\n",
      "Iter #1014720:  Learning rate = 0.003324:   Batch Loss = 0.673383, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.884375810623169, Accuracy = 0.788118839263916\n",
      "Iter #1015200:  Learning rate = 0.003324:   Batch Loss = 0.636357, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9126588106155396, Accuracy = 0.7861385941505432\n",
      "Iter #1015680:  Learning rate = 0.003324:   Batch Loss = 0.742515, Accuracy = 0.75\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9106214642524719, Accuracy = 0.7772276997566223\n",
      "Iter #1016160:  Learning rate = 0.003324:   Batch Loss = 0.640922, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9067820310592651, Accuracy = 0.7831683158874512\n",
      "Iter #1016640:  Learning rate = 0.003324:   Batch Loss = 0.658337, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9168984889984131, Accuracy = 0.7782177925109863\n",
      "Iter #1017120:  Learning rate = 0.003324:   Batch Loss = 0.677357, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9003115892410278, Accuracy = 0.7841584086418152\n",
      "Iter #1017600:  Learning rate = 0.003324:   Batch Loss = 0.608072, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.904875636100769, Accuracy = 0.7821782231330872\n",
      "Iter #1018080:  Learning rate = 0.003324:   Batch Loss = 0.661522, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8651191592216492, Accuracy = 0.7871286869049072\n",
      "Iter #1018560:  Learning rate = 0.003324:   Batch Loss = 0.750123, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8641701340675354, Accuracy = 0.7970296740531921\n",
      "Iter #1019040:  Learning rate = 0.003324:   Batch Loss = 0.706071, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8897917866706848, Accuracy = 0.7821782231330872\n",
      "Iter #1019520:  Learning rate = 0.003324:   Batch Loss = 0.659913, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8857437968254089, Accuracy = 0.78910893201828\n",
      "Iter #1020000:  Learning rate = 0.003324:   Batch Loss = 0.640211, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8868575692176819, Accuracy = 0.790099024772644\n",
      "Iter #1020480:  Learning rate = 0.003324:   Batch Loss = 0.724513, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8899528980255127, Accuracy = 0.788118839263916\n",
      "Iter #1020960:  Learning rate = 0.003324:   Batch Loss = 0.756450, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8863333463668823, Accuracy = 0.7990099191665649\n",
      "Iter #1021440:  Learning rate = 0.003324:   Batch Loss = 0.658620, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8825675249099731, Accuracy = 0.7970296740531921\n",
      "Iter #1021920:  Learning rate = 0.003324:   Batch Loss = 0.698553, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8592100143432617, Accuracy = 0.8099009990692139\n",
      "Iter #1022400:  Learning rate = 0.003324:   Batch Loss = 0.525042, Accuracy = 0.8999999761581421\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.862160325050354, Accuracy = 0.800990104675293\n",
      "Iter #1022880:  Learning rate = 0.003324:   Batch Loss = 0.616928, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8714019656181335, Accuracy = 0.78910893201828\n",
      "Iter #1023360:  Learning rate = 0.003324:   Batch Loss = 0.633700, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8608202934265137, Accuracy = 0.7960395812988281\n",
      "Iter #1023840:  Learning rate = 0.003324:   Batch Loss = 0.559844, Accuracy = 0.9166666865348816\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8599355220794678, Accuracy = 0.7950494885444641\n",
      "Iter #1024320:  Learning rate = 0.003324:   Batch Loss = 0.739064, Accuracy = 0.8166666626930237\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8717072606086731, Accuracy = 0.7960395812988281\n",
      "Iter #1024800:  Learning rate = 0.003324:   Batch Loss = 0.634623, Accuracy = 0.800000011920929\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8838000297546387, Accuracy = 0.7970296740531921\n",
      "Iter #1025280:  Learning rate = 0.003324:   Batch Loss = 0.490384, Accuracy = 0.9333333373069763\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8716500401496887, Accuracy = 0.7960395812988281\n",
      "Iter #1025760:  Learning rate = 0.003324:   Batch Loss = 0.670133, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8708671927452087, Accuracy = 0.7861385941505432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter #1026240:  Learning rate = 0.003324:   Batch Loss = 0.690369, Accuracy = 0.8500000238418579\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8684769868850708, Accuracy = 0.7940593957901001\n",
      "Iter #1026720:  Learning rate = 0.003324:   Batch Loss = 0.617716, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8759497404098511, Accuracy = 0.7960395812988281\n",
      "Iter #1027200:  Learning rate = 0.003324:   Batch Loss = 0.539217, Accuracy = 0.8833333253860474\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8480490446090698, Accuracy = 0.7990099191665649\n",
      "Iter #1027680:  Learning rate = 0.003324:   Batch Loss = 0.611964, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.862163782119751, Accuracy = 0.7980198264122009\n",
      "Iter #1028160:  Learning rate = 0.003324:   Batch Loss = 0.781689, Accuracy = 0.7833333611488342\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9154297709465027, Accuracy = 0.7742574214935303\n",
      "Iter #1028640:  Learning rate = 0.003324:   Batch Loss = 0.633087, Accuracy = 0.8666666746139526\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.9186161756515503, Accuracy = 0.7792079448699951\n",
      "Iter #1029120:  Learning rate = 0.003324:   Batch Loss = 0.678514, Accuracy = 0.8333333134651184\n",
      "PERFORMANCE ON TEST SET:             Batch Loss = 0.8880382180213928, Accuracy = 0.801980197429657\n",
      "Optimization Finished!\n",
      "FINAL RESULT: Batch Loss = 0.8880382180213928, Accuracy = 0.801980197429657\n",
      "TOTAL TIME:  172.46515464782715\n"
     ]
    }
   ],
   "source": [
    "# Perform Training steps with \"batch_size\" amount of data at each loop. \n",
    "# Elements of each batch are chosen randomly, without replacement, from X_train, \n",
    "# restarting when remaining datapoints < batch_size\n",
    "step = 1\n",
    "time_start = time.time()\n",
    "unsampled_indices = range(0,len(X_train))\n",
    "\n",
    "if not train:\n",
    "    \n",
    "    # only perform testing\n",
    "    loss, acc = sess.run(\n",
    "        [cost, accuracy], \n",
    "        feed_dict={\n",
    "            x: X_test,\n",
    "            y: one_hot(y_test)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"PERFORMANCE ON TEST SET:             \" + \\\n",
    "          \"Batch Loss = {}\".format(loss) + \\\n",
    "          \", Accuracy = {}\".format(acc))\n",
    "\n",
    "while train and (step * batch_size <= training_iters):\n",
    "    #print (sess.run(learning_rate)) #decaying learning rate\n",
    "    #print (sess.run(global_step)) # global number of iterations\n",
    "    if len(unsampled_indices) < batch_size:\n",
    "        unsampled_indices = range(0,len(X_train)) \n",
    "    batch_xs, raw_labels, unsampled_indicies = extract_batch_size(X_train, y_train, unsampled_indices, batch_size)\n",
    "    batch_ys = one_hot(raw_labels)\n",
    "    # check that encoded output is same length as num_classes, if not, pad it \n",
    "    if len(batch_ys[0]) < n_classes:\n",
    "        temp_ys = np.zeros((batch_size, n_classes))\n",
    "        temp_ys[:batch_ys.shape[0],:batch_ys.shape[1]] = batch_ys\n",
    "        batch_ys = temp_ys\n",
    "       \n",
    "    # Fit training using batch data\n",
    "    _, loss, acc = sess.run(\n",
    "        [optimizer, cost, accuracy],\n",
    "        feed_dict={\n",
    "            x: batch_xs, \n",
    "            y: batch_ys\n",
    "        }\n",
    "    )\n",
    "    train_losses.append(loss)\n",
    "    train_accuracies.append(acc)\n",
    "    \n",
    "    # Evaluate network only at some steps for faster training: \n",
    "    if (step*batch_size % display_iter == 0) or (step == 1) or (step * batch_size > training_iters):\n",
    "        \n",
    "        # To not spam console, show training accuracy/loss in this \"if\"\n",
    "        print(\"Iter #\" + str(step*batch_size) + \\\n",
    "              \":  Learning rate = \" + \"{:.6f}\".format(sess.run(learning_rate)) + \\\n",
    "              \":   Batch Loss = \" + \"{:.6f}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "        \n",
    "        # Evaluation on the test set (no learning made here - just evaluation for diagnosis)\n",
    "        loss, acc = sess.run(\n",
    "            [cost, accuracy], \n",
    "            feed_dict={\n",
    "                x: X_test,\n",
    "                y: one_hot(y_test)\n",
    "            }\n",
    "        )\n",
    "        test_losses.append(loss)\n",
    "        test_accuracies.append(acc)\n",
    "        print(\"PERFORMANCE ON TEST SET:             \" + \\\n",
    "              \"Batch Loss = {}\".format(loss) + \\\n",
    "              \", Accuracy = {}\".format(acc))\n",
    "\n",
    "    step += 1\n",
    "\n",
    "print(\"Optimization Finished!\")\n",
    "\n",
    "# Accuracy for test data\n",
    "\n",
    "one_hot_predictions, accuracy_fin, final_loss = sess.run(\n",
    "    [pred, accuracy, cost],\n",
    "    feed_dict={\n",
    "        x: X_test,\n",
    "        y: one_hot(y_test)\n",
    "    }\n",
    ")\n",
    "\n",
    "test_losses.append(final_loss)\n",
    "test_accuracies.append(accuracy_fin)\n",
    "\n",
    "print(\"FINAL RESULT: \" + \\\n",
    "      \"Batch Loss = {}\".format(final_loss) + \\\n",
    "      \", Accuracy = {}\".format(accuracy_fin))\n",
    "time_stop = time.time()\n",
    "print(\"TOTAL TIME:  {}\".format(time_stop - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in file: data/Overlap_fixed4_separated/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "##### Check if you want to save your current model\n",
    "if update:\n",
    "    save_path = saver.save(sess, DATASET_PATH + \"model.ckpt\")\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 5.194926  , -1.2720153 , -2.8847473 , -0.9853008 ],\n",
      "       [ 1.1357379 ,  1.0422585 ,  0.7233293 , -2.5520568 ],\n",
      "       [ 1.9575088 , -0.4498188 ,  0.160128  , -1.4173847 ],\n",
      "       ...,\n",
      "       [-3.0523832 , -1.6476591 ,  3.955299  ,  1.124397  ],\n",
      "       [-2.091292  , -2.3387723 ,  0.45968875,  4.281103  ],\n",
      "       [-4.1597466 , -0.5643099 ,  3.0452852 ,  1.929373  ]],\n",
      "      dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "##### Inferencing\n",
    "\n",
    "# X_infer_path = \"utilities/something/something.txt\"\n",
    "X_infer_path = DATASET_PATH + \"X_test.txt\"\n",
    "\n",
    "X_val = load_X(X_infer_path)\n",
    "\n",
    "preds = sess.run(\n",
    "    [pred],\n",
    "    feed_dict={\n",
    "        x: X_val\n",
    "   }\n",
    ")\n",
    "\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2146\n",
      "17152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PF~4\\Python\\AN~3\\lib\\site-packages\\matplotlib\\font_manager.py:1297: UserWarning: findfont: Font family ['Bitstream Vera Sans'] not found. Falling back to DejaVu Sans\n",
      "  (prop.get_family(), self.defaultFamily[fontext]))\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuwAAALfCAYAAAA6zpf/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1849671eac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 80.1980197429657%\n",
      "\n",
      "Precision: 80.19847176241264%\n",
      "Recall: 80.19801980198021%\n",
      "f1_score: 80.12773397564436%\n",
      "\n",
      "Confusion Matrix:\n",
      "Created using test set of 1010 datapoints, normalised to % of each class in the test dataset\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzsAAAM3CAYAAAAa7U5UAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x184a1e291d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# (Inline plots: )\n",
    "%matplotlib inline\n",
    "\n",
    "font = {\n",
    "    'family' : 'Bitstream Vera Sans',\n",
    "    'weight' : 'bold',\n",
    "    'size'   : 18\n",
    "}\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "indep_train_axis = np.array(range(batch_size, (len(train_losses)+1)*batch_size, batch_size))\n",
    "#plt.plot(indep_train_axis, np.array(train_losses),     \"b--\", label=\"Train losses\")\n",
    "plt.plot(indep_train_axis, np.array(train_accuracies), \"g--\", label=\"Train accuracies\")\n",
    "\n",
    "indep_test_axis = np.append(\n",
    "    np.array(range(batch_size, len(test_losses)*display_iter, display_iter)[:-1]),\n",
    "    [training_iters]\n",
    ")\n",
    "#plt.plot(indep_test_axis, np.array(test_losses), \"b-\", linewidth=2.0, label=\"Test losses\")\n",
    "plt.plot(indep_test_axis, np.array(test_accuracies), \"b-\", linewidth=2.0, label=\"Test accuracies\")\n",
    "print(len(test_accuracies))\n",
    "print(len(train_accuracies))\n",
    "\n",
    "plt.title(\"Training session's Accuracy over Iterations\")\n",
    "plt.legend(loc='lower right', shadow=True)\n",
    "plt.ylabel('Training Accuracy')\n",
    "plt.xlabel('Training Iteration')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Results\n",
    "\n",
    "predictions = one_hot_predictions.argmax(1)\n",
    "\n",
    "print(\"Testing Accuracy: {}%\".format(100*accuracy_fin))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Precision: {}%\".format(100*metrics.precision_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"Recall: {}%\".format(100*metrics.recall_score(y_test, predictions, average=\"weighted\")))\n",
    "print(\"f1_score: {}%\".format(100*metrics.f1_score(y_test, predictions, average=\"weighted\")))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"Created using test set of {} datapoints, normalised to % of each class in the test dataset\".format(len(y_test)))\n",
    "confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
    "\n",
    "\n",
    "#print(confusion_matrix)\n",
    "normalised_confusion_matrix = np.array(confusion_matrix, dtype=np.float32)/np.sum(confusion_matrix)*100\n",
    "\n",
    "\n",
    "# Plot Results: \n",
    "width = 12\n",
    "height = 12\n",
    "plt.figure(figsize=(width, height))\n",
    "plt.imshow(\n",
    "    normalised_confusion_matrix, \n",
    "    interpolation='nearest', \n",
    "    cmap=plt.cm.Blues\n",
    ")\n",
    "plt.title(\"Confusion matrix \\n(normalised to % of total test data)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(n_classes)\n",
    "plt.xticks(tick_marks, LABELS, rotation=90)\n",
    "plt.yticks(tick_marks, LABELS)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#\n",
    "#X_val_path = DATASET_PATH + \"X_val.txt\"\n",
    "#X_val = load_X(X_val_path)\n",
    "#print X_val\n",
    "#\n",
    "#preds = sess.run(\n",
    "#    [pred],\n",
    "#    feed_dict={\n",
    "#        x: X_val\n",
    "#   }\n",
    "#)\n",
    "#\n",
    "#print preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34950495, 0.33861387, 0.48019803, 0.43564355, 0.4940594, 0.48514852, 0.46633664, 0.44752476, 0.5009901, 0.5019802, 0.54653466, 0.5188119, 0.5168317, 0.52079207, 0.54158413, 0.4950495, 0.54752475, 0.55841583, 0.44356436, 0.5049505, 0.5237624, 0.52970296, 0.54554456, 0.52970296, 0.519802, 0.46336633, 0.5049505, 0.4128713, 0.47623762, 0.4980198, 0.4970297, 0.5346535, 0.54851484, 0.52178216, 0.5435644, 0.5158416, 0.5267327, 0.54158413, 0.53267324, 0.56534654, 0.57128716, 0.5247525, 0.5257426, 0.55544555, 0.55346537, 0.5594059, 0.4861386, 0.519802, 0.56336635, 0.57128716, 0.5673267, 0.5752475, 0.5594059, 0.5772277, 0.54554456, 0.54752475, 0.51287127, 0.53168315, 0.5168317, 0.5059406, 0.5118812, 0.55742574, 0.5188119, 0.55544555, 0.5524753, 0.5841584, 0.560396, 0.5366337, 0.56138617, 0.55445546, 0.55643564, 0.5693069, 0.5009901, 0.52178216, 0.46138614, 0.44158417, 0.5168317, 0.42574257, 0.47524753, 0.5277228, 0.5188119, 0.55742574, 0.4940594, 0.54752475, 0.5425743, 0.5267327, 0.5514851, 0.5188119, 0.5435644, 0.5029703, 0.4920792, 0.55544555, 0.55643564, 0.54950494, 0.5960396, 0.53267324, 0.5257426, 0.54752475, 0.5445545, 0.55346537, 0.52079207, 0.5673267, 0.54950494, 0.5910891, 0.53960395, 0.46534654, 0.5366337, 0.5108911, 0.5019802, 0.53762376, 0.55346537, 0.56138617, 0.55544555, 0.5435644, 0.55643564, 0.5356436, 0.52079207, 0.5356436, 0.5346535, 0.53267324, 0.55445546, 0.54158413, 0.5237624, 0.52970296, 0.519802, 0.53069305, 0.5267327, 0.54554456, 0.5435644, 0.5782178, 0.58118814, 0.57227725, 0.57326734, 0.5831683, 0.53267324, 0.45742574, 0.53960395, 0.509901, 0.5118812, 0.4861386, 0.5346535, 0.53168315, 0.5158416, 0.5089109, 0.5435644, 0.54752475, 0.5594059, 0.54554456, 0.53762376, 0.55643564, 0.54950494, 0.54653466, 0.52970296, 0.55841583, 0.55445546, 0.5435644, 0.5693069, 0.56336635, 0.5772277, 0.55841583, 0.570297, 0.5425743, 0.56633663, 0.56336635, 0.55841583, 0.55841583, 0.54059404, 0.5148515, 0.5356436, 0.519802, 0.5277228, 0.52079207, 0.53168315, 0.5168317, 0.5356436, 0.56138617, 0.56435645, 0.54950494, 0.5118812, 0.52277225, 0.53960395, 0.5425743, 0.5188119, 0.5514851, 0.5366337, 0.54851484, 0.56237626, 0.55346537, 0.56534654, 0.5841584, 0.5257426, 0.5851485, 0.5435644, 0.54851484, 0.54752475, 0.54752475, 0.56534654, 0.570297, 0.5762376, 0.5831683, 0.5594059, 0.54554456, 0.55445546, 0.57227725, 0.57326734, 0.5841584, 0.56534654, 0.55841583, 0.55841583, 0.56534654, 0.53762376, 0.5524753, 0.54158413, 0.5514851, 0.5831683, 0.56633663, 0.5752475, 0.56336635, 0.55346537, 0.5752475, 0.5514851, 0.55544555, 0.52079207, 0.53069305, 0.56435645, 0.54950494, 0.55346537, 0.56435645, 0.5524753, 0.53960395, 0.54653466, 0.54851484, 0.55841583, 0.5514851, 0.5366337, 0.53069305, 0.5168317, 0.5435644, 0.5524753, 0.5980198, 0.5772277, 0.5891089, 0.5950495, 0.5891089, 0.55445546, 0.5871287, 0.5990099, 0.55346537, 0.54851484, 0.56534654, 0.54059404, 0.57128716, 0.5693069, 0.570297, 0.5841584, 0.59009904, 0.58217824, 0.57128716, 0.56633663, 0.53267324, 0.54059404, 0.55742574, 0.56138617, 0.55841583, 0.57425743, 0.56435645, 0.56138617, 0.52079207, 0.58019805, 0.54752475, 0.54158413, 0.55742574, 0.5524753, 0.5851485, 0.54752475, 0.5752475, 0.5752475, 0.4861386, 0.560396, 0.5366337, 0.5168317, 0.5267327, 0.5524753, 0.5267327, 0.5425743, 0.560396, 0.5356436, 0.560396, 0.55643564, 0.55445546, 0.5059406, 0.54752475, 0.59009904, 0.43168318, 0.48118812, 0.44455445, 0.5267327, 0.54059404, 0.5247525, 0.52277225, 0.53861386, 0.52178216, 0.55742574, 0.4881188, 0.51386136, 0.53960395, 0.48118812, 0.5178218, 0.5059406, 0.52871287, 0.4940594, 0.5168317, 0.53267324, 0.5168317, 0.5514851, 0.55346537, 0.53861386, 0.54752475, 0.52079207, 0.519802, 0.4980198, 0.5118812, 0.48514852, 0.5346535, 0.4960396, 0.5425743, 0.55445546, 0.5118812, 0.5, 0.53960395, 0.5108911, 0.46138614, 0.5049505, 0.5049505, 0.53762376, 0.53960395, 0.5336634, 0.53762376, 0.54158413, 0.53762376, 0.5594059, 0.5356436, 0.55544555, 0.52079207, 0.54950494, 0.53861386, 0.52970296, 0.46237624, 0.45841584, 0.53267324, 0.52871287, 0.57326734, 0.5168317, 0.52277225, 0.5267327, 0.5188119, 0.5673267, 0.5594059, 0.5277228, 0.519802, 0.46534654, 0.5039604, 0.4980198, 0.53069305, 0.5366337, 0.57425743, 0.5752475, 0.5990099, 0.55841583, 0.5910891, 0.54752475, 0.59009904, 0.5693069, 0.5841584, 0.58217824, 0.4990099, 0.48415843, 0.56435645, 0.55346537, 0.5079208, 0.48118812, 0.46930692, 0.46633664, 0.5089109, 0.48019803, 0.5069307, 0.51287127, 0.52871287, 0.5594059, 0.5158416, 0.5247525, 0.5079208, 0.53861386, 0.5346535, 0.52970296, 0.5435644, 0.56336635, 0.55742574, 0.54158413, 0.550495, 0.5178218, 0.48118812, 0.51386136, 0.51287127, 0.5366337, 0.55742574, 0.5445545, 0.57326734, 0.5594059, 0.57425743, 0.5762376, 0.5683168, 0.55445546, 0.5693069, 0.5445545, 0.4970297, 0.56237626, 0.52277225, 0.5425743, 0.52079207, 0.5782178, 0.5425743, 0.53861386, 0.53861386, 0.55445546, 0.5524753, 0.5445545, 0.54059404, 0.57227725, 0.5524753, 0.57326734, 0.56534654, 0.56633663, 0.56237626, 0.57326734, 0.5792079, 0.58118814, 0.5366337, 0.55742574, 0.55643564, 0.6069307, 0.6, 0.58118814, 0.5693069, 0.6118812, 0.61782175, 0.5871287, 0.5782178, 0.5960396, 0.5950495, 0.57128716, 0.5782178, 0.57128716, 0.6049505, 0.6188119, 0.5940594, 0.5960396, 0.5930693, 0.5891089, 0.5970297, 0.58118814, 0.55742574, 0.56336635, 0.5762376, 0.5782178, 0.4990099, 0.56336635, 0.58019805, 0.5871287, 0.58019805, 0.59009904, 0.6138614, 0.5990099, 0.5980198, 0.5960396, 0.5871287, 0.5990099, 0.5920792, 0.55742574, 0.54752475, 0.5851485, 0.56534654, 0.56435645, 0.570297, 0.5940594, 0.5891089, 0.58019805, 0.5772277, 0.5782178, 0.57425743, 0.6, 0.5940594, 0.5930693, 0.5851485, 0.5960396, 0.5841584, 0.609901, 0.6019802, 0.5851485, 0.6, 0.5841584, 0.6059406, 0.5990099, 0.6029703, 0.6118812, 0.6188119, 0.6386139, 0.64059407, 0.6227723, 0.6465347, 0.6475248, 0.63564354, 0.550495, 0.56633663, 0.6069307, 0.5970297, 0.62376237, 0.609901, 0.61782175, 0.6089109, 0.62376237, 0.6049505, 0.56138617, 0.56435645, 0.5851485, 0.5940594, 0.5940594, 0.6118812, 0.58217824, 0.5683168, 0.6207921, 0.6316832, 0.639604, 0.63465345, 0.6207921, 0.64158416, 0.63465345, 0.64950496, 0.6465347, 0.6455445, 0.6287129, 0.63267326, 0.62673265, 0.63267326, 0.63564354, 0.6544554, 0.64158416, 0.6455445, 0.61584157, 0.6386139, 0.63465345, 0.6118812, 0.6316832, 0.570297, 0.5970297, 0.5108911, 0.5841584, 0.58118814, 0.58019805, 0.6019802, 0.6188119, 0.6316832, 0.63564354, 0.62376237, 0.60792077, 0.61584157, 0.62376237, 0.6287129, 0.6465347, 0.63465345, 0.64455444, 0.63267326, 0.63564354, 0.6316832, 0.65346533, 0.670297, 0.63663363, 0.66039604, 0.670297, 0.6475248, 0.65148515, 0.670297, 0.6316832, 0.62574255, 0.63267326, 0.64851487, 0.6475248, 0.6019802, 0.53069305, 0.5831683, 0.6108911, 0.63366336, 0.6089109, 0.62376237, 0.6376238, 0.63663363, 0.6316832, 0.6128713, 0.64356434, 0.67524755, 0.65940595, 0.66930693, 0.67524755, 0.66138613, 0.6643564, 0.65742576, 0.66732675, 0.66633666, 0.6633663, 0.6316832, 0.64158416, 0.62376237, 0.65841585, 0.6950495, 0.68514854, 0.62475246, 0.63663363, 0.6544554, 0.65742576, 0.64356434, 0.63663363, 0.5920792, 0.6029703, 0.6277228, 0.6386139, 0.550495, 0.6138614, 0.6465347, 0.6633663, 0.670297, 0.670297, 0.6386139, 0.6623762, 0.680198, 0.6633663, 0.6475248, 0.65940595, 0.63366336, 0.6287129, 0.6633663, 0.65841585, 0.65742576, 0.64356434, 0.6633663, 0.6732673, 0.6277228, 0.6782178, 0.6475248, 0.6148515, 0.6039604, 0.609901, 0.6108911, 0.609901, 0.5673267, 0.6089109, 0.6039604, 0.63267326, 0.6217822, 0.62475246, 0.63366336, 0.6118812, 0.64950496, 0.6821782, 0.6277228, 0.6643564, 0.5910891, 0.55346537, 0.5178218, 0.5762376, 0.57128716, 0.5445545, 0.5891089, 0.56534654, 0.6118812, 0.57326734, 0.5990099, 0.5762376, 0.56237626, 0.6029703, 0.6207921, 0.5920792, 0.59009904, 0.5782178, 0.5782178, 0.6059406, 0.63465345, 0.61782175, 0.65148515, 0.63267326, 0.62475246, 0.66831684, 0.6544554, 0.65247524, 0.6732673, 0.66534656, 0.65247524, 0.6376238, 0.6722772, 0.6554455, 0.66138613, 0.68415844, 0.6782178, 0.670297, 0.6722772, 0.65049505, 0.66930693, 0.6732673, 0.6920792, 0.6722772, 0.670297, 0.6207921, 0.6128713, 0.64851487, 0.6544554, 0.6732673, 0.65742576, 0.6188119, 0.6118812, 0.6188119, 0.6554455, 0.59009904, 0.5980198, 0.6108911, 0.5990099, 0.62673265, 0.66138613, 0.66138613, 0.6287129, 0.64059407, 0.6554455, 0.64158416, 0.6722772, 0.6623762, 0.65643567, 0.65742576, 0.66138613, 0.6782178, 0.6881188, 0.65841585, 0.64257425, 0.639604, 0.65643567, 0.66732675, 0.7009901, 0.65940595, 0.6990099, 0.6831683, 0.68514854, 0.7069307, 0.71188116, 0.72772276, 0.7039604, 0.6960396, 0.709901, 0.6831683, 0.68613863, 0.7059406, 0.709901, 0.6881188, 0.66039604, 0.6930693, 0.6891089, 0.680198, 0.6871287, 0.6108911, 0.66633666, 0.65148515, 0.65742576, 0.63465345, 0.6306931, 0.6623762, 0.65841585, 0.6831683, 0.67722774, 0.670297, 0.6891089, 0.7158416, 0.6811881, 0.67722774, 0.6960396, 0.6920792, 0.68613863, 0.6910891, 0.6821782, 0.6930693, 0.7, 0.73069304, 0.65940595, 0.6811881, 0.6821782, 0.7049505, 0.68514854, 0.67623764, 0.6881188, 0.68415844, 0.6465347, 0.6722772, 0.6742574, 0.66633666, 0.66633666, 0.6980198, 0.6881188, 0.690099, 0.6910891, 0.6792079, 0.66831684, 0.63564354, 0.6712871, 0.67623764, 0.6623762, 0.6891089, 0.68613863, 0.7079208, 0.6881188, 0.65049505, 0.65049505, 0.6881188, 0.6960396, 0.6881188, 0.680198, 0.6980198, 0.6930693, 0.67524755, 0.690099, 0.7059406, 0.7049505, 0.6990099, 0.66633666, 0.6930693, 0.6811881, 0.6891089, 0.68613863, 0.7089109, 0.7257426, 0.7148515, 0.6792079, 0.7247525, 0.7138614, 0.7168317, 0.709901, 0.7039604, 0.7069307, 0.7079208, 0.7059406, 0.709901, 0.71881187, 0.71089107, 0.72079206, 0.71980196, 0.7059406, 0.7089109, 0.7257426, 0.7336634, 0.66831684, 0.68415844, 0.6782178, 0.69405943, 0.6970297, 0.7178218, 0.71089107, 0.709901, 0.690099, 0.73069304, 0.72772276, 0.7, 0.72079206, 0.73564357, 0.7089109, 0.73861384, 0.73069304, 0.71188116, 0.74356437, 0.7405941, 0.7326733, 0.74752474, 0.7316832, 0.71881187, 0.7148515, 0.7148515, 0.73861384, 0.7217822, 0.7178218, 0.7029703, 0.72871286, 0.7059406, 0.7019802, 0.7257426, 0.7148515, 0.72871286, 0.74356437, 0.7405941, 0.75247526, 0.7346535, 0.7405941, 0.7584158, 0.7316832, 0.72871286, 0.73069304, 0.71089107, 0.74455446, 0.75445545, 0.74455446, 0.7495049, 0.75346535, 0.74653465, 0.7584158, 0.75643563, 0.7504951, 0.7594059, 0.7594059, 0.7673267, 0.7504951, 0.7594059, 0.74752474, 0.75247526, 0.7653465, 0.76435643, 0.7504951, 0.6792079, 0.72871286, 0.7069307, 0.72871286, 0.73564357, 0.7336634, 0.75544554, 0.7326733, 0.6891089, 0.680198, 0.7009901, 0.6871287, 0.68613863, 0.7257426, 0.72772276, 0.73069304, 0.73762375, 0.7158416, 0.7257426, 0.72871286, 0.73663366, 0.72871286, 0.72079206, 0.7346535, 0.7059406, 0.71881187, 0.7089109, 0.73762375, 0.72079206, 0.72970295, 0.7217822, 0.7247525, 0.72871286, 0.7326733, 0.7237624, 0.7138614, 0.7059406, 0.7178218, 0.7227723, 0.7178218, 0.7267327, 0.73861384, 0.7227723, 0.7217822, 0.73069304, 0.7178218, 0.72772276, 0.7336634, 0.7336634, 0.73663366, 0.7267327, 0.72970295, 0.74752474, 0.7326733, 0.73960394, 0.75346535, 0.74356437, 0.7574257, 0.73069304, 0.73861384, 0.72079206, 0.74356437, 0.7584158, 0.75544554, 0.6910891, 0.7415842, 0.7267327, 0.7425743, 0.7405941, 0.74356437, 0.7405941, 0.75247526, 0.7425743, 0.7257426, 0.74752474, 0.75445545, 0.7495049, 0.74851483, 0.76435643, 0.73663366, 0.75445545, 0.76039606, 0.7326733, 0.74356437, 0.7504951, 0.74851483, 0.74554455, 0.74752474, 0.74752474, 0.5069307, 0.74455446, 0.7653465, 0.75346535, 0.75445545, 0.75643563, 0.75148517, 0.7772277, 0.7584158, 0.7247525, 0.74851483, 0.77128714, 0.75346535, 0.7425743, 0.74356437, 0.7346535, 0.73564357, 0.7158416, 0.73861384, 0.75346535, 0.74554455, 0.75544554, 0.73762375, 0.75346535, 0.77128714, 0.76435643, 0.7415842, 0.7663366, 0.75148517, 0.74554455, 0.7574257, 0.75148517, 0.74356437, 0.7495049, 0.74851483, 0.75445545, 0.73564357, 0.7049505, 0.7217822, 0.7217822, 0.7089109, 0.7227723, 0.65247524, 0.66138613, 0.65247524, 0.7, 0.7227723, 0.71089107, 0.7227723, 0.72772276, 0.72772276, 0.7267327, 0.7237624, 0.7316832, 0.7049505, 0.7405941, 0.72079206, 0.7267327, 0.7663366, 0.74752474, 0.7405941, 0.7504951, 0.74653465, 0.7495049, 0.7425743, 0.73663366, 0.74653465, 0.7584158, 0.7574257, 0.76930696, 0.73861384, 0.7504951, 0.72079206, 0.72871286, 0.7415842, 0.7089109, 0.72079206, 0.7495049, 0.75346535, 0.7504951, 0.75148517, 0.7158416, 0.73960394, 0.76930696, 0.7594059, 0.74752474, 0.76336634, 0.74554455, 0.75148517, 0.75445545, 0.7653465, 0.7821782, 0.7782178, 0.76930696, 0.76336634, 0.72970295, 0.73564357, 0.73069304, 0.7425743, 0.74653465, 0.75247526, 0.77029705, 0.7574257, 0.75544554, 0.76336634, 0.74752474, 0.7415842, 0.75445545, 0.73960394, 0.7009901, 0.7148515, 0.73861384, 0.7405941, 0.74752474, 0.75247526, 0.75544554, 0.7742574, 0.7663366, 0.7574257, 0.75247526, 0.7495049, 0.75247526, 0.75445545, 0.76435643, 0.7574257, 0.7673267, 0.77227724, 0.75148517, 0.77128714, 0.77128714, 0.74356437, 0.76435643, 0.74851483, 0.7495049, 0.75544554, 0.7495049, 0.76138616, 0.7673267, 0.7841584, 0.76930696, 0.78118813, 0.71980196, 0.73861384, 0.74752474, 0.75247526, 0.75346535, 0.7851485, 0.7772277, 0.7841584, 0.7851485, 0.7861386, 0.77920794, 0.7752475, 0.7653465, 0.78811884, 0.7940594, 0.78118813, 0.78811884, 0.7851485, 0.7762376, 0.7821782, 0.7910891, 0.7851485, 0.7821782, 0.7772277, 0.7831683, 0.7683168, 0.76930696, 0.7574257, 0.7663366, 0.77920794, 0.7653465, 0.7772277, 0.7574257, 0.77128714, 0.7732673, 0.76237625, 0.7594059, 0.76039606, 0.7673267, 0.74851483, 0.77920794, 0.73663366, 0.7653465, 0.77920794, 0.7762376, 0.7594059, 0.77128714, 0.7405941, 0.74752474, 0.75346535, 0.75643563, 0.77029705, 0.7821782, 0.7821782, 0.7841584, 0.7752475, 0.78118813, 0.7851485, 0.7772277, 0.7673267, 0.76039606, 0.7841584, 0.7673267, 0.7653465, 0.7504951, 0.75643563, 0.7594059, 0.75247526, 0.75643563, 0.75148517, 0.75346535, 0.75346535, 0.7683168, 0.7504951, 0.76435643, 0.7594059, 0.7653465, 0.75346535, 0.75346535, 0.74554455, 0.75643563, 0.76237625, 0.7742574, 0.76435643, 0.7326733, 0.76435643, 0.7148515, 0.75148517, 0.74851483, 0.76039606, 0.77227724, 0.7752475, 0.77029705, 0.7267327, 0.74851483, 0.74851483, 0.74455446, 0.7415842, 0.75346535, 0.7742574, 0.7920792, 0.7574257, 0.7584158, 0.7683168, 0.7732673, 0.7851485, 0.7683168, 0.77227724, 0.7752475, 0.7495049, 0.7663366, 0.76930696, 0.77227724, 0.76930696, 0.7653465, 0.77227724, 0.7584158, 0.7752475, 0.77128714, 0.7752475, 0.7861386, 0.77227724, 0.7772277, 0.7851485, 0.7831683, 0.7782178, 0.7772277, 0.77920794, 0.77227724, 0.78910893, 0.7405941, 0.76039606, 0.7495049, 0.7574257, 0.76336634, 0.7574257, 0.7673267, 0.77128714, 0.7425743, 0.7831683, 0.7841584, 0.76930696, 0.7752475, 0.7871287, 0.76435643, 0.7673267, 0.7584158, 0.77920794, 0.7851485, 0.77128714, 0.7782178, 0.7653465, 0.76435643, 0.7584158, 0.77128714, 0.77128714, 0.7762376, 0.78118813, 0.77227724, 0.7732673, 0.7831683, 0.76336634, 0.77128714, 0.76039606, 0.76138616, 0.76435643, 0.75445545, 0.76336634, 0.78811884, 0.78811884, 0.7732673, 0.7762376, 0.7752475, 0.78118813, 0.7851485, 0.7782178, 0.78019804, 0.77920794, 0.7772277, 0.7752475, 0.7831683, 0.77920794, 0.7831683, 0.77227724, 0.73762375, 0.76435643, 0.7683168, 0.78019804, 0.7772277, 0.75643563, 0.75445545, 0.75643563, 0.74653465, 0.75445545, 0.7683168, 0.7574257, 0.73069304, 0.74455446, 0.7584158, 0.7663366, 0.76930696, 0.7673267, 0.75148517, 0.76930696, 0.75346535, 0.73861384, 0.74653465, 0.7594059, 0.76930696, 0.76039606, 0.76237625, 0.75643563, 0.74554455, 0.7138614, 0.7089109, 0.7217822, 0.690099, 0.7326733, 0.7346535, 0.7673267, 0.77227724, 0.77920794, 0.7673267, 0.7653465, 0.7683168, 0.7782178, 0.76138616, 0.7504951, 0.76930696, 0.76930696, 0.7663366, 0.7762376, 0.77920794, 0.7683168, 0.7663366, 0.7663366, 0.77128714, 0.7871287, 0.77920794, 0.7663366, 0.76138616, 0.7495049, 0.73564357, 0.73960394, 0.73762375, 0.75643563, 0.7594059, 0.7504951, 0.76039606, 0.77029705, 0.7752475, 0.7683168, 0.7732673, 0.7574257, 0.76435643, 0.76336634, 0.74554455, 0.73564357, 0.7495049, 0.75445545, 0.76237625, 0.71089107, 0.7415842, 0.73960394, 0.73762375, 0.7683168, 0.75148517, 0.7574257, 0.76138616, 0.7594059, 0.75148517, 0.7584158, 0.76435643, 0.77029705, 0.7673267, 0.7732673, 0.7752475, 0.7673267, 0.75148517, 0.77029705, 0.7594059, 0.7732673, 0.76138616, 0.7851485, 0.7762376, 0.7772277, 0.7782178, 0.78019804, 0.7841584, 0.7910891, 0.78118813, 0.78019804, 0.7683168, 0.77227724, 0.77128714, 0.7752475, 0.75643563, 0.7316832, 0.75445545, 0.7663366, 0.76930696, 0.76336634, 0.77128714, 0.77128714, 0.77920794, 0.78118813, 0.76237625, 0.7653465, 0.76435643, 0.7574257, 0.76435643, 0.7772277, 0.7871287, 0.7821782, 0.75544554, 0.76138616, 0.76039606, 0.7504951, 0.7574257, 0.7574257, 0.76138616, 0.77029705, 0.7742574, 0.7683168, 0.76138616, 0.76138616, 0.7673267, 0.76039606, 0.76039606, 0.7732673, 0.7742574, 0.7663366, 0.77029705, 0.76237625, 0.7841584, 0.7861386, 0.7782178, 0.76237625, 0.78811884, 0.77920794, 0.7772277, 0.75247526, 0.7683168, 0.7504951, 0.7673267, 0.75643563, 0.7752475, 0.75445545, 0.76138616, 0.7782178, 0.7782178, 0.7742574, 0.7742574, 0.77128714, 0.76930696, 0.7732673, 0.77029705, 0.7841584, 0.7851485, 0.7762376, 0.7405941, 0.7683168, 0.75643563, 0.7752475, 0.7653465, 0.76336634, 0.7594059, 0.77227724, 0.7841584, 0.76930696, 0.7653465, 0.76336634, 0.77029705, 0.7663366, 0.76930696, 0.7782178, 0.7742574, 0.7752475, 0.7742574, 0.7831683, 0.7742574, 0.76435643, 0.7772277, 0.7683168, 0.7673267, 0.77227724, 0.77920794, 0.78019804, 0.7683168, 0.77227724, 0.7861386, 0.76237625, 0.74356437, 0.76237625, 0.7742574, 0.7782178, 0.77227724, 0.77227724, 0.76039606, 0.76336634, 0.7871287, 0.7742574, 0.7841584, 0.7861386, 0.7861386, 0.7752475, 0.7772277, 0.7841584, 0.77128714, 0.7831683, 0.78811884, 0.7831683, 0.7861386, 0.7821782, 0.7920792, 0.7762376, 0.76435643, 0.7772277, 0.77128714, 0.7584158, 0.7257426, 0.7663366, 0.74455446, 0.7752475, 0.76138616, 0.7594059, 0.76930696, 0.7772277, 0.7821782, 0.7732673, 0.7762376, 0.74554455, 0.7772277, 0.7831683, 0.7851485, 0.7851485, 0.78910893, 0.7841584, 0.7732673, 0.77920794, 0.7762376, 0.77227724, 0.7762376, 0.7841584, 0.7772277, 0.77227724, 0.7594059, 0.7732673, 0.77029705, 0.76435643, 0.75544554, 0.7752475, 0.76435643, 0.7584158, 0.76039606, 0.7732673, 0.75346535, 0.75445545, 0.7584158, 0.7851485, 0.7762376, 0.77128714, 0.7772277, 0.7772277, 0.76435643, 0.7821782, 0.7732673, 0.7782178, 0.7762376, 0.7762376, 0.7920792, 0.77227724, 0.76237625, 0.7683168, 0.77920794, 0.76237625, 0.7782178, 0.7772277, 0.7762376, 0.77128714, 0.7960396, 0.78019804, 0.7871287, 0.78118813, 0.7574257, 0.78118813, 0.7831683, 0.7930693, 0.75544554, 0.7871287, 0.7851485, 0.7782178, 0.78811884, 0.7841584, 0.7930693, 0.7732673, 0.78910893, 0.8039604, 0.7980198, 0.78910893, 0.790099, 0.7861386, 0.78019804, 0.7861386, 0.7950495, 0.76930696, 0.7821782, 0.7782178, 0.7762376, 0.7930693, 0.7762376, 0.7782178, 0.77128714, 0.78019804, 0.78910893, 0.7871287, 0.7752475, 0.78118813, 0.7841584, 0.7821782, 0.78019804, 0.7841584, 0.7920792, 0.8009901, 0.7910891, 0.7871287, 0.7762376, 0.78019804, 0.77920794, 0.7910891, 0.7851485, 0.7821782, 0.7732673, 0.7752475, 0.7742574, 0.7990099, 0.7950495, 0.78118813, 0.7930693, 0.77920794, 0.7732673, 0.7752475, 0.7841584, 0.8019802, 0.7821782, 0.7910891, 0.7950495, 0.8019802, 0.7940594, 0.7940594, 0.7831683, 0.790099, 0.77920794, 0.7752475, 0.7732673, 0.78910893, 0.7980198, 0.7910891, 0.7940594, 0.7990099, 0.78811884, 0.7742574, 0.78019804, 0.7861386, 0.7980198, 0.7930693, 0.790099, 0.7970297, 0.8019802, 0.7920792, 0.7742574, 0.7732673, 0.7831683, 0.790099, 0.7762376, 0.7752475, 0.78811884, 0.7772277, 0.7920792, 0.7871287, 0.7752475, 0.7237624, 0.76930696, 0.7752475, 0.8019802, 0.72871286, 0.7742574, 0.77128714, 0.7683168, 0.7762376, 0.7782178, 0.7732673, 0.7861386, 0.7821782, 0.7851485, 0.75643563, 0.7594059, 0.7574257, 0.7742574, 0.78811884, 0.7930693, 0.7861386, 0.77920794, 0.78118813, 0.77227724, 0.7495049, 0.7752475, 0.74653465, 0.74455446, 0.77227724, 0.77029705, 0.74752474, 0.7851485, 0.75643563, 0.7594059, 0.7663366, 0.73762375, 0.7405941, 0.75247526, 0.75346535, 0.7772277, 0.7752475, 0.78118813, 0.790099, 0.78019804, 0.7821782, 0.7950495, 0.78811884, 0.7861386, 0.7861386, 0.8029703, 0.7930693, 0.7841584, 0.77227724, 0.78019804, 0.7861386, 0.78811884, 0.7831683, 0.7831683, 0.7732673, 0.7732673, 0.75643563, 0.73861384, 0.76336634, 0.78019804, 0.7732673, 0.7742574, 0.7752475, 0.77128714, 0.7742574, 0.7851485, 0.76930696, 0.7980198, 0.7732673, 0.77128714, 0.76138616, 0.7673267, 0.7495049, 0.75445545, 0.77227724, 0.7950495, 0.78019804, 0.7772277, 0.77920794, 0.8019802, 0.78811884, 0.78019804, 0.7990099, 0.7980198, 0.78118813, 0.7732673, 0.7821782, 0.7861386, 0.7772277, 0.7782178, 0.76930696, 0.74554455, 0.78019804, 0.7762376, 0.7871287, 0.8019802, 0.78811884, 0.7980198, 0.7910891, 0.77227724, 0.7762376, 0.77128714, 0.7871287, 0.78811884, 0.790099, 0.7821782, 0.7732673, 0.78811884, 0.7940594, 0.7841584, 0.7762376, 0.78910893, 0.7930693, 0.7970297, 0.7990099, 0.78118813, 0.7653465, 0.7851485, 0.7762376, 0.76039606, 0.7495049, 0.76138616, 0.7831683, 0.77128714, 0.77128714, 0.7732673, 0.7742574, 0.7594059, 0.7673267, 0.7405941, 0.76039606, 0.7425743, 0.7247525, 0.72079206, 0.7495049, 0.75445545, 0.7653465, 0.7653465, 0.7752475, 0.77029705, 0.7742574, 0.77227724, 0.76930696, 0.7841584, 0.7782178, 0.7732673, 0.7574257, 0.7782178, 0.7742574, 0.77227724, 0.75346535, 0.7227723, 0.7158416, 0.75445545, 0.7415842, 0.76435643, 0.74851483, 0.76336634, 0.77227724, 0.7673267, 0.7653465, 0.7683168, 0.76930696, 0.7752475, 0.7762376, 0.77227724, 0.7831683, 0.7762376, 0.7772277, 0.77227724, 0.7772277, 0.77920794, 0.78019804, 0.7752475, 0.77227724, 0.7683168, 0.76930696, 0.77029705, 0.7673267, 0.7782178, 0.77029705, 0.7732673, 0.78118813, 0.6831683, 0.75544554, 0.7504951, 0.7584158, 0.7405941, 0.74851483, 0.74356437, 0.7316832, 0.73861384, 0.75346535, 0.7752475, 0.75643563, 0.76930696, 0.75346535, 0.73861384, 0.74752474, 0.76138616, 0.7594059, 0.7594059, 0.76435643, 0.74851483, 0.75148517, 0.75643563, 0.76039606, 0.7584158, 0.7594059, 0.77227724, 0.76237625, 0.7673267, 0.76435643, 0.75643563, 0.7871287, 0.7653465, 0.78118813, 0.7782178, 0.77920794, 0.7831683, 0.7841584, 0.78118813, 0.7772277, 0.7871287, 0.7841584, 0.7841584, 0.77920794, 0.77128714, 0.7732673, 0.7782178, 0.76336634, 0.78118813, 0.7732673, 0.76237625, 0.7673267, 0.73861384, 0.76237625, 0.76930696, 0.7762376, 0.76435643, 0.76237625, 0.7584158, 0.77128714, 0.77128714, 0.7653465, 0.7653465, 0.73663366, 0.7574257, 0.73762375, 0.76435643, 0.76435643, 0.7752475, 0.7782178, 0.7673267, 0.75643563, 0.7752475, 0.7653465, 0.74752474, 0.76138616, 0.7851485, 0.7584158, 0.7762376, 0.77029705, 0.76435643, 0.7841584, 0.7762376, 0.7910891, 0.78118813, 0.7950495, 0.7851485, 0.76138616, 0.7574257, 0.76930696, 0.7574257, 0.73861384, 0.73861384, 0.74554455, 0.76237625, 0.7673267, 0.77227724, 0.7495049, 0.7742574, 0.7782178, 0.7851485, 0.7782178, 0.7762376, 0.7851485, 0.7841584, 0.7841584, 0.7851485, 0.7821782, 0.7683168, 0.76237625, 0.7584158, 0.76237625, 0.75445545, 0.77227724, 0.78019804, 0.78019804, 0.77029705, 0.76138616, 0.76138616, 0.74554455, 0.7415842, 0.77029705, 0.7732673, 0.76930696, 0.76237625, 0.75544554, 0.75247526, 0.74653465, 0.73564357, 0.75247526, 0.7683168, 0.7732673, 0.78019804, 0.7732673, 0.7851485, 0.7752475, 0.7831683, 0.7861386, 0.78118813, 0.78019804, 0.7673267, 0.76237625, 0.7594059, 0.75445545, 0.7683168, 0.7772277, 0.7742574, 0.74653465, 0.7584158, 0.76138616, 0.7653465, 0.7683168, 0.75643563, 0.7594059, 0.72772276, 0.73861384, 0.7683168, 0.7663366, 0.7663366, 0.7920792, 0.7841584, 0.77227724, 0.7732673, 0.7782178, 0.7732673, 0.78019804, 0.78118813, 0.7970297, 0.7851485, 0.7950495, 0.7861386, 0.7831683, 0.7841584, 0.77029705, 0.7752475, 0.7920792, 0.7851485, 0.7851485, 0.7871287, 0.7871287, 0.7851485, 0.76336634, 0.7683168, 0.76237625, 0.75346535, 0.76237625, 0.7831683, 0.7970297, 0.78811884, 0.7732673, 0.78910893, 0.7841584, 0.7831683, 0.7732673, 0.7653465, 0.76930696, 0.77920794, 0.7752475, 0.78811884, 0.7861386, 0.7772277, 0.7831683, 0.7782178, 0.7841584, 0.7821782, 0.7871287, 0.7970297, 0.7821782, 0.78910893, 0.790099, 0.78811884, 0.7990099, 0.7970297, 0.809901, 0.8009901, 0.78910893, 0.7960396, 0.7950495, 0.7960396, 0.7970297, 0.7960396, 0.7861386, 0.7940594, 0.7960396, 0.7990099, 0.7980198, 0.7742574, 0.77920794, 0.8019802, 0.8019802]\n"
     ]
    }
   ],
   "source": [
    "#sess.close()\n",
    "print(test_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Final accuracy of >90% is pretty good, considering that training takes about 7 minutes.\n",
    "\n",
    "Noticeable confusion between activities of Clapping Hands and Boxing, and between Jumping Jacks and Waving Two Hands which is understandable.\n",
    "\n",
    "In terms of the applicability of this to a wider dataset, I would imagine that it would be able to work for any activities in which the training included a views from all angles to be tested on. It would be interesting to see it's applicability to camera angles in between the 4 used in this dataset, without training on them specifically.\n",
    "\n",
    " Overall, this experiment validates the idea that 2D pose can be used for at least human activity recognition, and provides verification to continue onto use of 2D pose for behaviour estimation in both people and animals\n",
    " \n",
    "\n",
    " ### With regards to Using LSTM-RNNs\n",
    " - Batch sampling\n",
    "     - It is neccessary to ensure you are not just sampling classes one at a time! (ie y_train is ordered by class and batch chosen in order)The use of random sampling of batches without replacement from the training data resolves this.    \n",
    " \n",
    " - Architecture\n",
    "     - Testing has been run using a variety of hidden units per LSTM cell, with results showing that testing accuracy achieves a higher score when using a number of hidden cells approximately equal to that of the input, ie 34. The following figure displays the final accuracy achieved on the testing dataset for a variety of hidden units, all using a batch size of 4096 and 300 epochs (a total of 1657 iterations, with testing performed every 8th iteration).\n",
    "   \n",
    " \n",
    " \n",
    "\n",
    "## Future Works\n",
    "\n",
    "Inclusion of :\n",
    "\n",
    " - A pipeline for qualitative results\n",
    " - A validation dataset\n",
    " - Momentum     \n",
    " - Normalise input data (each point with respect to distribution of itself only)\n",
    " - Dropout\n",
    " - Comparison of effect of changing batch size\n",
    " \n",
    "\n",
    "Further research will be made into the use on more subtle activity classes, such as walking versus running, agitated movement versus calm movement, and perhaps normal versus abnormal behaviour, based on a baseline of normal motion.\n",
    "\n",
    "\n",
    "## References\n",
    "\n",
    "The dataset can be found at http://tele-immersion.citris-uc.org/berkeley_mhad released under the BSD-2 license\n",
    ">Copyright (c) 2013, Regents of the University of California All rights reserved.\n",
    "\n",
    "The network used in this experiment is based on the following, available under the [MIT License](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition/blob/master/LICENSE). :\n",
    "> Guillaume Chevalier, LSTMs for Human Activity Recognition, 2016\n",
    "> https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook LSTM.ipynb to markdown\n",
      "[NbConvertApp] Writing 389281 bytes to LSTM.md\n"
     ]
    }
   ],
   "source": [
    "# Let's convert this notebook to a README for the GitHub project's title page:\n",
    "!jupyter nbconvert --to markdown LSTM.ipynb\n",
    "!mv LSTM.md README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### Camera\n",
    "\n",
    "# import argparse\n",
    "# import logging\n",
    "# import time\n",
    "\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# from tf_pose.estimator import TfPoseEstimator\n",
    "# from tf_pose.networks import get_graph_path, model_wh\n",
    "\n",
    "# logger = logging.getLogger('TfPoseEstimator-WebCam')\n",
    "# logger.setLevel(logging.DEBUG)\n",
    "# ch = logging.StreamHandler()\n",
    "# ch.setLevel(logging.DEBUG)\n",
    "# formatter = logging.Formatter('[%(asctime)s] [%(name)s] [%(levelname)s] %(message)s')\n",
    "# ch.setFormatter(formatter)\n",
    "# logger.addHandler(ch)\n",
    "\n",
    "# class openpose:\n",
    "#     def __init__(self, camera=0,resize='0x0',resize_out_ratio=4.0,model='mobilenet_thin',show_process=False):\n",
    "\n",
    "#         logger.debug('initialization %s : %s' % (model, get_graph_path(model)))\n",
    "#         w, h = model_wh(resize)\n",
    "#         if w > 0 and h > 0:\n",
    "#             e = TfPoseEstimator(get_graph_path(model), target_size=(w, h))\n",
    "#         else:\n",
    "#             e = TfPoseEstimator(get_graph_path(model), target_size=(432, 368))\n",
    "#         logger.debug('cam read+')\n",
    "#         cam = cv2.VideoCapture(camera)\n",
    "#         ret_val, image = cam.read()\n",
    "#         logger.info('cam image=%dx%d' % (image.shape[1], image.shape[0]))\n",
    "        \n",
    "#         fps_time = 0\n",
    "\n",
    "#         while True:\n",
    "#             ret_val, image = cam.read()\n",
    "\n",
    "#             logger.debug('image process+')\n",
    "#             humans = e.inference(image, resize_to_default=(w > 0 and h > 0), upsample_size=resize_out_ratio)\n",
    "\n",
    "#             logger.debug('postprocess+')\n",
    "#             image = TfPoseEstimator.draw_humans(image, humans, imgcopy=False)\n",
    "\n",
    "#             logger.debug('show+')\n",
    "#             cv2.putText(image,\n",
    "#                         \"FPS: %f\" % (1.0 / (time.time() - fps_time)),\n",
    "#                         (10, 10),  cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n",
    "#                         (0, 255, 0), 2)\n",
    "#             cv2.imshow('tf-pose-estimation result', image)\n",
    "#             fps_time = time.time()\n",
    "#             if cv2.waitKey(1) == 27:\n",
    "#                 break\n",
    "#             logger.debug('finished+')\n",
    "\n",
    "#         cv2.destroyAllWindows()\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     openpose()\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
